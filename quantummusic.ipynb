{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "intro",
      "metadata": {},
      "source": [
        "# QuantumMusic - Unified Version with Larger LUFS Chunk\n",
        "\n",
        "### Introduction\n",
        "\n",
        "This is the Python notebook for the entire quantum music analysis project\n",
        "Author: Rohan Agarwal\n",
        "\n",
        "Now we want to expand our notebook to do comparison between two tracks - a teacher and a student\n",
        "Add a new module for this\n",
        "This module will: a) take two tracks one called \"teacher\" and one called \"student\".  Currently we have hard coded a filename in grade_single_file.  This piece should be replaced with the file names being passed from this function  b) run the complete program on each track and collect all metrics for both tracks, c) write new functionality that will compare the two tracks and generate a detailed comparison of the two tracks and store all the differences in a different data structure.  This comparison will be interesting because you are assuming that the \"teacher\" track is correct.  Therefore, even if the \"student\" track is not off pitch but it differs from the \"teacher\" track at that point, you will mark the difference.  You will have to write some proper code to compare and contrast two tracks and see where the differences are.  Do a first pass now and then we can refine from there. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "constants",
      "metadata": {},
      "source": [
        "## 1. Constants & Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "code-constants",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. CONSTANTS & IMPORTS\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import psycopg2\n",
        "from psycopg2.extras import Json\n",
        "import numpy as np\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import re\n",
        "from matplotlib import colormaps\n",
        "import scipy.signal  # (if not already imported)\n",
        "from scipy.signal import find_peaks\n",
        "from scipy.signal import iirnotch, filtfilt\n",
        "\n",
        "\n",
        "from scipy.signal import butter, filtfilt\n",
        "\n",
        "# For Praat-based HNR\n",
        "import parselmouth\n",
        "from parselmouth.praat import call\n",
        "\n",
        "# For audio playback in Jupyter\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "# For LUFS-based loudness\n",
        "try:\n",
        "    import pyloudnorm as pyln\n",
        "    LOUDNORM_AVAILABLE = True\n",
        "except ImportError:\n",
        "    LOUDNORM_AVAILABLE = False\n",
        "    print(\"Warning: pyloudnorm not installed, LUFS computations will be skipped.\")\n",
        "\n",
        "# Quantum Computing imports\n",
        "# Qiskit imports\n",
        "from qiskit import QuantumCircuit, transpile\n",
        "from qiskit_aer import AerSimulator  # modern Aer simulator from qiskit-aer\n",
        "\n",
        "\n",
        "# Directory constants\n",
        "INPUT_DIR = \"data/trainingdata\"\n",
        "OUTPUT_DIR = \"output\"\n",
        "\n",
        "# Database constants\n",
        "DB_NAME = \"quantummusic\"\n",
        "DB_HOST = \"localhost\"\n",
        "DB_USER = \"postgres\"  # placeholder\n",
        "DB_PASSWORD = \"postgres\"  # placeholder\n",
        "\n",
        "# Audio processing constants\n",
        "STANDARD_SR = 44100  # Standard sampling rate\n",
        "SILENCE_THRESHOLD_DB = 30  # dB threshold for silence trimming\n",
        "\n",
        "# Band-pass filter constants\n",
        "LOW_FREQ = 80.0\n",
        "HIGH_FREQ = 3000.0\n",
        "\n",
        "# Visualization constants\n",
        "FIG_SIZE = (10, 4)\n",
        "\n",
        "# Save-to-DB constant\n",
        "SAVE_TO_DB = False\n",
        "\n",
        "# Frame-based approach for pitch detection\n",
        "FRAME_SIZE = 2048\n",
        "HOP_LENGTH = 512  # normal frames\n",
        "# Praat-based chunk size\n",
        "PRAAT_CHUNK_SIZE = 2048  # for Praat\n",
        "\n",
        "# Deviation threshold in cents, for dev_flag\n",
        "DEVIATION_THRESHOLD = 50.0\n",
        "\n",
        "# Multi-chunk tempo analysis\n",
        "TEMPO_CHUNK_SIZE_MEDIUM = 4096\n",
        "TEMPO_CHUNK_SIZE_LARGE = 16384\n",
        "\n",
        "# New constants for advanced vocal feature extraction\n",
        "VOCAL_FEATURE_CHUNK_SIZE = 16384       # New chunk size for advanced feature extraction (samples)\n",
        "VOCAL_FEATURE_CHUNK_HOP = 4096        # Hop size for overlapping chunks (e.g., 50% overlap)\n",
        "\n",
        "# NEW constant for LUFS calculations (0.5s @ 44.1kHz)\n",
        "LUFS_CHUNK_SIZE = 22050\n",
        "\n",
        "# --- NEW CONSTANTS FOR ADVANCED VOCAL FEATURE EXTRACTION ---\n",
        "\n",
        "# Formant analysis parameters\n",
        "FORMANT_ANALYSIS_TIME = 0.1            # Time (in seconds) at which to extract formants\n",
        "FORMANT_TIME_STEP = 0.01               # Time step for formant analysis\n",
        "MAX_NUMBER_OF_FORMANTS = 5             # Maximum number of formants computed by Praat\n",
        "MAXIMUM_FORMANT_FREQUENCY = 5500       # Maximum frequency considered for formant analysis\n",
        "NUM_FORMANTS_TO_EXTRACT = 3            # Number of formants to extract (e.g., F1, F2, F3)\n",
        "\n",
        "# Pitch / jitter / shimmer parameters\n",
        "MIN_F0 = 75                          # Minimum expected fundamental frequency (Hz)\n",
        "MAX_F0 = 500                         # Maximum expected fundamental frequency (Hz)\n",
        "JITTER_TIME_STEP = 0.0001             # Time step for jitter computation\n",
        "JITTER_MIN_PERIOD = 0.02              # Minimum period threshold for jitter\n",
        "JITTER_MAX_PERIOD = 1.3               # Maximum period threshold factor for jitter\n",
        "SHIMMER_MIN_AMPLITUDE = 0.0001         # Minimum amplitude threshold for shimmer\n",
        "SHIMMER_MAX_AMPLITUDE = 0.02           # Maximum amplitude threshold for shimmer\n",
        "SHIMMER_FACTOR = 1.6                 # Shimmer factor (per Praat defaults)\n",
        "\n",
        "# Vibrato analysis parameters\n",
        "VIBRATO_MIN_HZ = 3                   # Lower bound for vibrato rate (Hz)\n",
        "VIBRATO_MAX_HZ = 10                  # Upper bound for vibrato rate (Hz)\n",
        "MEDIAN_FILTER_KERNEL_SIZE = 9        # Kernel size for median filtering pitch contours\n",
        "\n",
        "\n",
        "# Ensure output directories exist\n",
        "os.makedirs(INPUT_DIR, exist_ok=True)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "def test_imports():\n",
        "    print(\"Imports tested, all modules are available.\")\n",
        "\n",
        "test_imports()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25acf27a",
      "metadata": {},
      "source": [
        "## 2 - Converting Numbers to Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4c12516",
      "metadata": {},
      "outputs": [],
      "source": [
        "########################\n",
        "# NEW: Classification Helpers\n",
        "########################\n",
        "\n",
        "def classify_pitch_deviation(dev_cents):\n",
        "    \"\"\"\n",
        "    Classify pitch deviation into categories based on absolute distance from nearest note.\n",
        "    \"\"\"\n",
        "    if dev_cents is None or np.isnan(dev_cents):\n",
        "        return \"unknown\"\n",
        "    dev_abs = abs(dev_cents)\n",
        "    if dev_abs < 10:\n",
        "        return \"perfect\"\n",
        "    elif dev_abs < 30:\n",
        "        return \"good\"\n",
        "    elif dev_abs < 50:\n",
        "        return \"fair\"\n",
        "    elif dev_abs < 100:\n",
        "        return \"poor\"\n",
        "    else:\n",
        "        return \"very poor\"\n",
        "\n",
        "\n",
        "def classify_tone_to_noise(sf_val):\n",
        "    \"\"\"\n",
        "    Classify spectral flatness into a rough 'tonal vs. noisy' scale.\n",
        "    Typical spectral flatness range is ~0 to 1.\n",
        "    \"\"\"\n",
        "    if np.isnan(sf_val):\n",
        "        return \"unknown\"\n",
        "    if sf_val < 0.02:\n",
        "        return \"very tonal\"\n",
        "    elif sf_val < 0.08:\n",
        "        return \"mostly tonal\"\n",
        "    elif sf_val < 0.15:\n",
        "        return \"mixed\"\n",
        "    elif sf_val < 0.3:\n",
        "        return \"noisy\"\n",
        "    else:\n",
        "        return \"very noisy\"\n",
        "\n",
        "\n",
        "def classify_transition_score(score):\n",
        "    \"\"\"\n",
        "    Classify how smoothly the pitch transitions from the previous chunk.\n",
        "    Score ~ 0..1, with 1 = no pitch jump, 0 = extremely large jump\n",
        "    \"\"\"\n",
        "    if np.isnan(score):\n",
        "        return \"unknown\"\n",
        "    if score >= 0.8:\n",
        "        return \"smooth\"\n",
        "    elif score >= 0.5:\n",
        "        return \"moderate\"\n",
        "    else:\n",
        "        return \"abrupt\"\n",
        "\n",
        "\n",
        "def classify_rms_db(rms_db):\n",
        "    \"\"\"\n",
        "    Classify RMS in dB into categories from very soft => very loud\n",
        "    \"\"\"\n",
        "    if np.isnan(rms_db):\n",
        "        return \"unknown\"\n",
        "    if rms_db < -40:\n",
        "        return \"very soft\"\n",
        "    elif rms_db < -20:\n",
        "        return \"soft\"\n",
        "    elif rms_db < -10:\n",
        "        return \"moderate\"\n",
        "    elif rms_db < -2:\n",
        "        return \"loud\"\n",
        "    else:\n",
        "        return \"very loud\"\n",
        "\n",
        "\n",
        "def classify_lufs(lufs_val):\n",
        "    \"\"\"\n",
        "    Classify LUFS values into rough loudness categories.\n",
        "    Typically, lower (more negative) = quieter, higher = louder.\n",
        "    \"\"\"\n",
        "    if lufs_val is None or np.isnan(lufs_val):\n",
        "        return \"unknown\"\n",
        "    if lufs_val < -40:\n",
        "        return \"very soft\"\n",
        "    elif lufs_val < -23:\n",
        "        return \"soft\"\n",
        "    elif lufs_val < -14:\n",
        "        return \"moderate\"\n",
        "    elif lufs_val < -5:\n",
        "        return \"loud\"\n",
        "    else:\n",
        "        return \"very loud\"\n",
        "\n",
        "\n",
        "def classify_praat_hnr(hnr_val):\n",
        "    \"\"\"\n",
        "    Classify Praat HNR (dB). Typical range: 0..35 dB for normal voices.\n",
        "    Higher = more harmonic, lower = more noise.\n",
        "    \"\"\"\n",
        "    if np.isnan(hnr_val):\n",
        "        return \"unknown\"\n",
        "    if hnr_val < 5:\n",
        "        return \"very noisy\"\n",
        "    elif hnr_val < 15:\n",
        "        return \"noisy\"\n",
        "    elif hnr_val < 25:\n",
        "        return \"moderately harmonic\"\n",
        "    else:\n",
        "        return \"very harmonic\"\n",
        "\n",
        "\n",
        "def classify_tempo_bpm(bpm_val):\n",
        "    \"\"\"\n",
        "    Classify BPM values into slow, moderate, fast, etc.\n",
        "    \"\"\"\n",
        "    if np.isnan(bpm_val):\n",
        "        return \"unknown\"\n",
        "    if bpm_val < 40:\n",
        "        return \"very slow\"\n",
        "    elif bpm_val < 70:\n",
        "        return \"slow\"\n",
        "    elif bpm_val < 110:\n",
        "        return \"moderate\"\n",
        "    elif bpm_val < 160:\n",
        "        return \"fast\"\n",
        "    else:\n",
        "        return \"very fast\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db-scaffolding",
      "metadata": {},
      "source": [
        "## 3. Database Scaffolding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "code-database",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. DATABASE SCAFFOLDING\n",
        "\n",
        "class QuantumMusicDB:\n",
        "    \"\"\"\n",
        "    Handles connection to the PostgreSQL database and basic CRUD operations.\n",
        "    \"\"\"\n",
        "    def __init__(self, db_name=DB_NAME, host=DB_HOST, user=DB_USER, password=DB_PASSWORD):\n",
        "        self.db_name = db_name\n",
        "        self.host = host\n",
        "        self.user = user\n",
        "        self.password = password\n",
        "        self.conn = None\n",
        "        self.connect()\n",
        "\n",
        "    def connect(self):\n",
        "        try:\n",
        "            self.conn = psycopg2.connect(\n",
        "                dbname=self.db_name,\n",
        "                host=self.host,\n",
        "                user=self.user,\n",
        "                password=self.password\n",
        "            )\n",
        "            print(f\"Connected to database {self.db_name} successfully.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error connecting to database: {e}\")\n",
        "\n",
        "    def create_tables(self):\n",
        "        create_table_query = \"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS audio_analysis (\n",
        "            id SERIAL PRIMARY KEY,\n",
        "            file_name VARCHAR(255),\n",
        "            upload_date TIMESTAMP DEFAULT NOW(),\n",
        "            sample_rate INT,\n",
        "            analysis_data JSONB\n",
        "        );\n",
        "        \"\"\"\n",
        "        with self.conn.cursor() as cur:\n",
        "            cur.execute(create_table_query)\n",
        "            self.conn.commit()\n",
        "        print(\"Tables ensured.\")\n",
        "\n",
        "    def insert_analysis(self, file_name, sample_rate, analysis_data):\n",
        "        \"\"\"\n",
        "        Insert a new analysis record into the DB.\n",
        "        analysis_data is stored as a JSONB column using psycopg2.extras.Json.\n",
        "        \"\"\"\n",
        "        insert_query = \"\"\"\n",
        "        INSERT INTO audio_analysis(file_name, sample_rate, analysis_data)\n",
        "        VALUES (%s, %s, %s)\n",
        "        RETURNING id;\n",
        "        \"\"\"\n",
        "        with self.conn.cursor() as cur:\n",
        "            cur.execute(insert_query, (file_name, sample_rate, Json(analysis_data)))\n",
        "            new_id = cur.fetchone()[0]\n",
        "            self.conn.commit()\n",
        "        return new_id\n",
        "\n",
        "    def fetch_analysis(self, record_id):\n",
        "        \"\"\"\n",
        "        Fetch a specific analysis record by ID.\n",
        "        \"\"\"\n",
        "        select_query = \"\"\"\n",
        "        SELECT id, file_name, sample_rate, analysis_data\n",
        "        FROM audio_analysis\n",
        "        WHERE id=%s;\n",
        "        \"\"\"\n",
        "        with self.conn.cursor() as cur:\n",
        "            cur.execute(select_query, (record_id,))\n",
        "            row = cur.fetchone()\n",
        "        return row\n",
        "\n",
        "    def close(self):\n",
        "        if self.conn:\n",
        "            self.conn.close()\n",
        "            print(\"Database connection closed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "preprocessing-mod",
      "metadata": {},
      "source": [
        "## 3. Module 1: Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "preprocessing-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. MODULE 1: PREPROCESSING\n",
        "\n",
        "def preprocess_audio(file_path,\n",
        "                     target_sr=STANDARD_SR,\n",
        "                     silence_threshold_db=SILENCE_THRESHOLD_DB):\n",
        "    audio_data, sr = librosa.load(file_path, sr=None)\n",
        "    if sr != target_sr:\n",
        "        audio_data = librosa.resample(audio_data, orig_sr=sr, target_sr=target_sr)\n",
        "        sr = target_sr\n",
        "\n",
        "    peak = np.max(np.abs(audio_data))\n",
        "    if peak > 0:\n",
        "        audio_data = audio_data / peak\n",
        "\n",
        "    audio_data, _ = librosa.effects.trim(audio_data, top_db=silence_threshold_db)\n",
        "    return audio_data, sr\n",
        "\n",
        "\n",
        "\n",
        "def remove_percussive_components(audio_data, sr, margin=(1.0, 1.0)):\n",
        "    \"\"\"\n",
        "    Uses harmonic-percussive source separation (HPSS) to split audio into harmonic and percussive components.\n",
        "    By default, returns only the harmonic component (vocals, sustained instruments).\n",
        "    \n",
        "    :param audio_data: np.array containing the audio samples\n",
        "    :param sr: sampling rate\n",
        "    :param margin: A tuple specifying HPSS margin for harmonic/percussive. \n",
        "                   Larger margins can better isolate percussive vs. harmonic content, \n",
        "                   but it may remove more from the harmonic portion.\n",
        "    :return: np.array containing only the harmonic component\n",
        "    \"\"\"\n",
        "    # Perform HPSS with Librosa\n",
        "    # margin=(harmonic_margin, percussive_margin)\n",
        "    # Adjust margins if you have a strongly percussive track\n",
        "    harmonic_part, percussive_part = librosa.decompose.hpss(\n",
        "        librosa.stft(audio_data), \n",
        "        margin=margin\n",
        "    )\n",
        "    # Convert back from STFT to time-domain\n",
        "    harmonic_audio = librosa.istft(harmonic_part)\n",
        "    \n",
        "    return harmonic_audio\n",
        "\n",
        "\n",
        "def remove_drone(audio_data, sr, search_duration=2.0, q_factor=30.0):\n",
        "    \"\"\"\n",
        "    Identify and remove a strong background drone at a specific pitch.\n",
        "    \n",
        "    :param audio_data: 1D numpy array of the audio samples\n",
        "    :param sr: sampling rate\n",
        "    :param search_duration: length (in seconds) of audio to analyze for the drone freq\n",
        "    :param q_factor: notch filter Q-factor (sharpness). Higher => narrower notch.\n",
        "    :return: drone_removed_audio (numpy array)\n",
        "    \"\"\"\n",
        "    # 1) Analyze a short portion of the audio (up to search_duration seconds) \n",
        "    #    to estimate the main drone frequency.\n",
        "    max_samples = min(len(audio_data), int(search_duration * sr))\n",
        "    snippet = audio_data[:max_samples]\n",
        "\n",
        "    # 2) Compute an FFT-based power spectrum.\n",
        "    #    We'll find the frequency bin with the highest magnitude (excluding DC).\n",
        "    fft_spectrum = np.fft.rfft(snippet)\n",
        "    freqs = np.fft.rfftfreq(len(snippet), d=1.0/sr)\n",
        "    \n",
        "    # Exclude DC (freq=0) to avoid picking silence or offset as the drone\n",
        "    fft_spectrum[0] = 0.0  # zero out DC component\n",
        "    power_spectrum = np.abs(fft_spectrum)\n",
        "    \n",
        "    # 3) Identify the peak frequency\n",
        "    peak_index = np.argmax(power_spectrum)\n",
        "    drone_freq = freqs[peak_index]\n",
        "    print(f\"Identified drone frequency ~ {drone_freq:.2f} Hz\")\n",
        "    \n",
        "    # 4) Design a narrow notch filter around that frequency\n",
        "    # w0 = (target freq) / (nyquist freq)\n",
        "    nyquist = 0.5 * sr\n",
        "    w0 = drone_freq / nyquist\n",
        "    b, a = iirnotch(w0, Q=q_factor)  # scipy.signal.iirnotch\n",
        "    \n",
        "    # 5) Apply the notch filter to the entire audio using filtfilt\n",
        "    drone_removed = filtfilt(b, a, audio_data)\n",
        "    \n",
        "    return drone_removed\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "noise-mod",
      "metadata": {},
      "source": [
        "## 4. Module 2: Noise Identification & Reduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "noise-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. MODULE 2: NOISE IDENTIFICATION & REDUCTION\n",
        "\n",
        "def bandpass_filter(audio_data, sr, low_freq=LOW_FREQ, high_freq=HIGH_FREQ):\n",
        "    nyquist = 0.5 * sr\n",
        "    low = low_freq / nyquist\n",
        "    high = high_freq / nyquist\n",
        "    b, a = butter(N=4, Wn=[low, high], btype='band')\n",
        "    filtered_audio = filtfilt(b, a, audio_data)\n",
        "    return filtered_audio\n",
        "\n",
        "def simple_noise_reduction(audio_data, sr):\n",
        "    filtered = bandpass_filter(audio_data, sr)\n",
        "    return filtered"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pitch-mod",
      "metadata": {},
      "source": [
        "## 5. Module 3: Pitch Contour Extraction and Analysis\n",
        "\n",
        "### Key Points:\n",
        "- We define `extract_pitch_contour` to detect pitch frames with `librosa.pyin`.\n",
        "- We define `freq_to_closest_note` and `map_pitches_to_notes` to map a pitch to the nearest 12-tone note.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pitch-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "NOTE_NAMES = [\"C\", \"C#\", \"D\", \"D#\", \"E\", \"F\", \"F#\", \"G\", \"G#\", \"A\", \"A#\", \"B\"]\n",
        "\n",
        "def extract_pitch_contour(audio_data, sr):\n",
        "    \"\"\"\n",
        "    Uses librosa.pyin to extract pitch contour.\n",
        "    Returns (times, pitches, confidences).\n",
        "    \"\"\"\n",
        "    fmin = librosa.note_to_hz('C2')\n",
        "    fmax = librosa.note_to_hz('C7')\n",
        "    # Check if audio_data is stereo, if so, take the mean?\n",
        "    # TODO: Check if I want to take audio_data or harmonic_data here?\n",
        "    pitches, voiced_flags, confidences = librosa.pyin(\n",
        "        y=audio_data,\n",
        "        fmin=fmin,\n",
        "        fmax=fmax,\n",
        "        sr=sr,\n",
        "        frame_length=FRAME_SIZE,\n",
        "        hop_length=HOP_LENGTH\n",
        "    )\n",
        "    times = librosa.times_like(pitches, sr=sr, hop_length=HOP_LENGTH)\n",
        "    return times, pitches, confidences\n",
        "\n",
        "def freq_to_closest_note(freq):\n",
        "    \"\"\"\n",
        "    Convert freq in Hz to the nearest semitone and compute the difference in cents.\n",
        "    Returns (note_name, note_freq, deviation_cents) or (None,None,None) if invalid.\n",
        "    \"\"\"\n",
        "    if freq is None or freq <= 0 or np.isnan(freq):\n",
        "        return (None, None, None)\n",
        "    note_num = 69 + 12 * np.log2(freq / 440.0)\n",
        "    if np.isnan(note_num) or np.isinf(note_num):\n",
        "        return (None, None, None)\n",
        "    rounded_note = int(round(note_num))\n",
        "    closest_note_freq = 440.0 * (2.0 ** ((rounded_note - 69)/12.0))\n",
        "    deviation_cents = 1200 * np.log2(freq / closest_note_freq)\n",
        "    note_name = NOTE_NAMES[rounded_note % 12]\n",
        "    octave = (rounded_note // 12) - 1\n",
        "    full_note_name = f\"{note_name}{octave}\"\n",
        "    return (full_note_name, closest_note_freq, deviation_cents)\n",
        "\n",
        "def map_pitches_to_notes(pitches):\n",
        "    \"\"\"\n",
        "    For an array of pitch frames, map each to (note_name, note_freq, deviation_cents).\n",
        "    Return arrays: note_names, note_freqs, deviations.\n",
        "    \"\"\"\n",
        "    note_names = []\n",
        "    note_freqs = []\n",
        "    deviations = []\n",
        "    for p in pitches:\n",
        "        name, freq, dev = freq_to_closest_note(p)\n",
        "        note_names.append(name)\n",
        "        note_freqs.append(freq)\n",
        "        deviations.append(dev)\n",
        "    return note_names, note_freqs, deviations\n",
        "\n",
        "\n",
        "###########################################\n",
        "# 22 Śrutis Microtonal Classification\n",
        "###########################################\n",
        "\n",
        "###########################################\n",
        "# 22 Śrutis Microtonal Classification\n",
        "###########################################\n",
        "\n",
        "def classify_sruti_offset(offset_cents):\n",
        "    \"\"\"\n",
        "    Attempt to classify the pitch offset into one of 22 śrutis (approx).\n",
        "    We treat an octave as 1200 cents, split into 22 equal segments (~54.5454 cents each).\n",
        "\n",
        "    offset_cents can be negative or positive. We'll do a rough approach:\n",
        "      - shift offset range by +600 so that 0..1200 covers a full sruti scale\n",
        "      - clamp <0 to 0, >1200 to 1200\n",
        "      - compute sruti index = int(shifted / sruti_size)\n",
        "    \"\"\"\n",
        "    if offset_cents is None or np.isnan(offset_cents):\n",
        "        return \"sruti_unknown\"\n",
        "\n",
        "    sruti_size = 1200.0 / 22.0  # ~54.5454 cents per śruti\n",
        "    shifted = offset_cents + 600.0  # shift so -600 -> 0, +600 -> 1200\n",
        "\n",
        "    if shifted < 0:\n",
        "        shifted = 0\n",
        "    elif shifted > 1200:\n",
        "        shifted = 1200\n",
        "\n",
        "    sruti_index = int(shifted // sruti_size)\n",
        "    if sruti_index > 21:\n",
        "        sruti_index = 21\n",
        "\n",
        "    return f\"sruti_{sruti_index}\"\n",
        "\n",
        "\n",
        "def detect_microtone_offset_22sruti(freq, base_freq):\n",
        "    \"\"\"\n",
        "    Return (offset_cents, sruti_label).\n",
        "      offset_cents: how many cents above or below base_freq\n",
        "      sruti_label: the 22-sruti classification, e.g. 'sruti_0'..'sruti_21'\n",
        "    \"\"\"\n",
        "    if freq <= 0 or base_freq <= 0:\n",
        "        return (None, \"sruti_unknown\")\n",
        "\n",
        "    offset_cents = 1200.0 * np.log2(freq / base_freq)\n",
        "    sruti_label = classify_sruti_offset(offset_cents)\n",
        "    return (offset_cents, sruti_label)\n",
        "\n",
        "\n",
        "def map_pitches_to_notes_22sruti(pitches, tonic=240.0):\n",
        "    \"\"\"\n",
        "    Maps a list of pitch values (in Hz) to Indian musical note names and sruti labels.\n",
        "    \n",
        "    For each pitch:\n",
        "      - Compute the cents difference relative to the tonic.\n",
        "      - Wrap the cents into one octave (0–1200 cents).\n",
        "      - Quantize that range into 22 equal intervals.\n",
        "      - Use the quantized index to assign a sruti label (e.g. \"sruti_X\").\n",
        "      - Map the 22 intervals to the 7-note scale (Sa, Re, Ga, Ma, Pa, Dha, Ni).\n",
        "      - Compute the center frequency of the quantized bin.\n",
        "      - Compute the deviation (in cents) from the center.\n",
        "    \n",
        "    Returns four lists:\n",
        "      - note_names: The mapped note names (e.g. \"Sa\", \"Re\", etc.).\n",
        "      - note_freqs: The frequency corresponding to the center of the quantized note.\n",
        "      - deviations: The deviation (in cents) of the pitch from that center.\n",
        "      - sruti_labels: The raw sruti label (e.g. \"sruti_0\", \"sruti_1\", ..., \"sruti_21\").\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "\n",
        "    note_names = []\n",
        "    note_freqs = []\n",
        "    deviations = []\n",
        "    sruti_labels = []\n",
        "    \n",
        "    # Define the basic note names for the 7-note scale.\n",
        "    basic_notes = [\"Sa\", \"Re\", \"Ga\", \"Ma\", \"Pa\", \"Dha\", \"Ni\"]\n",
        "    \n",
        "    # Each sruti interval spans:\n",
        "    sruti_width = 1200 / 22.0  # approximately 54.55 cents\n",
        "    \n",
        "    for p in pitches:\n",
        "        # Check for invalid pitches: None, <= 0, or NaN.\n",
        "        if p is None or p <= 0 or np.isnan(p):\n",
        "            note_names.append(None)\n",
        "            note_freqs.append(None)\n",
        "            deviations.append(None)\n",
        "            sruti_labels.append(None)\n",
        "            continue\n",
        "        \n",
        "        # Compute the cents difference from the tonic.\n",
        "        cents_diff = 1200 * np.log2(p / tonic)\n",
        "        # Wrap the difference into one octave (0 to 1200 cents).\n",
        "        cents_diff_mod = cents_diff % 1200\n",
        "        \n",
        "        # Quantize into one of 22 sruti intervals.\n",
        "        sruti_index = int(round(cents_diff_mod / sruti_width)) % 22\n",
        "        sruti_label = f\"sruti_{sruti_index}\"\n",
        "        sruti_labels.append(sruti_label)\n",
        "        \n",
        "        # Map the 22 intervals to the 7-note scale via a simple linear mapping.\n",
        "        note_index = int(sruti_index * 7 / 22)\n",
        "        note_index = min(note_index, len(basic_notes) - 1)\n",
        "        note_name = basic_notes[note_index]\n",
        "        note_names.append(note_name)\n",
        "        \n",
        "        # Compute the center of the sruti bin (in cents).\n",
        "        center_cents = sruti_index * sruti_width\n",
        "        # Convert the center cents back to frequency.\n",
        "        center_freq = tonic * (2 ** (center_cents / 1200))\n",
        "        note_freqs.append(center_freq)\n",
        "        \n",
        "        # Compute the deviation in cents from the center.\n",
        "        deviation = cents_diff_mod - center_cents\n",
        "        if deviation > sruti_width / 2:\n",
        "            deviation -= 1200\n",
        "        elif deviation < -sruti_width / 2:\n",
        "            deviation += 1200\n",
        "        deviations.append(deviation)\n",
        "    \n",
        "    return note_names, note_freqs, deviations, sruti_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "praat-hnr",
      "metadata": {},
      "source": [
        "## 6. Utility for Praat HNR (larger chunk), plus separate time_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "praat-hnr-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_praat_hnr(audio_chunk, sr):\n",
        "    if len(audio_chunk) == 0:\n",
        "        return 0.0\n",
        "    required_length_sec = 3.0 / 86.1329  # ~0.0348 s\n",
        "    required_length_samples = int(required_length_sec * sr)\n",
        "    if len(audio_chunk) < required_length_samples:\n",
        "        return 0.0\n",
        "\n",
        "    try:\n",
        "        sound = parselmouth.Sound(audio_chunk, sr)\n",
        "        harmonicity = call(sound, \"To Harmonicity (cc)\", 0.01, 86.1329, 0.1, 1.0)\n",
        "        hnr = call(harmonicity, \"Get mean\", 0, 0)\n",
        "        if np.isnan(hnr) or np.isinf(hnr):\n",
        "            return 0.0\n",
        "        return float(hnr)\n",
        "    except:\n",
        "        return 0.0\n",
        "\n",
        "def build_praat_time_matrix(audio_data, sr):\n",
        "    matrix = []\n",
        "    num_samples = len(audio_data)\n",
        "    idx = 0\n",
        "    chunk_counter = 0\n",
        "    chunk_hop = PRAAT_CHUNK_SIZE\n",
        "\n",
        "    while idx < num_samples:\n",
        "        end = idx + chunk_hop\n",
        "        if end > num_samples:\n",
        "            end = num_samples\n",
        "        chunk_data = audio_data[idx:end]\n",
        "        start_time = idx / sr\n",
        "        hnr_value = compute_praat_hnr(chunk_data, sr)\n",
        "\n",
        "        matrix.append({\n",
        "            \"chunk_index\": chunk_counter,\n",
        "            \"start_time_s\": float(start_time),\n",
        "            \"praat_hnr\": hnr_value\n",
        "        })\n",
        "\n",
        "        idx += chunk_hop\n",
        "        chunk_counter += 1\n",
        "\n",
        "    return matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mod7-dynamics-again",
      "metadata": {},
      "source": [
        "## 7. Module 7: Dynamics (RMS & LUFS) with Larger Chunk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dynamics-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_rms_energy_advanced(audio_chunk):\n",
        "    if len(audio_chunk) == 0:\n",
        "        return 0.0\n",
        "    n_fft_local = min(len(audio_chunk), LUFS_CHUNK_SIZE)\n",
        "    rms = librosa.feature.rms(\n",
        "        y=audio_chunk,\n",
        "        frame_length=n_fft_local,\n",
        "        hop_length=n_fft_local\n",
        "    )\n",
        "    return float(rms.mean())\n",
        "\n",
        "def compute_lufs(audio_chunk, sr):\n",
        "    if not LOUDNORM_AVAILABLE:\n",
        "        return None\n",
        "    if len(audio_chunk) == 0:\n",
        "        return None\n",
        "\n",
        "    # Convert to mono if multi-channel\n",
        "    if audio_chunk.ndim > 1:\n",
        "        audio_chunk = np.mean(audio_chunk, axis=1)\n",
        "\n",
        "    # Make sure chunk is long enough for integrated loudness:\n",
        "    meter = pyln.Meter(sr)  # by default block_size=0.4\n",
        "    min_required = int(meter.block_size * sr)  # typically 0.4 * 44100 = 17640\n",
        "    if len(audio_chunk) < min_required:\n",
        "        return 0.0\n",
        "\n",
        "    loudness_val = meter.integrated_loudness(audio_chunk)\n",
        "    return float(loudness_val)\n",
        "\n",
        "def analyze_dynamics_module7(audio_data, sr, time_matrix):\n",
        "    \"\"\"\n",
        "    Now we use LUFS_CHUNK_SIZE (~0.5s) for integrated loudness.\n",
        "    For each row in time_matrix, we gather the start_time and read ~0.5s of audio.\n",
        "    Then store RMS(dB) and LUFS.\n",
        "    Also store summary in the final row.\n",
        "    \"\"\"\n",
        "    rms_values_db = []\n",
        "    lufs_values = []\n",
        "\n",
        "    for i, row in enumerate(time_matrix):\n",
        "        start_time = row['time_s']\n",
        "        start_sample = int(start_time * sr)\n",
        "        end_sample = start_sample + LUFS_CHUNK_SIZE\n",
        "        if end_sample > len(audio_data):\n",
        "            end_sample = len(audio_data)\n",
        "        chunk_data = audio_data[start_sample:end_sample]\n",
        "        # TODO: Grab harmonic data instead?\n",
        "\n",
        "        raw_rms = compute_rms_energy_advanced(chunk_data)\n",
        "        rms_db = 20.0 * np.log10(raw_rms + 1e-12)\n",
        "        row['rms_db'] = rms_db\n",
        "        ################################\n",
        "        # NEW: Store RMS category\n",
        "        ################################\n",
        "        row['rms_db_category'] = classify_rms_db(rms_db)\n",
        "        rms_values_db.append(rms_db)\n",
        "\n",
        "        lufs_val = compute_lufs(chunk_data, sr)\n",
        "        if lufs_val is None:\n",
        "            lufs_val = 0.0\n",
        "        row['lufs'] = lufs_val\n",
        "        ################################\n",
        "        # NEW: Store LUFS category\n",
        "        ################################\n",
        "        row['lufs_category'] = classify_lufs(lufs_val)\n",
        "\n",
        "        lufs_values.append(lufs_val)\n",
        "\n",
        "    # Summaries\n",
        "    valid_rms = [x for x in rms_values_db if not np.isnan(x) and not np.isinf(x)]\n",
        "    if valid_rms:\n",
        "        mean_rms = float(np.mean(valid_rms))\n",
        "        med_rms = float(np.median(valid_rms))\n",
        "        min_rms = float(np.min(valid_rms))\n",
        "        max_rms = float(np.max(valid_rms))\n",
        "        range_rms = max_rms - min_rms\n",
        "        std_rms = float(np.std(valid_rms))\n",
        "        dyn_range_rms = range_rms\n",
        "    else:\n",
        "        mean_rms = med_rms = min_rms = max_rms = range_rms = std_rms = dyn_range_rms = 0.0\n",
        "\n",
        "    valid_lufs = [x for x in lufs_values if not np.isnan(x) and not np.isinf(x)]\n",
        "    if valid_lufs:\n",
        "        mean_lufs = float(np.mean(valid_lufs))\n",
        "        med_lufs = float(np.median(valid_lufs))\n",
        "        min_lufs = float(np.min(valid_lufs))\n",
        "        max_lufs = float(np.max(valid_lufs))\n",
        "        range_lufs = max_lufs - min_lufs\n",
        "        std_lufs = float(np.std(valid_lufs))\n",
        "        dyn_range_lufs = range_lufs\n",
        "    else:\n",
        "        mean_lufs = med_lufs = min_lufs = max_lufs = range_lufs = std_lufs = dyn_range_lufs = 0.0\n",
        "\n",
        "    dyn_summary = {\n",
        "        'rms_db': {\n",
        "            'mean': mean_rms,\n",
        "            'median': med_rms,\n",
        "            'min': min_rms,\n",
        "            'max': max_rms,\n",
        "            'range': range_rms,\n",
        "            'std': std_rms,\n",
        "            'dynamic_range': dyn_range_rms\n",
        "        },\n",
        "        'lufs': {\n",
        "            'mean': mean_lufs,\n",
        "            'median': med_lufs,\n",
        "            'min': min_lufs,\n",
        "            'max': max_lufs,\n",
        "            'range': range_lufs,\n",
        "            'std': std_lufs,\n",
        "            'dynamic_range': dyn_range_lufs\n",
        "        }\n",
        "    }\n",
        "\n",
        "    if time_matrix:\n",
        "        time_matrix[-1]['dynamics_summary'] = dyn_summary\n",
        "\n",
        "    return time_matrix, dyn_summary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8-analyze-transitions",
      "metadata": {},
      "source": [
        "## 8. Normal time_matrix analysis (tone_to_noise, transitions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "transitions-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_spectral_flatness_chunk(audio_chunk, sr):\n",
        "    if len(audio_chunk) < 2:\n",
        "        return 0.0\n",
        "    # Short-chunk fix:\n",
        "    n_fft_local = min(len(audio_chunk), FRAME_SIZE)\n",
        "    sf = librosa.feature.spectral_flatness(\n",
        "        y=audio_chunk,\n",
        "        n_fft=n_fft_local,\n",
        "        hop_length=HOP_LENGTH\n",
        "    )\n",
        "    return float(np.mean(sf))\n",
        "\n",
        "def analyze_note_transitions(audio_data, sr, time_matrix):\n",
        "    for i, row in enumerate(time_matrix):\n",
        "        start_time = row['time_s']\n",
        "        start_sample = int(start_time * sr)\n",
        "        end_sample = start_sample + HOP_LENGTH\n",
        "        if end_sample > len(audio_data):\n",
        "            end_sample = len(audio_data)\n",
        "        chunk_data = audio_data[start_sample:end_sample]\n",
        "\n",
        "        if len(chunk_data) == 0:\n",
        "            # skip or set default\n",
        "            row['tone_to_noise'] = 0.0\n",
        "            row['tone_to_noise_category'] = \"unknown\"\n",
        "            row['transition_score'] = 0.5\n",
        "            row['transition_category'] = \"moderate\"\n",
        "            continue\n",
        "\n",
        "        # tone_to_noise => spectral flatness\n",
        "        sf_val = compute_spectral_flatness_chunk(chunk_data, sr)\n",
        "        row['tone_to_noise'] = sf_val\n",
        "        ################################\n",
        "        # NEW: Store tone category\n",
        "        ################################\n",
        "        row['tone_to_noise_category'] = classify_tone_to_noise(sf_val)\n",
        "\n",
        "\n",
        "        # transition_score => pitch difference\n",
        "        if i == 0:\n",
        "            transition_score = 1.0\n",
        "        else:\n",
        "            prev_pitch = time_matrix[i-1].get('pitch_hz', None)\n",
        "            curr_pitch = row.get('pitch_hz', None)\n",
        "            if not prev_pitch or not curr_pitch or prev_pitch <= 0 or curr_pitch <= 0:\n",
        "                transition_score = 0.5\n",
        "            else:\n",
        "                diff_cents = 1200.0 * np.log2(curr_pitch / prev_pitch)\n",
        "                diff_abs = abs(diff_cents)\n",
        "                transition_score = max(0.0, 1.0 - diff_abs/200.0)\n",
        "        row['transition_score'] = transition_score\n",
        "        ################################\n",
        "        # NEW: Store transition category\n",
        "        ################################\n",
        "        row['transition_category'] = classify_transition_score(transition_score)\n",
        "\n",
        "\n",
        "    return time_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9-spectrogram",
      "metadata": {},
      "source": [
        "## 9. Spectrogram Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "spectrogram-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_zero_crossing_rate(audio_chunk):\n",
        "    if len(audio_chunk) == 0:\n",
        "        return 0.0\n",
        "    # Short-chunk fix:\n",
        "    n_fft_local = min(len(audio_chunk), FRAME_SIZE)\n",
        "    zcr = librosa.feature.zero_crossing_rate(\n",
        "        y=audio_chunk,\n",
        "        frame_length=n_fft_local,\n",
        "        hop_length=HOP_LENGTH\n",
        "    )\n",
        "    return float(zcr.mean())\n",
        "\n",
        "def compute_spectral_centroid_advanced(audio_chunk, sr):\n",
        "    if len(audio_chunk) == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    # If the chunk is shorter than FRAME_SIZE, pad it with zeros.\n",
        "    if len(audio_chunk) < FRAME_SIZE:\n",
        "        padding_needed = FRAME_SIZE - len(audio_chunk)\n",
        "        audio_chunk = np.pad(audio_chunk, (0, padding_needed), mode='constant')\n",
        "\n",
        "    n_fft_local = min(len(audio_chunk), FRAME_SIZE)\n",
        "    c = librosa.feature.spectral_centroid(\n",
        "        y=audio_chunk,\n",
        "        sr=sr,\n",
        "        n_fft=n_fft_local,\n",
        "        hop_length=HOP_LENGTH\n",
        "    )\n",
        "    return float(c.mean())\n",
        "\n",
        "def compute_spectral_rolloff_advanced(audio_chunk, sr):\n",
        "\n",
        "    if audio_chunk is None or len(audio_chunk) == 0:\n",
        "        return 0.0\n",
        "    # If the chunk is too short, you can either skip it or pad it.\n",
        "    if len(audio_chunk) < FRAME_SIZE:\n",
        "        # Option A: Return a default value (skip processing)\n",
        "        # return 0.0\n",
        "\n",
        "        # Option B: Pad the chunk so that it meets the minimum length requirement.\n",
        "        padding_needed = FRAME_SIZE - len(audio_chunk)\n",
        "        audio_chunk = np.pad(audio_chunk, (0, padding_needed), mode='constant')\n",
        "\n",
        "\n",
        "    n_fft_local = min(len(audio_chunk), FRAME_SIZE)\n",
        "    r = librosa.feature.spectral_rolloff(\n",
        "        y=audio_chunk,\n",
        "        sr=sr,\n",
        "        n_fft=n_fft_local,\n",
        "        hop_length=HOP_LENGTH\n",
        "    )\n",
        "    return float(r.mean())\n",
        "\n",
        "def compute_spectral_bandwidth_advanced(audio_chunk, sr):\n",
        "    if audio_chunk is None or len(audio_chunk) == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    if len(audio_chunk) < FRAME_SIZE:\n",
        "        # Option A: Return a default value (skip processing)\n",
        "        # return 0.0\n",
        "\n",
        "        # Option B: Pad the chunk so that it meets the minimum length requirement.\n",
        "        padding_needed = FRAME_SIZE - len(audio_chunk)\n",
        "        audio_chunk = np.pad(audio_chunk, (0, padding_needed), mode='constant')\n",
        "\n",
        "    n_fft_local = min(len(audio_chunk), FRAME_SIZE)\n",
        "    bw = librosa.feature.spectral_bandwidth(\n",
        "        y=audio_chunk,\n",
        "        sr=sr,\n",
        "        n_fft=n_fft_local,\n",
        "        hop_length=HOP_LENGTH\n",
        "    )\n",
        "    return float(bw.mean())\n",
        "\n",
        "def compute_mfccs_advanced(audio_chunk, sr, n_mfcc=13):\n",
        "    if audio_chunk is None or len(audio_chunk) == 0:\n",
        "        return [0.0]*n_mfcc    \n",
        "  \n",
        "    # If the audio chunk is too short, pad it with zeros so that it has at least FRAME_SIZE samples\n",
        "    if len(audio_chunk) < FRAME_SIZE:\n",
        "        padding_needed = FRAME_SIZE - len(audio_chunk)\n",
        "        audio_chunk = np.pad(audio_chunk, (0, padding_needed), mode='constant')\n",
        "  \n",
        "    n_fft_local = min(len(audio_chunk), FRAME_SIZE)\n",
        "    mfcc_data = librosa.feature.mfcc(\n",
        "        y=audio_chunk,\n",
        "        sr=sr,\n",
        "        n_mfcc=n_mfcc,\n",
        "        n_fft=n_fft_local,\n",
        "        hop_length=HOP_LENGTH,\n",
        "        fmax=sr/2\n",
        "    )\n",
        "    return mfcc_data.mean(axis=1).tolist()\n",
        "\n",
        "def compute_chroma_advanced(audio_chunk, sr):\n",
        "    if audio_chunk is None or len(audio_chunk) == 0:\n",
        "        return [0.0]*12\n",
        "    \n",
        "    # If the chunk is too short for a single frame, pad it with zeros.\n",
        "    if len(audio_chunk) < FRAME_SIZE:\n",
        "        padding_needed = FRAME_SIZE - len(audio_chunk)\n",
        "        audio_chunk = np.pad(audio_chunk, (0, padding_needed), mode='constant')\n",
        "    \n",
        " \n",
        "    n_fft_local = min(len(audio_chunk), FRAME_SIZE)\n",
        "    c = librosa.feature.chroma_stft(\n",
        "        y=audio_chunk,\n",
        "        sr=sr,\n",
        "        n_fft=n_fft_local,\n",
        "        hop_length=HOP_LENGTH\n",
        "    )\n",
        "    return c.mean(axis=1).tolist()\n",
        "\n",
        "\n",
        "\n",
        "def update_time_matrix_small_with_advanced_features(clean_audio, sr, time_matrix_small):\n",
        "    \"\"\"\n",
        "    For each row in time_matrix_small (which currently are 512-hop chunks), compute overlapping advanced vocal\n",
        "    features on windows of length VOCAL_FEATURE_CHUNK_SIZE, with a hop of VOCAL_FEATURE_CHUNK_HOP.\n",
        "    \n",
        "    Then, for each row in time_matrix_small, find the overlapping advanced feature (computed on the larger window)\n",
        "    whose start time is closest to the row's time and update the row with those advanced metrics.\n",
        "    \n",
        "    Returns:\n",
        "        The updated time_matrix_small, where each row now includes keys such as 'jitter', 'shimmer',\n",
        "        'formants', 'vibrato_extent', 'vibrato_rate', 'asd_details', 'onset_frames', and 'notes'.\n",
        "    \"\"\"\n",
        "    # First, compute overlapping advanced features for the entire audio.\n",
        "    advanced_features_list = []\n",
        "    total_samples = len(clean_audio)\n",
        "    for start_sample in range(0, total_samples - VOCAL_FEATURE_CHUNK_SIZE + 1, VOCAL_FEATURE_CHUNK_HOP):\n",
        "        end_sample = start_sample + VOCAL_FEATURE_CHUNK_SIZE\n",
        "        chunk_data = clean_audio[start_sample:end_sample]\n",
        "        # Compute advanced features on this overlapping chunk.\n",
        "        adv_features = compute_advanced_vocal_features_chunk(chunk_data, sr)\n",
        "        # Record the chunk's start time (in seconds)\n",
        "        adv_features['time_s'] = start_sample / sr\n",
        "        advanced_features_list.append(adv_features)\n",
        "    \n",
        "    # Now, update each row in time_matrix_small.\n",
        "    # For each row (which has a key 'time_s' indicating its start time),\n",
        "    # find the advanced features dictionary whose 'time_s' is closest.\n",
        "    for row in time_matrix_small:\n",
        "        row_time = row['time_s']\n",
        "        # Find the overlapping chunk whose start time is closest to the row's time.\n",
        "        closest = min(advanced_features_list, key=lambda x: abs(x['time_s'] - row_time))\n",
        "        # Update the row with the advanced features from the closest overlapping chunk.\n",
        "        row.update(closest)\n",
        "    \n",
        "    return time_matrix_small"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d2d27df",
      "metadata": {},
      "source": [
        "## 9.5: Advanced features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54c8b2d4",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_jitter_shimmer(audio_data, sr):\n",
        "    \"\"\"\n",
        "    Compute jitter (local) and shimmer (local) on the provided audio array using Praat.\n",
        "    Returns a tuple: (jitter_local, shimmer_local)\n",
        "    \"\"\"\n",
        "    snd = parselmouth.Sound(audio_data, sr)\n",
        "    point_process = call(snd, \"To PointProcess (periodic, cc)\", MIN_F0, MAX_F0)\n",
        "    jitter_local = call(point_process, \"Get jitter (local)\",\n",
        "                          0, 0, JITTER_TIME_STEP, JITTER_MIN_PERIOD, JITTER_MAX_PERIOD)\n",
        "    shimmer_local = call([snd, point_process], \"Get shimmer (local)\",\n",
        "                           0, 0, SHIMMER_MIN_AMPLITUDE, SHIMMER_MAX_AMPLITUDE,\n",
        "                           JITTER_MAX_PERIOD, SHIMMER_FACTOR)\n",
        "    return jitter_local, shimmer_local\n",
        "\n",
        "def compute_formants(audio_data, sr, time=FORMANT_ANALYSIS_TIME, num_formants=NUM_FORMANTS_TO_EXTRACT):\n",
        "    \"\"\"\n",
        "    Extract the first 'num_formants' formant frequencies from the provided audio chunk at a given time.\n",
        "    Returns a dictionary (e.g., {\"F1\": ..., \"F2\": ..., \"F3\": ...}).\n",
        "    \"\"\"\n",
        "    snd = parselmouth.Sound(audio_data, sr)\n",
        "    formant = snd.to_formant_burg(time_step=FORMANT_TIME_STEP,\n",
        "                                  max_number_of_formants=MAX_NUMBER_OF_FORMANTS,\n",
        "                                  maximum_formant=MAXIMUM_FORMANT_FREQUENCY)\n",
        "    extracted = {}\n",
        "    for i in range(1, num_formants+1):\n",
        "        extracted[f\"F{i}\"] = formant.get_value_at_time(i, time)\n",
        "    return extracted\n",
        "\n",
        "def compute_vibrato(audio_data, sr):\n",
        "    \"\"\"\n",
        "    Estimate vibrato extent (std of pitch deviations) and vibrato rate (dominant frequency)\n",
        "    from the pitch contour computed on the provided audio chunk.\n",
        "    Returns (vibrato_extent, vibrato_rate)\n",
        "    \"\"\"\n",
        "    f0, _, _ = librosa.pyin(audio_data, fmin=MIN_F0, fmax=MAX_F0, sr=sr)\n",
        "    valid = ~np.isnan(f0)\n",
        "    if np.sum(valid) == 0:\n",
        "        return None, None\n",
        "    f0 = f0[valid]\n",
        "    smoothed = scipy.signal.medfilt(f0, kernel_size=MEDIAN_FILTER_KERNEL_SIZE)\n",
        "    vibrato_extent = np.std(f0 - smoothed)\n",
        "    deviation = f0 - smoothed\n",
        "    deviation -= np.mean(deviation)\n",
        "    fft_vals = np.fft.rfft(deviation)\n",
        "    freqs = np.fft.rfftfreq(len(deviation), d=HOP_LENGTH/sr)\n",
        "    band = (freqs > VIBRATO_MIN_HZ) & (freqs < VIBRATO_MAX_HZ)\n",
        "    if not np.any(band):\n",
        "        vibrato_rate = None\n",
        "    else:\n",
        "        idx = np.argmax(np.abs(fft_vals[band]))\n",
        "        vibrato_rate = freqs[band][idx]\n",
        "    return vibrato_extent, vibrato_rate\n",
        "\n",
        "def compute_asd(audio_data, sr):\n",
        "    \"\"\"\n",
        "    Compute the RMS envelope and detect onsets for the audio chunk.\n",
        "    Returns (times, rms, onset_frames)\n",
        "    \"\"\"\n",
        "    rms = librosa.feature.rms(y=audio_data, frame_length=FRAME_SIZE, hop_length=HOP_LENGTH)[0]\n",
        "    times = librosa.frames_to_time(np.arange(len(rms)), sr=sr, hop_length=HOP_LENGTH)\n",
        "    onset_frames = librosa.onset.onset_detect(y=audio_data, sr=sr, hop_length=HOP_LENGTH)\n",
        "    return times, rms, onset_frames\n",
        "\n",
        "def segment_note_asd(audio_data, sr, onset_frame, offset_frame):\n",
        "    \"\"\"\n",
        "    For a given note segment (from onset_frame to offset_frame in a chunk), compute:\n",
        "      - Attack time (time to reach peak RMS)\n",
        "      - Sustain duration (duration where RMS >= 90% of peak)\n",
        "      - Decay duration (time from end of sustain to note end)\n",
        "    Returns (attack, sustain, decay)\n",
        "    \"\"\"\n",
        "    note_signal = audio_data[onset_frame * HOP_LENGTH : offset_frame * HOP_LENGTH]\n",
        "    rms = librosa.feature.rms(y=note_signal, frame_length=FRAME_SIZE, hop_length=HOP_LENGTH)[0]\n",
        "    times = librosa.frames_to_time(np.arange(len(rms)), sr=sr, hop_length=HOP_LENGTH)\n",
        "    if len(rms) == 0:\n",
        "        return None, None, None\n",
        "    peak = np.argmax(rms)\n",
        "    attack = times[peak]\n",
        "    sustain_frames = np.where(rms >= 0.9 * rms[peak])[0]\n",
        "    sustain = times[sustain_frames[-1]] - times[sustain_frames[0]] if sustain_frames.size > 0 else 0\n",
        "    decay = times[-1] - times[sustain_frames[-1]] if sustain_frames.size > 0 else 0\n",
        "    return attack, sustain, decay\n",
        "\n",
        "def detect_notes(audio_data, sr):\n",
        "    \"\"\"\n",
        "    Detect onsets and segment the audio chunk into notes.\n",
        "    Returns a list of (start_time, end_time) tuples for the chunk.\n",
        "    \"\"\"\n",
        "    onset_frames = librosa.onset.onset_detect(y=audio_data, sr=sr, hop_length=HOP_LENGTH)\n",
        "    onset_times = librosa.frames_to_time(onset_frames, sr=sr, hop_length=HOP_LENGTH)\n",
        "    notes = []\n",
        "    for i, onset in enumerate(onset_times):\n",
        "        start = onset\n",
        "        end = onset_times[i+1] if i < len(onset_times)-1 else len(audio_data)/sr\n",
        "        notes.append((start, end))\n",
        "    return notes\n",
        "\n",
        "def compute_advanced_vocal_features_chunk(audio_chunk, sr):\n",
        "    \"\"\"\n",
        "    Wrapper: Compute all advanced vocal features on a given audio chunk.\n",
        "    Returns a dictionary containing:\n",
        "      - jitter, shimmer,\n",
        "      - formants (dict with keys F1, F2, F3),\n",
        "      - vibrato_extent, vibrato_rate,\n",
        "      - asd_details (list per detected note),\n",
        "      - onset_frames, and note segmentation (notes)\n",
        "    If the chunk is too short, it is padded.\n",
        "    \"\"\"\n",
        "    if audio_chunk is None or len(audio_chunk) == 0:\n",
        "        return {\n",
        "            \"jitter\": 0.0,\n",
        "            \"shimmer\": 0.0,\n",
        "            \"formants\": {\"F1\": 0.0, \"F2\": 0.0, \"F3\": 0.0},\n",
        "            \"vibrato_extent\": 0.0,\n",
        "            \"vibrato_rate\": 0.0,\n",
        "            \"asd_details\": [],\n",
        "            \"onset_frames\": [],\n",
        "            \"notes\": []\n",
        "        }\n",
        "    # Pad if shorter than VOCAL_FEATURE_CHUNK_SIZE\n",
        "    if len(audio_chunk) < VOCAL_FEATURE_CHUNK_SIZE:\n",
        "        audio_chunk = np.pad(audio_chunk, (0, VOCAL_FEATURE_CHUNK_SIZE - len(audio_chunk)), mode='constant')\n",
        "    \n",
        "    jitter, shimmer = compute_jitter_shimmer(audio_chunk, sr)\n",
        "    formants = compute_formants(audio_chunk, sr)\n",
        "    vibrato_extent, vibrato_rate = compute_vibrato(audio_chunk, sr)\n",
        "    times_asd, rms_asd, onset_frames = compute_asd(audio_chunk, sr)\n",
        "    notes = detect_notes(audio_chunk, sr)\n",
        "    \n",
        "    asd_details = []\n",
        "    for i in range(len(onset_frames)):\n",
        "        onset_frame = onset_frames[i]\n",
        "        offset_frame = onset_frames[i+1] if i < len(onset_frames)-1 else len(rms_asd)-1\n",
        "        attack, sustain, decay = segment_note_asd(audio_chunk, sr, onset_frame, offset_frame)\n",
        "        asd_details.append({\n",
        "            \"attack\": attack,\n",
        "            \"sustain\": sustain,\n",
        "            \"decay\": decay\n",
        "        })\n",
        "    \n",
        "    # At the end of compute_advanced_vocal_features_chunk (for debugging)\n",
        "    features = {\n",
        "        \"jitter\": jitter,\n",
        "        \"shimmer\": shimmer,\n",
        "        \"formants\": formants,\n",
        "        \"vibrato_extent\": vibrato_extent,\n",
        "        \"vibrato_rate\": vibrato_rate,\n",
        "        \"asd_details\": asd_details,\n",
        "        \"onset_frames\": onset_frames.tolist() if isinstance(onset_frames, np.ndarray) else onset_frames,\n",
        "        \"notes\": notes\n",
        "    }\n",
        "    #print(\"Advanced features for chunk:\", features)\n",
        "    return features\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def update_note_transition_analysis_with_advanced_features(clean_audio, sr, note_segments):\n",
        "    \"\"\"\n",
        "    For each note segment (provided as a list of (start_time, end_time) tuples),\n",
        "    compute the advanced vocal features (averaged over the note) and add these\n",
        "    to the note transition analysis dictionary.\n",
        "    \n",
        "    Returns a list of dictionaries, one per note segment.\n",
        "    \"\"\"\n",
        "    advanced_note_metrics = []\n",
        "    for (note_start, note_end) in note_segments:\n",
        "        start_sample = int(note_start * sr)\n",
        "        end_sample = int(note_end * sr)\n",
        "        note_audio = clean_audio[start_sample:end_sample]\n",
        "        # Use the same advanced features functions on the note segment\n",
        "        note_adv = compute_advanced_vocal_features_chunk(note_audio, sr)\n",
        "        # You might also want to compute note-level averages from the ASD details.\n",
        "        advanced_note_metrics.append(note_adv)\n",
        "    return advanced_note_metrics\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mod10-tempo",
      "metadata": {},
      "source": [
        "## 10. Tempo Adherence (512‐hop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tempo512-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_tempo_adherence(clean_audio, sr, time_matrix_small):\n",
        "    \"\"\"\n",
        "    For each chunk in time_matrix_small, estimate local tempo (in BPM)\n",
        "    and store it as row['tempo_bpm']. Uses 512-sample frames.\n",
        "    \"\"\"\n",
        "    import librosa\n",
        "    from librosa.feature import rhythm\n",
        "\n",
        "    for i, row in enumerate(time_matrix_small):\n",
        "        start_time = row['time_s']\n",
        "        start_sample = int(start_time * sr)\n",
        "        end_sample = start_sample + HOP_LENGTH\n",
        "        if end_sample > len(clean_audio):\n",
        "            end_sample = len(clean_audio)\n",
        "        chunk_data = clean_audio[start_sample:end_sample]\n",
        "\n",
        "        if len(chunk_data) == 0 or not np.any(chunk_data):\n",
        "            row['tempo_bpm'] = 0.0\n",
        "            ################################\n",
        "            # NEW: Store tempo category\n",
        "            ################################\n",
        "            row['tempo_bpm_category'] = \"unknown\"\n",
        "            continue\n",
        "\n",
        "        local_tempo = rhythm.tempo(\n",
        "            y=chunk_data,\n",
        "            sr=sr,\n",
        "            hop_length=HOP_LENGTH,\n",
        "            aggregate=None\n",
        "        )\n",
        "        if (local_tempo is None) or (len(local_tempo) == 0):\n",
        "            row['tempo_bpm'] = 0.0\n",
        "            row['tempo_bpm_category'] = \"unknown\"\n",
        "        else:\n",
        "            ################################\n",
        "            # NEW: Store tempo category\n",
        "            ################################\n",
        "            tempo_val = float(np.mean(local_tempo))\n",
        "            row['tempo_bpm'] = tempo_val\n",
        "            row['tempo_bpm_category'] = classify_tempo_bpm(tempo_val)\n",
        "\n",
        "\n",
        "    return time_matrix_small"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mod10b-tempo",
      "metadata": {},
      "source": [
        "## 10b. Additional Multi‐chunk Tempo (4096, 16384)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tempo-larger-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_larger_tempo_matrix(clean_audio, sr, chunk_size=4096, overlap=0.5):\n",
        "    \"\"\"\n",
        "    Build a separate time matrix for tempo analysis using a larger chunk.\n",
        "    e.g. chunk_size=4096 or 16384, with default 50% overlap => hop=chunk_size/2.\n",
        "    \"\"\"\n",
        "    import librosa\n",
        "    from librosa.feature import rhythm\n",
        "\n",
        "    matrix = []\n",
        "    num_samples = len(clean_audio)\n",
        "    chunk_hop = int(chunk_size * (1.0 - overlap))\n",
        "    idx = 0\n",
        "    chunk_counter = 0\n",
        "\n",
        "    while idx < num_samples:\n",
        "        end = idx + chunk_size\n",
        "        if end > num_samples:\n",
        "            end = num_samples\n",
        "        chunk_data = clean_audio[idx:end]\n",
        "        start_time = idx / sr\n",
        "\n",
        "        if len(chunk_data) == 0 or not np.any(chunk_data):\n",
        "            tempo_bpm = 0.0\n",
        "            tempo_bpm_category = \"unknown\"\n",
        "\n",
        "        else:\n",
        "            local_tempo = rhythm.tempo(\n",
        "                y=chunk_data,\n",
        "                sr=sr,\n",
        "                hop_length=HOP_LENGTH,\n",
        "                aggregate=None\n",
        "            )\n",
        "            if (local_tempo is None) or (len(local_tempo) == 0):\n",
        "                tempo_bpm = 0.0\n",
        "                tempo_bpm_category = \"unknown\"\n",
        "\n",
        "            else:\n",
        "                tempo_bpm = float(np.mean(local_tempo))\n",
        "                ################################\n",
        "                # NEW: Store tempo category\n",
        "                ################################\n",
        "                tempo_bpm_category = classify_tempo_bpm(tempo_bpm)\n",
        "\n",
        "\n",
        "        matrix.append({\n",
        "            \"chunk_index\": chunk_counter,\n",
        "            \"start_time_s\": float(start_time),\n",
        "            \"tempo_bpm\": tempo_bpm,\n",
        "            \"tempo_bpm_category\": tempo_bpm_category  # <--- NEW\n",
        "\n",
        "        })\n",
        "\n",
        "        idx += chunk_hop\n",
        "        chunk_counter += 1\n",
        "\n",
        "    return matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95121eae",
      "metadata": {},
      "source": [
        "## 18 - Raga Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68728036",
      "metadata": {},
      "outputs": [],
      "source": [
        "raga_db = [\n",
        "    {\n",
        "        \"name\": \"Yaman\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"N3\", \"R2\", \"G2\", \"M2\", \"P\", \"D2\", \"N3\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"N3\", \"D2\", \"P\", \"M2\", \"G2\", \"R2\", \"S\"]\n",
        "        },\n",
        "        \"western_scale\": {\n",
        "            \"aaroh\": [\"B3\", \"D4\", \"E4\", \"F#4\", \"G4\", \"A4\", \"B4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"B4\", \"A4\", \"G4\", \"F#4\", \"E4\", \"D4\", \"B3\"]\n",
        "        },\n",
        "        \"vaadi\": \"G2\",\n",
        "        \"samvaadi\": \"N3\",\n",
        "        \"typical_phrases\": [\n",
        "            \"N3 R2 G2 M2 G2 R2 S\",\n",
        "            \"R2 G2 R2 S\",\n",
        "            \"G2 M2 D2 N3 S'\",\n",
        "            \"M2 G2 R2 S\",\n",
        "            \"N3 D2 P M2 G2\",\n",
        "            \"N3 R2 S\",\n",
        "            \"R2 G2 M2 P D2 N3 S'\",\n",
        "            \"M2 G2 R2 S N3\",\n",
        "            \"R2 S N3 D2 P\",\n",
        "            \"G2 M2 D2 N3 R2 S\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"R2 from S with a gentle glide\", \n",
        "            \"G2 from R2 with slight kampan\",\n",
        "            \"M2 from G2 with a meend\"\n",
        "        ],\n",
        "        \"skipped_swars\": [],\n",
        "        \"microtonal_nuances\": \"Tivra Ma (M2) is emphasized; Ni is shuddha. Subtle meends on Re-Ga.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Bhairav\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"S\", \"r1\", \"G2\", \"M1\", \"P\", \"d1\", \"N2\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"N2\", \"d1\", \"P\", \"M1\", \"G2\", \"r1\", \"S\"]\n",
        "        },\n",
        "        \"western_scale\": {\n",
        "            \"aaroh\": [\"C4\", \"Db4\", \"E4\", \"F4\", \"G4\", \"Ab4\", \"Bb4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"Bb4\", \"Ab4\", \"G4\", \"F4\", \"E4\", \"Db4\", \"C4\"]\n",
        "        },\n",
        "        \"vaadi\": \"d1\",\n",
        "        \"samvaadi\": \"r1\",\n",
        "        \"typical_phrases\": [\n",
        "            \"S r1 G2 M1 P\",\n",
        "            \"r1 G2 M1 P d1 N2 S'\",\n",
        "            \"S' N2 d1 P M1 G2 r1 S\",\n",
        "            \"M1 G2 r1 S\",\n",
        "            \"r1 G2 M1\",\n",
        "            \"G2 r1 S r1\",\n",
        "            \"P d1 N2 S'\",\n",
        "            \"S' N2 d1 P\",\n",
        "            \"r1 S r1 G2\",\n",
        "            \"M1 G2 r1 S\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"r1 from S with a slow andolan\",\n",
        "            \"d1 from P with a slight andolan\"\n",
        "        ],\n",
        "        \"skipped_swars\": [],\n",
        "        \"microtonal_nuances\": \"Komal Re (r1) and Komal Dha (d1) each with andolan; Ga is shuddha.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Todi\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"S\", \"r1\", \"g1\", \"M2\", \"P\", \"d1\", \"N2\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"N2\", \"d1\", \"P\", \"M2\", \"g1\", \"r1\", \"S\"]\n",
        "        },\n",
        "        \"western_scale\": {\n",
        "            \"aaroh\": [\"C4\", \"Db4\", \"Eb4\", \"F#4\", \"G4\", \"Ab4\", \"Bb4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"Bb4\", \"Ab4\", \"G4\", \"F#4\", \"Eb4\", \"Db4\", \"C4\"]\n",
        "        },\n",
        "        \"vaadi\": \"r1\",\n",
        "        \"samvaadi\": \"g1\",\n",
        "        \"typical_phrases\": [\n",
        "            \"S r1 g1 M2 P d1 N2 S'\",\n",
        "            \"M2 g1 r1 S\",\n",
        "            \"r1 g1 M2 d1 N2 S'\",\n",
        "            \"N2 d1 P M2 g1 r1 S\",\n",
        "            \"P M2 g1 r1\",\n",
        "            \"g1 M2 d1 N2 S'\",\n",
        "            \"r1 S N2 d1 P\",\n",
        "            \"g1 r1 S\",\n",
        "            \"r1 g1 M2 P\",\n",
        "            \"d1 N2 S'\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"r1 is approached with delicate andolan\",\n",
        "            \"g1 from r1 with slow glide\"\n",
        "        ],\n",
        "        \"skipped_swars\": [],\n",
        "        \"microtonal_nuances\": \"Komal Re, Ga, Tivra Ma; Re and Ga often have slow glides.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Bilawal\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"S\", \"R2\", \"G2\", \"M1\", \"P\", \"D2\", \"N2\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"N2\", \"D2\", \"P\", \"M1\", \"G2\", \"R2\", \"S\"]\n",
        "        },\n",
        "        \"western_scale\": {\n",
        "            \"aaroh\": [\"C4\", \"D4\", \"E4\", \"F4\", \"G4\", \"A4\", \"B4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"B4\", \"A4\", \"G4\", \"F4\", \"E4\", \"D4\", \"C4\"]\n",
        "        },\n",
        "        \"vaadi\": \"D2\",\n",
        "        \"samvaadi\": \"G2\",\n",
        "        \"typical_phrases\": [\n",
        "            \"S R2 G2 M1 P D2 N2 S'\",\n",
        "            \"P D2 N2 S'\",\n",
        "            \"G2 R2 S\",\n",
        "            \"R2 G2 M1 G2 R2 S\",\n",
        "            \"N2 D2 P M1 G2 R2 S\",\n",
        "            \"M1 P D2 N2 S'\",\n",
        "            \"S' N2 D2 P\",\n",
        "            \"G2 M1 G2 R2 S\",\n",
        "            \"D2 P M1 G2\",\n",
        "            \"R2 S N2 D2 P\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"R2 from S with direct approach\", \n",
        "            \"M1 from G2 with a slight meend\"\n",
        "        ],\n",
        "        \"skipped_swars\": [],\n",
        "        \"microtonal_nuances\": \"All shuddha except M1 can have subtle andolan; typically straightforward.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Kafi\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"S\", \"R2\", \"G2\", \"M1\", \"P\", \"D2\", \"N2\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"N2\", \"D2\", \"P\", \"M1\", \"G2\", \"R2\", \"S\"]\n",
        "        },\n",
        "        \"western_scale\": {\n",
        "            \"aaroh\": [\"C4\", \"D4\", \"E4\", \"F4\", \"G4\", \"A4\", \"B4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"B4\", \"A4\", \"G4\", \"F4\", \"E4\", \"D4\", \"C4\"]\n",
        "        },\n",
        "        \"vaadi\": \"P\",\n",
        "        \"samvaadi\": \"R2\",\n",
        "        \"typical_phrases\": [\n",
        "            \"S R2 G2 M1 P D2 N2 S'\",\n",
        "            \"P D2 N2 S'\",\n",
        "            \"N2 S' R2 S\",\n",
        "            \"G2 M1 P D2 N2 S'\",\n",
        "            \"P M1 G2 R2 S\",\n",
        "            \"R2 G2 M1 R2 S\",\n",
        "            \"M1 P D2 N2\",\n",
        "            \"S' N2 D2 P\",\n",
        "            \"R2 S N2 D2 P\",\n",
        "            \"G2 M1 G2 R2 S\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"R2 from S directly\",\n",
        "            \"G2 from R2 with a small glide\"\n",
        "        ],\n",
        "        \"skipped_swars\": [],\n",
        "        \"microtonal_nuances\": \"All shuddha notes; minor kampan on Ga or Ni in folk styles.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Bhupali\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"S\", \"R2\", \"G2\", \"P\", \"D2\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"D2\", \"P\", \"G2\", \"R2\", \"S\"]\n",
        "        },\n",
        "        \"western_scale\": {\n",
        "            \"aaroh\": [\"C4\", \"D4\", \"E4\", \"G4\", \"A4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"A4\", \"G4\", \"E4\", \"D4\", \"C4\"]\n",
        "        },\n",
        "        \"vaadi\": \"G2\",\n",
        "        \"samvaadi\": \"D2\",\n",
        "        \"typical_phrases\": [\n",
        "            \"S R2 G2 P D2 S'\",\n",
        "            \"R2 G2 P D2 S'\",\n",
        "            \"S' D2 P G2 R2 S\",\n",
        "            \"R2 S G2 P D2 S'\",\n",
        "            \"P G2 R2 S\",\n",
        "            \"G2 P D2 S'\",\n",
        "            \"S' D2 P G2\",\n",
        "            \"R2 G2 P\",\n",
        "            \"G2 R2 S\",\n",
        "            \"D2 P R2 S\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"R2 from S softly\",\n",
        "            \"G2 from R2 with a meend\"\n",
        "        ],\n",
        "        \"skipped_swars\": [\"M1\", \"N2\"],\n",
        "        \"microtonal_nuances\": \"Shuddha Re, Ga, Dha; typically minimal microtonal oscillation.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Kalyan\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"S\", \"R2\", \"G2\", \"M2\", \"P\", \"D2\", \"N2\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"N2\", \"D2\", \"P\", \"M2\", \"G2\", \"R2\", \"S\"]\n",
        "        },\n",
        "        \"western_scale\": {\n",
        "            \"aaroh\": [\"C4\", \"D4\", \"E4\", \"F#4\", \"G4\", \"A4\", \"B4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"B4\", \"A4\", \"G4\", \"F#4\", \"E4\", \"D4\", \"C4\"]\n",
        "        },\n",
        "        \"vaadi\": \"R2\", \n",
        "        \"samvaadi\": \"P\", \n",
        "        \"typical_phrases\": [\n",
        "            \"S R2 G2 M2 P D2 N2 S'\",\n",
        "            \"R2 G2 M2 P\",\n",
        "            \"M2 P D2 N2 S'\",\n",
        "            \"N2 D2 P M2 G2 R2 S\",\n",
        "            \"G2 M2 R2 S\",\n",
        "            \"R2 S N2 D2 P\",\n",
        "            \"M2 G2 R2 S\",\n",
        "            \"R2 G2 M2 G2 R2 S\",\n",
        "            \"D2 N2 S'\",\n",
        "            \"P D2 N2\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"M2 from G2 with a slight meend\",\n",
        "            \"N2 from D2 quickly\"\n",
        "        ],\n",
        "        \"skipped_swars\": [],\n",
        "        \"microtonal_nuances\": \"Tivra Ma (M2), rest shuddha. Subtle meends from Re to Ga.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Bageshree\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"S\", \"G2\", \"M1\", \"D2\", \"N2\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"N2\", \"D2\", \"M1\", \"G2\", \"R2\", \"S\"]\n",
        "        },\n",
        "        \"western_scale\": {\n",
        "            \"aaroh\": [\"C4\", \"E4\", \"F4\", \"A4\", \"B4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"B4\", \"A4\", \"F4\", \"E4\", \"D4\", \"C4\"]\n",
        "        },\n",
        "        \"vaadi\": \"M1\",\n",
        "        \"samvaadi\": \"S\",\n",
        "        \"typical_phrases\": [\n",
        "            \"S G2 M1 D2 N2 S'\",\n",
        "            \"S' N2 D2 M1 G2 S\",\n",
        "            \"M1 D2 N2 S'\",\n",
        "            \"N2 S' M1 G2 S\",\n",
        "            \"G2 M1 G2 S\",\n",
        "            \"D2 M1 G2 S\",\n",
        "            \"M1 D2 N2 S'\",\n",
        "            \"S' N2 D2 M1 G2 S\",\n",
        "            \"G2 M1 R2 S\",\n",
        "            \"S G2 M1\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"G2 from S with a slight meend\",\n",
        "            \"M1 from G2 directly\"\n",
        "        ],\n",
        "        \"skipped_swars\": [\"P\"],\n",
        "        \"microtonal_nuances\": \"No Pa in aaroh, subtle andolan on Ga or Dha in slow elaborations.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Durga\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"S\", \"R2\", \"M1\", \"P\", \"D2\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"D2\", \"P\", \"M1\", \"R2\", \"S\"]\n",
        "        },\n",
        "        \"western_scale\": {\n",
        "            \"aaroh\": [\"C4\", \"D4\", \"F4\", \"G4\", \"A4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"A4\", \"G4\", \"F4\", \"D4\", \"C4\"]\n",
        "        },\n",
        "        \"vaadi\": \"R2\",\n",
        "        \"samvaadi\": \"D2\",\n",
        "        \"typical_phrases\": [\n",
        "            \"S R2 M1 P D2 S'\",\n",
        "            \"R2 M1 P D2 S'\",\n",
        "            \"S' D2 P M1 R2 S\",\n",
        "            \"M1 R2 S\",\n",
        "            \"P D2 S'\",\n",
        "            \"R2 M1 R2 S\",\n",
        "            \"M1 P D2 S'\",\n",
        "            \"S' D2 P M1\",\n",
        "            \"R2 S D2 P\",\n",
        "            \"M1 P R2 S\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"R2 from S quickly\",\n",
        "            \"M1 from R2\"\n",
        "        ],\n",
        "        \"skipped_swars\": [\"G2\", \"N2\"],\n",
        "        \"microtonal_nuances\": \"All shuddha except no Ga or Ni. Minimal microtonal usage.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Desh\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"S\", \"R2\", \"M1\", \"P\", \"N2\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"N2\", \"D2\", \"P\", \"M1\", \"G2\", \"R2\", \"S\"]\n",
        "        },\n",
        "        \"western_scale\": {\n",
        "            \"aaroh\": [\"C4\", \"D4\", \"F4\", \"G4\", \"B4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"B4\", \"A4\", \"G4\", \"F4\", \"E4\", \"D4\", \"C4\"]\n",
        "        },\n",
        "        \"vaadi\": \"R2\",\n",
        "        \"samvaadi\": \"P\",\n",
        "        \"typical_phrases\": [\n",
        "            \"S R2 M1 P N2 S'\",\n",
        "            \"R2 M1 P N2 S'\",\n",
        "            \"S' N2 D2 P\",\n",
        "            \"M1 G2 R2 S\",\n",
        "            \"R2 S N2 D2 P\",\n",
        "            \"M1 P N2 S'\",\n",
        "            \"S' N2 D2 P M1 G2 R2 S\",\n",
        "            \"M1 P N2 S' R2 S\",\n",
        "            \"G2 R2 S\",\n",
        "            \"R2 M1 P\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"R2 from S with mild meend\",\n",
        "            \"N2 from P\"\n",
        "        ],\n",
        "        \"skipped_swars\": [\"G2\"  # partial usage in aaroh\n",
        "        ],\n",
        "        \"microtonal_nuances\": \"Ga touched in avroh. Some variations allow slow meends on N2.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Malkauns\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"S\", \"g1\", \"M1\", \"d1\", \"n1\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"n1\", \"d1\", \"M1\", \"g1\", \"S\"]\n",
        "        },\n",
        "        \"western_scale\": {\n",
        "            \"aaroh\": [\"C4\", \"Eb4\", \"F4\", \"Ab4\", \"Bb4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"Bb4\", \"Ab4\", \"F4\", \"Eb4\", \"C4\"]\n",
        "        },\n",
        "        \"vaadi\": \"M1\",\n",
        "        \"samvaadi\": \"S\",\n",
        "        \"typical_phrases\": [\n",
        "            \"S g1 M1 d1 n1 S'\",\n",
        "            \"g1 M1 d1 n1 S'\",\n",
        "            \"S' n1 d1 M1 g1 S\",\n",
        "            \"n1 S' g1 M1 S\",\n",
        "            \"d1 n1 S'\",\n",
        "            \"g1 M1 g1 S\",\n",
        "            \"d1 n1 S' n1 d1\",\n",
        "            \"M1 g1 S\",\n",
        "            \"S g1 M1 d1\",\n",
        "            \"n1 d1 M1 g1 S\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"g1 from S with an andolan\",\n",
        "            \"d1 from M1 with a slow approach\"\n",
        "        ],\n",
        "        \"skipped_swars\": [\"R2\", \"G2\", \"P\", \"D2\", \"N2\"],\n",
        "        \"microtonal_nuances\": \"Komal Ga, Dha, Ni each can have deep andolan style.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Hamsadhwani\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"S\", \"R2\", \"G2\", \"P\", \"N2\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"N2\", \"P\", \"G2\", \"R2\", \"S\"]\n",
        "        },\n",
        "        \"western_scale\": {\n",
        "            \"aaroh\": [\"C4\", \"D4\", \"E4\", \"G4\", \"B4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"B4\", \"G4\", \"E4\", \"D4\", \"C4\"]\n",
        "        },\n",
        "        \"vaadi\": \"G2\",\n",
        "        \"samvaadi\": \"N2\",\n",
        "        \"typical_phrases\": [\n",
        "            \"S R2 G2 P N2 S'\",\n",
        "            \"R2 G2 P N2 S'\",\n",
        "            \"S' N2 P G2 R2 S\",\n",
        "            \"R2 S G2 P\",\n",
        "            \"G2 P N2 S'\",\n",
        "            \"N2 S' R2 G2\",\n",
        "            \"P G2 R2 S\",\n",
        "            \"S R2 G2\",\n",
        "            \"R2 G2 P\",\n",
        "            \"G2 P N2 S'\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"R2 from S quickly\",\n",
        "            \"G2 from R2 direct approach\"\n",
        "        ],\n",
        "        \"skipped_swars\": [\"M1\", \"D2\"],\n",
        "        \"microtonal_nuances\": \"Primarily shuddha notes, slight meend R2->G2.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Jaunpuri\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"S\", \"R2\", \"g1\", \"M1\", \"P\", \"d1\", \"N2\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"N2\", \"d1\", \"P\", \"M1\", \"g1\", \"R2\", \"S\"]\n",
        "        },\n",
        "        \"western_scale\": {\n",
        "            \"aaroh\": [\"C4\", \"D4\", \"Eb4\", \"F4\", \"G4\", \"Ab4\", \"B4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"B4\", \"Ab4\", \"G4\", \"F4\", \"Eb4\", \"D4\", \"C4\"]\n",
        "        },\n",
        "        \"vaadi\": \"P\",\n",
        "        \"samvaadi\": \"S\",\n",
        "        \"typical_phrases\": [\n",
        "            \"S R2 g1 M1 P d1 N2 S'\",\n",
        "            \"R2 g1 M1 P d1 N2 S'\",\n",
        "            \"S' N2 d1 P\",\n",
        "            \"M1 g1 R2 S\",\n",
        "            \"g1 M1 P d1 N2 S'\",\n",
        "            \"P d1 N2 S'\",\n",
        "            \"R2 S N2 d1 P\",\n",
        "            \"g1 R2 S\",\n",
        "            \"d1 N2 S'\",\n",
        "            \"M1 P d1 N2 S'\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"g1 from R2 with a small meend\",\n",
        "            \"d1 from P\"\n",
        "        ],\n",
        "        \"skipped_swars\": [],\n",
        "        \"microtonal_nuances\": \"Komal Ga, Dha. Some meends from Re to ga.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Alhaiya Bilawal\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"S\", \"R2\", \"G2\", \"M1\", \"P\", \"D2\", \"N2\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"N2\", \"D2\", \"P\", \"M1\", \"G2\", \"R2\", \"S\"]\n",
        "        },\n",
        "        \"western_scale\": {\n",
        "            \"aaroh\": [\"C4\", \"D4\", \"E4\", \"F4\", \"G4\", \"A4\", \"B4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"B4\", \"A4\", \"G4\", \"F4\", \"E4\", \"D4\", \"C4\"]\n",
        "        },\n",
        "        \"vaadi\": \"D2\",\n",
        "        \"samvaadi\": \"G2\",\n",
        "        \"typical_phrases\": [\n",
        "            \"S R2 G2 M1 P D2 N2 S'\",\n",
        "            \"R2 G2 M1 P D2 N2 S'\",\n",
        "            \"N2 D2 P M1 G2 R2 S\",\n",
        "            \"G2 M1 G2 R2 S\",\n",
        "            \"M1 P D2 N2 S'\",\n",
        "            \"R2 S N2 D2 P\",\n",
        "            \"S' N2 D2 P\",\n",
        "            \"R2 G2 M1 R2 S\",\n",
        "            \"D2 P M1 G2\",\n",
        "            \"R2 S N2 D2 P\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"R2 from S\",\n",
        "            \"G2 from R2 with meend\"\n",
        "        ],\n",
        "        \"skipped_swars\": [],\n",
        "        \"microtonal_nuances\": \"Similar to Bilawal but with special phrases. Shuddha notes mostly.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Kedar\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"S\", \"M1\", \"M2\", \"P\", \"N2\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"N2\", \"P\", \"M2\", \"M1\", \"G2\", \"R2\", \"S\"]\n",
        "        },\n",
        "        \"western_scale\": {\n",
        "            \"aaroh\": [\"C4\", \"F4\", \"F#4\", \"G4\", \"B4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"B4\", \"G4\", \"F#4\", \"F4\", \"E4\", \"D4\", \"C4\"]\n",
        "        },\n",
        "        \"vaadi\": \"M2\",\n",
        "        \"samvaadi\": \"S\",\n",
        "        \"typical_phrases\": [\n",
        "            \"S M1 M2 P N2 S'\",\n",
        "            \"M1 M2 P N2 S'\",\n",
        "            \"S' N2 P M2 M1 G2 R2 S\",\n",
        "            \"M2 P N2 S'\",\n",
        "            \"N2 P M2 M1\",\n",
        "            \"M1 G2 R2 S\",\n",
        "            \"M2 P N2 S'\",\n",
        "            \"P N2 S'\",\n",
        "            \"M1 M2 P\",\n",
        "            \"G2 R2 S\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"M2 from M1 with a quick oscillation\",\n",
        "            \"N2 from P direct\"\n",
        "        ],\n",
        "        \"skipped_swars\": [],\n",
        "        \"microtonal_nuances\": \"Both Ma shuddha (M1) and tivra (M2). Dramatic meends on Ma.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Patdeep\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"N2\", \"S\", \"G2\", \"M1\", \"P\", \"N2\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"N2\", \"P\", \"M1\", \"G2\", \"R2\", \"S\"]\n",
        "        },\n",
        "        \"western_scale\": {\n",
        "            \"aaroh\": [\"B3\", \"C4\", \"E4\", \"F4\", \"G4\", \"B4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"B4\", \"G4\", \"F4\", \"E4\", \"D4\", \"C4\"]\n",
        "        },\n",
        "        \"vaadi\": \"P\",\n",
        "        \"samvaadi\": \"S\",\n",
        "        \"typical_phrases\": [\n",
        "            \"N2 S G2 M1 P N2 S'\",\n",
        "            \"S G2 M1 P\",\n",
        "            \"N2 S' N2 P\",\n",
        "            \"P M1 G2 R2 S\",\n",
        "            \"G2 M1 P N2 S'\",\n",
        "            \"N2 P M1 G2\",\n",
        "            \"P N2 S' R2 S\",\n",
        "            \"G2 R2 S\",\n",
        "            \"N2 S' N2\",\n",
        "            \"S G2 M1\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"N2 from P\",\n",
        "            \"G2 from S\"\n",
        "        ],\n",
        "        \"skipped_swars\": [\"D2\"],\n",
        "        \"microtonal_nuances\": \"Shuddha Ni, approach from lower Pa. Subtle meends G2 <-> M1.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Darbari Kanada\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"S\", \"R2\", \"r1\", \"G2\", \"M1\", \"P\", \"D2\", \"n1\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"n1\", \"D2\", \"P\", \"M1\", \"G2\", \"r1\", \"S\"]\n",
        "        },\n",
        "        \"western_scale\": {\n",
        "            \"aaroh\": [\"C4\", \"D4\", \"Db4\", \"E4\", \"F4\", \"G4\", \"A4\", \"Ab4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"Ab4\", \"A4\", \"G4\", \"F4\", \"E4\", \"Db4\", \"C4\"]\n",
        "        },\n",
        "        \"vaadi\": \"R2\",\n",
        "        \"samvaadi\": \"P\",\n",
        "        \"typical_phrases\": [\n",
        "            \"S R2 r1 G2 M1 P D2 n1 S'\",\n",
        "            \"r1 G2 M1 P D2 n1 S'\",\n",
        "            \"S' n1 D2 P M1 G2 r1 S\",\n",
        "            \"G2 r1 S\",\n",
        "            \"r1 G2 M1\",\n",
        "            \"P D2 n1 S'\",\n",
        "            \"n1 S' r1 G2\",\n",
        "            \"M1 G2 r1 S\",\n",
        "            \"D2 P M1 G2\",\n",
        "            \"r1 S n1 D2 P\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"r1 from S with deep andolan\",\n",
        "            \"n1 from D2 with slow oscillation\"\n",
        "        ],\n",
        "        \"skipped_swars\": [],\n",
        "        \"microtonal_nuances\": \"Famous for andolan on Re and Ga, Komal Ni with heavy oscillation.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Shankara\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"S\", \"R2\", \"S\", \"R2\", \"M1\", \"P\", \"N2\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"N2\", \"P\", \"M1\", \"R2\", \"S\"]\n",
        "        },\n",
        "        \"western_scale\": {\n",
        "            \"aaroh\": [\"C4\", \"D4\", \"C4\", \"D4\", \"F4\", \"G4\", \"B4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"B4\", \"G4\", \"F4\", \"D4\", \"C4\"]\n",
        "        },\n",
        "        \"vaadi\": \"R2\",\n",
        "        \"samvaadi\": \"P\",\n",
        "        \"typical_phrases\": [\n",
        "            \"S R2 S R2 M1 P N2 S'\",\n",
        "            \"R2 S R2 M1 P\",\n",
        "            \"M1 P N2 S'\",\n",
        "            \"N2 S' R2 S\",\n",
        "            \"R2 M1 P N2 S'\",\n",
        "            \"S' N2 P M1 R2 S\",\n",
        "            \"P M1 R2 S R2\",\n",
        "            \"M1 R2 S\",\n",
        "            \"R2 S N2 P\",\n",
        "            \"M1 P N2\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"R2 from S repeated\",\n",
        "            \"N2 from P directly\"\n",
        "        ],\n",
        "        \"skipped_swars\": [\"G2\", \"D2\"],\n",
        "        \"microtonal_nuances\": \"Shuddha notes, repeated Re usage. Some meend from M1 to P.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Shree\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"S\", \"r1\", \"M1\", \"P\", \"N2\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"N2\", \"P\", \"M1\", \"r1\", \"S\"]\n",
        "        },\n",
        "        \"western_scale\": {\n",
        "            \"aaroh\": [\"C4\", \"Db4\", \"F4\", \"G4\", \"B4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"B4\", \"G4\", \"F4\", \"Db4\", \"C4\"]\n",
        "        },\n",
        "        \"vaadi\": \"P\",\n",
        "        \"samvaadi\": \"r1\",\n",
        "        \"typical_phrases\": [\n",
        "            \"S r1 M1 P N2 S'\",\n",
        "            \"r1 M1 P N2 S'\",\n",
        "            \"S' N2 P M1 r1 S\",\n",
        "            \"M1 r1 S\",\n",
        "            \"P N2 S'\",\n",
        "            \"r1 M1 P\",\n",
        "            \"N2 P r1 S\",\n",
        "            \"S' N2 P\",\n",
        "            \"r1 S r1 M1\",\n",
        "            \"M1 r1 S\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"r1 from S with andolan\",\n",
        "            \"N2 from P\"\n",
        "        ],\n",
        "        \"skipped_swars\": [\"G2\", \"D2\"],\n",
        "        \"microtonal_nuances\": \"Komal Re with slow oscillation, rest shuddha. P is strong note.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Marwa\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"S\", \"r1\", \"M2\", \"D2\", \"N2\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"N2\", \"D2\", \"M2\", \"r1\", \"S\"]\n",
        "        },\n",
        "        \"western_scale\": {\n",
        "            \"aaroh\": [\"C4\", \"Db4\", \"F#4\", \"A4\", \"B4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"B4\", \"A4\", \"F#4\", \"Db4\", \"C4\"]\n",
        "        },\n",
        "        \"vaadi\": \"r1\",\n",
        "        \"samvaadi\": \"D2\",\n",
        "        \"typical_phrases\": [\n",
        "            \"S r1 M2 D2 N2 S'\",\n",
        "            \"r1 M2 D2 N2 S'\",\n",
        "            \"S' N2 D2 M2 r1 S\",\n",
        "            \"D2 N2 S'\",\n",
        "            \"M2 D2 N2 S'\",\n",
        "            \"r1 S N2 D2\",\n",
        "            \"M2 r1 S\",\n",
        "            \"r1 M2 r1 S\",\n",
        "            \"N2 S' r1 M2\",\n",
        "            \"D2 N2 S'\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"r1 from S with tension\",\n",
        "            \"M2 from r1\"\n",
        "        ],\n",
        "        \"skipped_swars\": [\"G2\", \"P\"],\n",
        "        \"microtonal_nuances\": \"Komal Re with tension, Tivra Ma, no Pa. Tension Re-Ma hallmark.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Puriya\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"S\", \"r1\", \"G2\", \"M2\", \"D2\", \"N2\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"N2\", \"D2\", \"M2\", \"G2\", \"r1\", \"S\"]\n",
        "        },\n",
        "        \"western_scale\": {\n",
        "            \"aaroh\": [\"C4\", \"Db4\", \"E4\", \"F#4\", \"A4\", \"B4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"B4\", \"A4\", \"F#4\", \"E4\", \"Db4\", \"C4\"]\n",
        "        },\n",
        "        \"vaadi\": \"G2\",\n",
        "        \"samvaadi\": \"r1\",\n",
        "        \"typical_phrases\": [\n",
        "            \"S r1 G2 M2 D2 N2 S'\",\n",
        "            \"r1 G2 M2 D2 N2 S'\",\n",
        "            \"S' N2 D2 M2 G2 r1 S\",\n",
        "            \"r1 S r1 G2\",\n",
        "            \"G2 M2 D2 N2 S'\",\n",
        "            \"N2 S' r1 G2\",\n",
        "            \"M2 G2 r1 S\",\n",
        "            \"r1 G2 M2\",\n",
        "            \"D2 N2 S'\",\n",
        "            \"G2 r1 S\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"r1 from S with a meend\",\n",
        "            \"G2 from r1 quickly\"\n",
        "        ],\n",
        "        \"skipped_swars\": [\"P\"],\n",
        "        \"microtonal_nuances\": \"Similar tension as Marwa, skipping Pa, Komal Re, Tivra Ma.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Puriya Dhanashree\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"S\", \"r1\", \"G2\", \"M2\", \"P\", \"D2\", \"N2\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"N2\", \"D2\", \"P\", \"M2\", \"G2\", \"r1\", \"S\"]\n",
        "        },\n",
        "        \"western_scale\": {\n",
        "            \"aaroh\": [\"C4\", \"Db4\", \"E4\", \"F#4\", \"G4\", \"A4\", \"B4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"B4\", \"A4\", \"G4\", \"F#4\", \"E4\", \"Db4\", \"C4\"]\n",
        "        },\n",
        "        \"vaadi\": \"P\",\n",
        "        \"samvaadi\": \"r1\",\n",
        "        \"typical_phrases\": [\n",
        "            \"S r1 G2 M2 P D2 N2 S'\",\n",
        "            \"r1 G2 M2 P D2 N2 S'\",\n",
        "            \"S' N2 D2 P M2 G2 r1 S\",\n",
        "            \"r1 G2 M2 P\",\n",
        "            \"D2 N2 S'\",\n",
        "            \"P M2 G2 r1 S\",\n",
        "            \"N2 S' r1 G2\",\n",
        "            \"G2 M2 P D2 N2\",\n",
        "            \"r1 S r1 G2\",\n",
        "            \"P D2 N2 S'\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"r1 from S with andolan\",\n",
        "            \"D2 from P with short meend\"\n",
        "        ],\n",
        "        \"skipped_swars\": [],\n",
        "        \"microtonal_nuances\": \"Komal Re, Tivra Ma, strong Pa/Ni usage. Similar scale to Puriya but includes Pa.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Lalit\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"S\", \"r1\", \"G2\", \"M1\", \"M2\", \"D2\", \"N2\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"N2\", \"D2\", \"M2\", \"M1\", \"G2\", \"r1\", \"S\"]\n",
        "        },\n",
        "        \"western_scale\": {\n",
        "            \"aaroh\": [\"C4\", \"Db4\", \"E4\", \"F4\", \"F#4\", \"A4\", \"B4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"B4\", \"A4\", \"F#4\", \"F4\", \"E4\", \"Db4\", \"C4\"]\n",
        "        },\n",
        "        \"vaadi\": \"M1/M2\",\n",
        "        \"samvaadi\": \"S\",\n",
        "        \"typical_phrases\": [\n",
        "            \"S r1 G2 M1 M2 D2 N2 S'\",\n",
        "            \"M1 M2 D2 N2 S'\",\n",
        "            \"N2 D2 M2 M1 G2 r1 S\",\n",
        "            \"M2 M1 G2 r1 S\",\n",
        "            \"r1 S N2 D2\",\n",
        "            \"M1 M2 r1 S\",\n",
        "            \"r1 G2 M1 M2\",\n",
        "            \"D2 N2 S'\",\n",
        "            \"G2 r1 S\",\n",
        "            \"M1 M2 D2\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"M2 from M1 with direct slide\",\n",
        "            \"r1 from S with meend\"\n",
        "        ],\n",
        "        \"skipped_swars\": [],\n",
        "        \"microtonal_nuances\": \"Distinct usage of both Ma1 & Ma2 side by side, Komal Re, rest mostly shuddha.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Chandrakauns\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"S\", \"g1\", \"M1\", \"d1\", \"n1\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"n1\", \"d1\", \"M1\", \"g1\", \"S\"]\n",
        "        },\n",
        "        \"western_scale\": {\n",
        "            \"aaroh\": [\"C4\", \"Eb4\", \"F4\", \"Ab4\", \"Bb4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"Bb4\", \"Ab4\", \"F4\", \"Eb4\", \"C4\"]\n",
        "        },\n",
        "        \"vaadi\": \"M1\",\n",
        "        \"samvaadi\": \"S\",\n",
        "        \"typical_phrases\": [\n",
        "            \"S g1 M1 d1 n1 S'\",\n",
        "            \"g1 M1 d1 n1 S'\",\n",
        "            \"S' n1 d1 M1 g1 S\",\n",
        "            \"n1 S' g1 M1 S\",\n",
        "            \"d1 n1 S'\",\n",
        "            \"g1 M1 g1 S\",\n",
        "            \"M1 d1 n1 S'\",\n",
        "            \"n1 S' n1 d1\",\n",
        "            \"M1 d1 g1 S\",\n",
        "            \"g1 M1 d1 n1\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"g1 from S with slight andolan\",\n",
        "            \"d1 from M1 smoothly\"\n",
        "        ],\n",
        "        \"skipped_swars\": [\"R2\", \"P\", \"D2\", \"N2\"],\n",
        "        \"microtonal_nuances\": \"Similar to Malkauns but a variant approach. Komal Ga, Dha, Ni with oscillation.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Jog\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"S\", \"G2\", \"M1\", \"P\", \"n1\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"n1\", \"P\", \"M1\", \"G2\", \"S\"]\n",
        "        },\n",
        "        \"western_scale\": {\n",
        "            \"aaroh\": [\"C4\", \"E4\", \"F4\", \"G4\", \"Bb4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"Bb4\", \"G4\", \"F4\", \"E4\", \"C4\"]\n",
        "        },\n",
        "        \"vaadi\": \"G2\",\n",
        "        \"samvaadi\": \"S\",\n",
        "        \"typical_phrases\": [\n",
        "            \"S G2 M1 P n1 S'\",\n",
        "            \"G2 M1 P n1 S'\",\n",
        "            \"S' n1 P M1 G2 S\",\n",
        "            \"n1 S' G2 M1 S\",\n",
        "            \"P n1 S'\",\n",
        "            \"M1 G2 S\",\n",
        "            \"G2 M1 P\",\n",
        "            \"n1 S' n1\",\n",
        "            \"M1 P n1 S'\",\n",
        "            \"G2 M1 G2 S\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"G2 from S directly\",\n",
        "            \"n1 from P with a small meend\"\n",
        "        ],\n",
        "        \"skipped_swars\": [\"R2\", \"D2\", \"N2\"],\n",
        "        \"microtonal_nuances\": \"Hybrid scale, Komal Ni with rest mostly shuddha. Subtle meends from Pa to Ni.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Ahir Bhairav\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"S\", \"r1\", \"G2\", \"M1\", \"P\", \"N2\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"N2\", \"d1\", \"P\", \"M1\", \"G2\", \"r1\", \"S\"]\n",
        "        },\n",
        "        \"western_scale\": {\n",
        "            \"aaroh\": [\"C4\", \"Db4\", \"E4\", \"F4\", \"G4\", \"B4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"B4\", \"Ab4\", \"G4\", \"F4\", \"E4\", \"Db4\", \"C4\"]\n",
        "        },\n",
        "        \"vaadi\": \"M1\",\n",
        "        \"samvaadi\": \"S\",\n",
        "        \"typical_phrases\": [\n",
        "            \"S r1 G2 M1 P N2 S'\",\n",
        "            \"r1 G2 M1 P N2 S'\",\n",
        "            \"S' N2 d1 P M1 G2 r1 S\",\n",
        "            \"G2 r1 S\",\n",
        "            \"M1 G2 r1 S\",\n",
        "            \"P N2 S'\",\n",
        "            \"N2 S' r1 G2\",\n",
        "            \"d1 P M1 G2\",\n",
        "            \"r1 G2 M1 P\",\n",
        "            \"S' N2 d1 P\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"r1 from S with slow approach\",\n",
        "            \"N2 from P partial andolan\"\n",
        "        ],\n",
        "        \"skipped_swars\": [\"D2\"],\n",
        "        \"microtonal_nuances\": \"Mix of Bhairav & Kafi flavors. Komal Re, Shuddha Ga, Komal Dha in avroh.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Madhuvanti\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"S\", \"R2\", \"G2\", \"M2\", \"P\", \"N2\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"N2\", \"P\", \"M2\", \"G2\", \"R2\", \"S\"]\n",
        "        },\n",
        "        \"western_scale\": {\n",
        "            \"aaroh\": [\"C4\", \"D4\", \"E4\", \"F#4\", \"G4\", \"B4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"B4\", \"G4\", \"F#4\", \"E4\", \"D4\", \"C4\"]\n",
        "        },\n",
        "        \"vaadi\": \"R2\",\n",
        "        \"samvaadi\": \"P\",\n",
        "        \"typical_phrases\": [\n",
        "            \"S R2 G2 M2 P N2 S'\",\n",
        "            \"R2 G2 M2 P N2 S'\",\n",
        "            \"S' N2 P M2 G2 R2 S\",\n",
        "            \"N2 P M2 G2 R2 S\",\n",
        "            \"G2 M2 R2 S\",\n",
        "            \"R2 S N2 P\",\n",
        "            \"M2 G2 R2 S\",\n",
        "            \"P N2 S'\",\n",
        "            \"R2 G2 M2\",\n",
        "            \"N2 P R2 S\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"R2 from S with slight meend\",\n",
        "            \"M2 from G2 carefully\"\n",
        "        ],\n",
        "        \"skipped_swars\": [\"D2\"],\n",
        "        \"microtonal_nuances\": \"Tivra Ma, bright mood, subtle meends on Ga-Ma.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Saraswati\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"S\", \"R2\", \"M2\", \"P\", \"N2\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"N2\", \"P\", \"M2\", \"R2\", \"S\"]\n",
        "        },\n",
        "        \"western_scale\": {\n",
        "            \"aaroh\": [\"C4\", \"D4\", \"F#4\", \"G4\", \"B4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"B4\", \"G4\", \"F#4\", \"D4\", \"C4\"]\n",
        "        },\n",
        "        \"vaadi\": \"R2\",\n",
        "        \"samvaadi\": \"P\",\n",
        "        \"typical_phrases\": [\n",
        "            \"S R2 M2 P N2 S'\",\n",
        "            \"R2 M2 P N2 S'\",\n",
        "            \"S' N2 P M2 R2 S\",\n",
        "            \"M2 P N2 S'\",\n",
        "            \"R2 S N2 P\",\n",
        "            \"M2 R2 S\",\n",
        "            \"P N2 S'\",\n",
        "            \"N2 P M2 R2\",\n",
        "            \"R2 M2 P\",\n",
        "            \"S R2 M2\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"R2 from S quickly\",\n",
        "            \"M2 from R2\"\n",
        "        ],\n",
        "        \"skipped_swars\": [\"G2\", \"D2\"],\n",
        "        \"microtonal_nuances\": \"Tivra Ma, bright approach, minimal microtonal glides.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Basant\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"S\", \"r1\", \"M2\", \"P\", \"N2\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"N2\", \"P\", \"M2\", \"r1\", \"S\"]\n",
        "        },\n",
        "        \"western_scale\": {\n",
        "            \"aaroh\": [\"C4\", \"Db4\", \"F#4\", \"G4\", \"B4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"B4\", \"G4\", \"F#4\", \"Db4\", \"C4\"]\n",
        "        },\n",
        "        \"vaadi\": \"r1\",\n",
        "        \"samvaadi\": \"P\",\n",
        "        \"typical_phrases\": [\n",
        "            \"S r1 M2 P N2 S'\",\n",
        "            \"r1 M2 P N2 S'\",\n",
        "            \"S' N2 P M2 r1 S\",\n",
        "            \"M2 r1 S\",\n",
        "            \"r1 M2 P\",\n",
        "            \"N2 S' r1\",\n",
        "            \"M2 r1 M2 P\",\n",
        "            \"r1 S N2 P\",\n",
        "            \"r1 S r1 M2\",\n",
        "            \"N2 P r1 S\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"r1 from S with a meend\",\n",
        "            \"N2 from P directly\"\n",
        "        ],\n",
        "        \"skipped_swars\": [\"G2\", \"D2\"],\n",
        "        \"microtonal_nuances\": \"Komal Re, Tivra Ma. Associated with spring. Glides from Re to Ma.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Pilu\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"S\", \"R2\", \"G2\", \"M1\", \"P\", \"D2\", \"N2\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"N2\", \"D2\", \"P\", \"M1\", \"G2\", \"R2\", \"S\"]\n",
        "        },\n",
        "        \"western_scale\": {\n",
        "            \"aaroh\": [\"C4\", \"D4\", \"E4\", \"F4\", \"G4\", \"A4\", \"B4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"B4\", \"A4\", \"G4\", \"F4\", \"E4\", \"D4\", \"C4\"]\n",
        "        },\n",
        "        \"vaadi\": \"G2\",\n",
        "        \"samvaadi\": \"N2\",\n",
        "        \"typical_phrases\": [\n",
        "            \"S R2 G2 M1 P D2 N2 S'\",\n",
        "            \"R2 G2 M1 P D2 N2 S'\",\n",
        "            \"S' N2 D2 P M1 G2 R2 S\",\n",
        "            \"G2 M1 R2 S\",\n",
        "            \"R2 S N2 D2 P\",\n",
        "            \"P D2 N2 S'\",\n",
        "            \"S' N2 D2 P\",\n",
        "            \"G2 M1 G2 R2 S\",\n",
        "            \"D2 P M1 G2\",\n",
        "            \"R2 S D2 P\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"R2 from S\",\n",
        "            \"G2 from R2\"\n",
        "        ],\n",
        "        \"skipped_swars\": [],\n",
        "        \"microtonal_nuances\": \"Light classical usage, often allows thumri-style slides. Ga, Ni might vary slightly.\"\n",
        "    },\n",
        "    ################\n",
        "    # NEW RAGA #31\n",
        "    ################\n",
        "    {\n",
        "        \"name\": \"Amritvarshini\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"S\", \"G2\", \"M2\", \"P\", \"N3\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"N3\", \"P\", \"M2\", \"G2\", \"S\"]\n",
        "        },\n",
        "        \"western_scale\": {\n",
        "            \"aaroh\": [\"C4\", \"E4\", \"F#4\", \"G4\", \"B4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"B4\", \"G4\", \"F#4\", \"E4\", \"C4\"]\n",
        "        },\n",
        "        \"vaadi\": \"M2\",\n",
        "        \"samvaadi\": \"S\",\n",
        "        \"typical_phrases\": [\n",
        "            \"S G2 M2 P N3 S'\",\n",
        "            \"G2 M2 P\",\n",
        "            \"M2 P N3 S'\",\n",
        "            \"N3 P M2 G2 S\",\n",
        "            \"G2 M2 G2 S\",\n",
        "            \"P N3 S'\",\n",
        "            \"S' N3 P M2\",\n",
        "            \"M2 G2 S\",\n",
        "            \"G2 M2 P N3\",\n",
        "            \"N3 P G2 S\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"G2 from S with a slight meend\",\n",
        "            \"M2 from G2 with a direct approach\"\n",
        "        ],\n",
        "        \"skipped_swars\": [\"R2\", \"D2\", \"r1\", \"d1\"],\n",
        "        \"microtonal_nuances\": \"Carnatic-origin raga. Typically bright scale with Tivra Ma; minimal lower swaras.\"\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13e2539e",
      "metadata": {},
      "source": [
        "## Detect Raga from Notes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d7d5d06",
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def normalize_note(note):\n",
        "    \"\"\"\n",
        "    Convert a note into a canonical representation.\n",
        "\n",
        "    - If the note is already in Indian notation (e.g., S, G2, M2, etc.), return it unchanged.\n",
        "    - If it is in Western notation (e.g., \"C4\", \"F#4\"), remove the octave and map it to an Indian equivalent.\n",
        "    \n",
        "    Adjust the mapping dictionary as needed.\n",
        "    \"\"\"\n",
        "    indian_notes = {\"S\", \"R1\", \"R2\", \"R3\", \"G1\", \"G2\", \"G3\", \"M1\", \"M2\", \"P\", \"D1\", \"D2\", \"D3\", \"N1\", \"N2\", \"N3\"}\n",
        "    if note in indian_notes or note == \"S'\":  # allow S' for the upper tonic\n",
        "        return note\n",
        "\n",
        "    match = re.match(r\"^([A-G][#b]?)(\\d+)$\", note)\n",
        "    if match:\n",
        "        note_letter = match.group(1)\n",
        "        # Example mapping; adjust according to your tonic and definitions.\n",
        "        western_to_indian = {\n",
        "            \"C\": \"S\",\n",
        "            \"C#\": \"R1\",\n",
        "            \"D\": \"R2\",\n",
        "            \"D#\": \"G1\",\n",
        "            \"E\": \"G2\",\n",
        "            \"F\": \"M1\",\n",
        "            \"F#\": \"M2\",\n",
        "            \"G\": \"P\",\n",
        "            \"G#\": \"D1\",\n",
        "            \"A\": \"D2\",\n",
        "            \"A#\": \"N1\",\n",
        "            \"B\": \"N3\"\n",
        "        }\n",
        "        return western_to_indian.get(note_letter, note)\n",
        "    \n",
        "    return note\n",
        "\n",
        "def detect_raga_from_notes(detected_notes, raga_db):\n",
        "    \"\"\"\n",
        "    Compare the singer's note usage (detected_notes) to each raga in raga_db.\n",
        "    Uses note normalization so that both Indian and Western note names are handled.\n",
        "    Returns a dict with:\n",
        "       - 'best_raga': the highest-scoring raga,\n",
        "       - 'explanation': explanation for the best raga,\n",
        "       - 'top_ragas': a list of the top three scored ragas,\n",
        "       - 'all_scores': a list of all raga scores.\n",
        "    \"\"\"\n",
        "    # Normalize detected notes.\n",
        "    normalized_notes = [normalize_note(n) for n in detected_notes]\n",
        "    note_counter = Counter(normalized_notes)\n",
        "    total_notes = sum(note_counter.values()) if note_counter else 1\n",
        "    most_common_note, _ = note_counter.most_common(1)[0] if note_counter else (\"?\", 0)\n",
        "    \n",
        "    # Create a space-delimited string for contiguous phrase matching.\n",
        "    detected_sequence = \" \".join(normalized_notes)\n",
        "    \n",
        "    results = []\n",
        "    for raga in raga_db:\n",
        "        raga_name = raga[\"name\"]\n",
        "\n",
        "        # Normalize scale notes for the raga.\n",
        "        indian_aaroh = {normalize_note(n) for n in raga[\"indian_scale\"][\"aaroh\"]}\n",
        "        indian_avroh = {normalize_note(n) for n in raga[\"indian_scale\"][\"avroh\"]}\n",
        "        western_aaroh = {normalize_note(n) for n in raga[\"western_scale\"][\"aaroh\"]}\n",
        "        western_avroh = {normalize_note(n) for n in raga[\"western_scale\"][\"avroh\"]}\n",
        "        combined_scale = indian_aaroh.union(indian_avroh, western_aaroh, western_avroh)\n",
        "\n",
        "        # Normalize vaadi and samvaadi, if provided.\n",
        "        vaadi = normalize_note(raga.get(\"vaadi\", \"\"))\n",
        "        samvaadi = normalize_note(raga.get(\"samvaadi\", \"\"))\n",
        "        typical_phrases = raga.get(\"typical_phrases\", [])\n",
        "\n",
        "        # (A) Coverage: Fraction of detected notes that lie in the raga's scale.\n",
        "        in_scale_count = sum(count for note, count in note_counter.items() if note in combined_scale)\n",
        "        coverage = in_scale_count / total_notes\n",
        "\n",
        "        # (B) Vaadi/Samvaadi bonus.\n",
        "        vaadi_bonus = 0.0\n",
        "        reason_list = []\n",
        "        if most_common_note == vaadi:\n",
        "            vaadi_bonus += 0.2\n",
        "            reason_list.append(f\"Most used note matches vaadi={vaadi}.\")\n",
        "        if most_common_note == samvaadi:\n",
        "            vaadi_bonus += 0.1\n",
        "            reason_list.append(f\"Most used note matches samvaadi={samvaadi}.\")\n",
        "\n",
        "        # (C) Phrase bonus: Check for contiguous appearance of each normalized phrase.\n",
        "        phrase_bonus = 0.0\n",
        "        phrase_hits = 0\n",
        "        for phrase in typical_phrases:\n",
        "            # Normalize the phrase notes and join into a string.\n",
        "            phrase_norm = \" \".join(normalize_note(n) for n in phrase.split() if n)\n",
        "            # Check if the normalized phrase is a contiguous subsequence of the detected sequence.\n",
        "            if phrase_norm in detected_sequence:\n",
        "                phrase_bonus += 0.03  # Reduced bonus weight; adjust as needed.\n",
        "                phrase_hits += 1\n",
        "\n",
        "        match_score = coverage + vaadi_bonus + phrase_bonus\n",
        "        explanation = (f\"Coverage={coverage:.2f}, Vaadi bonus={vaadi_bonus:.2f}, \"\n",
        "                       f\"Phrase bonus={phrase_bonus:.2f} (matched {phrase_hits} phrases).\")\n",
        "        if reason_list:\n",
        "            explanation += \" | \" + \" \".join(reason_list)\n",
        "\n",
        "        results.append((raga_name, match_score, explanation))\n",
        "\n",
        "    results_sorted = sorted(results, key=lambda x: x[1], reverse=True)\n",
        "    \n",
        "    if not results_sorted:\n",
        "        return {\n",
        "            \"best_raga\": \"Unknown\",\n",
        "            \"explanation\": \"No ragas in the database or no notes detected.\",\n",
        "            \"top_ragas\": [],\n",
        "            \"all_scores\": []\n",
        "        }\n",
        "    \n",
        "    # Prepare the top three ragas (if available)\n",
        "    top_three = results_sorted[:3]\n",
        "    best_raga, best_score, best_explanation = results_sorted[0]\n",
        "    \n",
        "    return {\n",
        "        \"best_raga\": best_raga,\n",
        "        \"explanation\": best_explanation,  # Explanation for the best raga.\n",
        "        \"top_ragas\": [\n",
        "            {\"raga\": r[0], \"score\": r[1], \"explanation\": r[2]} for r in top_three\n",
        "        ],\n",
        "        \"all_scores\": [\n",
        "            {\"raga\": r[0], \"score\": r[1], \"explanation\": r[2]} for r in results_sorted\n",
        "        ]\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mod11-plot",
      "metadata": {},
      "source": [
        "## 11. Plotting All Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "plot-all-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "#def plot_sruti_overlay(ax, times_small, pitch_small, sruti_labels):\n",
        "#    \"\"\"\n",
        "#    Overlays a scatter plot on 'ax' using times vs. pitch, colored by sruti index.\n",
        "#    We'll parse the 'sruti_XX' from each label, if possible, and color them with a\n",
        "#    discrete colormap of 22 steps.\n",
        "#    \"\"\"\n",
        "#    import re\n",
        "#    from matplotlib import colormaps\n",
        "\n",
        "    # 1) Grab the base 'hsv' colormap\n",
        "#    base_cmap = colormaps[\"hsv\"]\n",
        "    # 2) Resample to 22 discrete levels\n",
        "#    cmap = base_cmap.resampled(22)\n",
        "\n",
        "#    sruti_indices = []\n",
        "#    for lbl in sruti_labels:\n",
        "#        match = re.search(r\"sruti_(\\d+)\", lbl)\n",
        "#        if match:\n",
        "#            sruti_indices.append(int(match.group(1)))\n",
        "#        else:\n",
        "            # default to 0 if unknown\n",
        "#            sruti_indices.append(0)\n",
        "\n",
        "#    sc = ax.scatter(\n",
        "#        times_small, pitch_small,\n",
        "#        c=sruti_indices, cmap=cmap,\n",
        "#        alpha=0.8, s=10, edgecolors='none'\n",
        "#    )\n",
        "#    ax.figure.colorbar(sc, ax=ax, label='22 Śruti Index (0..21)')\n",
        "#    ax.set_title(\"Pitch with 22-Sruti Classification\")\n",
        "#    ax.set_xlabel(\"Time (s)\")\n",
        "#    ax.set_ylabel(\"Pitch (Hz)\")\n",
        "\n",
        "\n",
        "def plot_sruti_overlay(ax, times_small, pitch_small, sruti_labels):\n",
        "    import re\n",
        "    sruti_indices = []\n",
        "    for lbl in sruti_labels:\n",
        "        # If the label is None, replace it with a default value.\n",
        "        if lbl is None:\n",
        "            lbl = \"sruti_unknown\"\n",
        "        # Attempt to extract the numeric portion.\n",
        "        match = re.search(r\"sruti_(\\d+)\", lbl)\n",
        "        if match:\n",
        "            sruti_indices.append(int(match.group(1)))\n",
        "        else:\n",
        "            # If the label doesn't match the expected format, default to 0.\n",
        "            sruti_indices.append(0)\n",
        "    # Continue with your plotting code, using sruti_indices as needed.\n",
        "    # For example, you might overlay these indices on your pitch plot.\n",
        "    ax.plot(times_small, pitch_small, label=\"Pitch contour\")\n",
        "    ax.scatter(times_small, sruti_indices, color='red', label=\"Sruti index\")\n",
        "    ax.set_xlabel(\"Time (s)\")\n",
        "    ax.set_ylabel(\"Pitch / Sruti Index\")\n",
        "    ax.legend()\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Define a constant for the output directory (instead of hard-coding it inline).\n",
        "OUTPUT_DIR = \"data/analysisoutput\"\n",
        "\n",
        "def plot_all_metrics(\n",
        "    time_matrix_small,\n",
        "    praat_matrix,\n",
        "    audio_data=None,\n",
        "    sr=None,\n",
        "    time_matrix_tempo_medium=None,\n",
        "    time_matrix_tempo_large=None,\n",
        "    aggregated_items=None,\n",
        "    analysis_dict=None,  # Contains advanced_vocal_features_per_chunk and/or advanced_note_features\n",
        "    input_filename=\"audio_input.wav\"  # <-- New parameter\n",
        "):\n",
        "    \"\"\"\n",
        "    Plot pitch, transitions, HNR, advanced features, plus RMS & LUFS (with new chunk size)\n",
        "    and multi-chunk tempo lines. Also annotate aggregated feedback intervals on the time axis,\n",
        "    and now save each figure to OUTPUT_DIR using <filename>_<plot>_<date>.\n",
        "    \"\"\"\n",
        "    import librosa.display\n",
        "\n",
        "    # Ensure our output directory exists\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "    # Create a timestamp string to reuse for all saved plots\n",
        "    now_str = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "    # Extract base filename (no directory, no extension) for naming\n",
        "    base_name = os.path.splitext(os.path.basename(input_filename))[0]\n",
        "\n",
        "    # ------------------------------------------------------------\n",
        "    # 1) Figure 1: pitch & note freq, spectral flatness & transition score, Praat HNR\n",
        "    fig, axes = plt.subplots(3, 1, figsize=(12, 8), sharex=False)\n",
        "\n",
        "    # Subplot 1: pitch & note freq\n",
        "    ax0 = axes[0]\n",
        "    times_small = [row.get('time_s', 0.0) for row in time_matrix_small]\n",
        "    pitch_small = [row.get('pitch_hz', 0.0) for row in time_matrix_small]\n",
        "    note_freq_small = [row.get('note_freq_hz', 0.0) for row in time_matrix_small]\n",
        "\n",
        "    ax0.plot(times_small, pitch_small, label='Pitch (Hz)', color='b')\n",
        "    ax0.plot(times_small, note_freq_small, label='NoteFreq (Hz)', color='orange', alpha=0.7)\n",
        "    ax0.set_ylabel('Frequency (Hz)')\n",
        "    ax0.set_title('Pitch & Note Frequency')\n",
        "    ax0.legend()\n",
        "\n",
        "    # Subplot 2: spectral flatness & transition score\n",
        "    ax1 = axes[1]\n",
        "    tone_vals = [row.get('tone_to_noise', 0.0) for row in time_matrix_small]\n",
        "    trans_vals = [row.get('transition_score', 0.0) for row in time_matrix_small]\n",
        "    ax1.plot(times_small, tone_vals, label='Spectral Flatness', color='g')\n",
        "    ax1.plot(times_small, trans_vals, label='Transition Score', color='r')\n",
        "    ax1.set_ylabel('Arbitrary scale')\n",
        "    ax1.set_title('Spectral Flatness & Transition Score')\n",
        "    ax1.set_ylim([0,1])\n",
        "    ax1.legend()\n",
        "\n",
        "    # Subplot 3: praat HNR\n",
        "    ax2 = axes[2]\n",
        "    praat_times = [row.get('start_time_s', 0.0) for row in praat_matrix]\n",
        "    praat_hnr = [row.get('praat_hnr', 0.0) for row in praat_matrix]\n",
        "    ax2.plot(praat_times, praat_hnr, label='Praat HNR (dB)', color='purple')\n",
        "    ax2.set_xlabel('Time (s)')\n",
        "    ax2.set_ylabel('HNR (dB)')\n",
        "    ax2.set_title('Praat HNR (2048-chunk)')\n",
        "    ax2.set_ylim([0,30])\n",
        "    ax2.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save Figure 1\n",
        "    fig_name = f\"{base_name}_PitchSpecHNR_{now_str}.png\"\n",
        "    fig_path = os.path.join(OUTPUT_DIR, fig_name)\n",
        "    fig.savefig(fig_path)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    # ------------------------------------------------------------\n",
        "    # Sruti overlay figure\n",
        "    fig_sruti, ax_s = plt.subplots(1, 1, figsize=(12, 4))\n",
        "    times_small = [row.get('time_s', 0.0) for row in time_matrix_small]\n",
        "    pitch_small = [row.get('pitch_hz', 0.0) for row in time_matrix_small]\n",
        "    sruti_labels = [row.get('sruti_class', 'sruti_unknown') for row in time_matrix_small]\n",
        "\n",
        "    # A simple overlay function (as in your code).\n",
        "    # The actual scatter logic is in 'plot_sruti_overlay' above, but we replicate or call it here:\n",
        "    ax_s.plot(times_small, pitch_small, label=\"Pitch contour\")\n",
        "    # Example: also plotting sruti indices in red\n",
        "    import re\n",
        "    sruti_indices = []\n",
        "    for lbl in sruti_labels:\n",
        "        if lbl is None:\n",
        "            lbl = \"sruti_unknown\"\n",
        "        match = re.search(r\"sruti_(\\d+)\", lbl)\n",
        "        if match:\n",
        "            sruti_indices.append(int(match.group(1)))\n",
        "        else:\n",
        "            sruti_indices.append(0)\n",
        "    ax_s.scatter(times_small, sruti_indices, color='red', label=\"Sruti index\")\n",
        "\n",
        "    ax_s.set_xlabel(\"Time (s)\")\n",
        "    ax_s.set_ylabel(\"Pitch / Sruti Index\")\n",
        "    ax_s.legend()\n",
        "    ax_s.set_title(\"22-Sruti Overlay\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save Sruti Figure\n",
        "    sruti_name = f\"{base_name}_SrutiOverlay_{now_str}.png\"\n",
        "    sruti_path = os.path.join(OUTPUT_DIR, sruti_name)\n",
        "    fig_sruti.savefig(sruti_path)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    # ------------------------------------------------------------\n",
        "    # 2) Figure 2: advanced spectrogram features + MFCC + Chromagram\n",
        "    if audio_data is not None and sr is not None:\n",
        "        n_fft_global = min(len(audio_data), FRAME_SIZE)\n",
        "\n",
        "        fig2, axes2 = plt.subplots(3, 1, figsize=(12,10))\n",
        "\n",
        "        # 2.1) Log-frequency spectrogram\n",
        "        D = librosa.stft(y=audio_data, n_fft=n_fft_global, hop_length=HOP_LENGTH)\n",
        "        D_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
        "        axes2[0].set_title('Log-frequency Spectrogram (Advanced)')\n",
        "        img0 = librosa.display.specshow(D_db, sr=sr, hop_length=HOP_LENGTH,\n",
        "                                        x_axis='time', y_axis='log', ax=axes2[0])\n",
        "        fig2.colorbar(img0, ax=axes2[0], format='%+2.0f dB')\n",
        "\n",
        "        # 2.2) MFCC(60)\n",
        "        mfcc_data = librosa.feature.mfcc(y=audio_data, sr=sr, n_mfcc=60,\n",
        "                                         n_fft=n_fft_global, hop_length=HOP_LENGTH)\n",
        "        axes2[1].set_title('MFCC(60)')\n",
        "        img1 = librosa.display.specshow(mfcc_data, sr=sr, hop_length=HOP_LENGTH,\n",
        "                                        x_axis='time', ax=axes2[1])\n",
        "        fig2.colorbar(img1, ax=axes2[1])\n",
        "\n",
        "        # 2.3) Chromagram\n",
        "        chroma_data = librosa.feature.chroma_stft(y=audio_data, sr=sr,\n",
        "                                                  n_fft=n_fft_global, hop_length=HOP_LENGTH)\n",
        "        axes2[2].set_title('Chromagram')\n",
        "        img2 = librosa.display.specshow(chroma_data, sr=sr, hop_length=HOP_LENGTH,\n",
        "                                        x_axis='time', y_axis='chroma', ax=axes2[2])\n",
        "        fig2.colorbar(img2, ax=axes2[2])\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save Figure 2\n",
        "        fig2_name = f\"{base_name}_SpectrogramMFCCChroma_{now_str}.png\"\n",
        "        fig2_path = os.path.join(OUTPUT_DIR, fig2_name)\n",
        "        fig2.savefig(fig2_path)\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "        # --------------------------------------------------------\n",
        "        # 3) Figure 3: chunk-based lines for advanced features (ZCR, centroid, rolloff, RMS, etc.)\n",
        "        times_small_ = [row.get('time_s', 0.0) for row in time_matrix_small]\n",
        "        zcr_vals = [row.get('zcr', 0.0) for row in time_matrix_small]\n",
        "        cent_vals = [row.get('spec_centroid', 0.0) for row in time_matrix_small]\n",
        "        roll_vals = [row.get('spec_rolloff', 0.0) for row in time_matrix_small]\n",
        "        bw_vals = [row.get('spec_bandwidth', 0.0) for row in time_matrix_small]\n",
        "        rms_vals = [row.get('rms_db', 0.0) for row in time_matrix_small]\n",
        "        lufs_vals = [row.get('lufs', 0.0) for row in time_matrix_small]\n",
        "        tempo_vals_small = [row.get('tempo_bpm', 0.0) for row in time_matrix_small]\n",
        "\n",
        "        fig3, axes3 = plt.subplots(8, 1, figsize=(12,22), sharex=True)\n",
        "\n",
        "        # 3.1) ZCR\n",
        "        axes3[0].plot(times_small_, zcr_vals, 'b', label='Zero Crossing Rate')\n",
        "        axes3[0].set_title('Zero Crossing Rate')\n",
        "        axes3[0].set_ylabel('ZCR')\n",
        "        axes3[0].legend()\n",
        "\n",
        "        # 3.2) Spectral Centroid\n",
        "        axes3[1].plot(times_small_, cent_vals, 'g', label='Spectral Centroid')\n",
        "        axes3[1].set_title('Spectral Centroid')\n",
        "        axes3[1].set_ylabel('Hz')\n",
        "        axes3[1].legend()\n",
        "\n",
        "        # 3.3) Rolloff\n",
        "        axes3[2].plot(times_small_, roll_vals, 'r', label='Spectral Rolloff')\n",
        "        axes3[2].set_title('Spectral Rolloff')\n",
        "        axes3[2].set_ylabel('Hz')\n",
        "        axes3[2].legend()\n",
        "\n",
        "        # 3.4) Bandwidth\n",
        "        axes3[3].plot(times_small_, bw_vals, 'm', label='Spectral Bandwidth')\n",
        "        axes3[3].set_title('Spectral Bandwidth')\n",
        "        axes3[3].set_ylabel('Hz')\n",
        "        axes3[3].legend()\n",
        "\n",
        "        # 3.5) RMS(dB)\n",
        "        axes3[4].plot(times_small_, rms_vals, 'orange', label='RMS(dB)')\n",
        "        axes3[4].set_title('RMS(dB) with LUF_CHUNK_SIZE')\n",
        "        axes3[4].set_ylabel('dB')\n",
        "        axes3[4].legend()\n",
        "\n",
        "        # 3.6) LUFS\n",
        "        axes3[5].plot(times_small_, lufs_vals, 'c', label='LUFS')\n",
        "        axes3[5].set_title('LUFS with LUF_CHUNK_SIZE')\n",
        "        axes3[5].set_ylabel('LUFS')\n",
        "        axes3[5].legend()\n",
        "\n",
        "        # 3.7) Tempo (512)\n",
        "        axes3[6].plot(times_small_, tempo_vals_small, 'k', label='Small-chunk Tempo (512)')\n",
        "        axes3[6].set_title('Small-chunk Tempo (512)')\n",
        "        axes3[6].set_ylabel('BPM')\n",
        "        axes3[6].legend()\n",
        "\n",
        "        # 3.8) Medium & Large Tempo\n",
        "        if time_matrix_tempo_medium is not None:\n",
        "            med_times = [row.get('start_time_s', 0.0) for row in time_matrix_tempo_medium]\n",
        "            med_bpm = [row.get('tempo_bpm', 0.0) for row in time_matrix_tempo_medium]\n",
        "            axes3[7].plot(med_times, med_bpm, 'y', label='Medium-chunk Tempo (4096)')\n",
        "\n",
        "        if time_matrix_tempo_large is not None:\n",
        "            large_times = [row.get('start_time_s', 0.0) for row in time_matrix_tempo_large]\n",
        "            large_bpm = [row.get('tempo_bpm', 0.0) for row in time_matrix_tempo_large]\n",
        "            axes3[7].plot(large_times, large_bpm, 'r', label='Large-chunk Tempo (16384)')\n",
        "\n",
        "        axes3[7].set_title('Tempo (4096 & 16384)')\n",
        "        axes3[7].set_ylabel('BPM')\n",
        "        axes3[7].set_xlabel('Time (s)')\n",
        "        axes3[7].legend()\n",
        "\n",
        "        # Overlays for aggregated feedback\n",
        "        if aggregated_items:\n",
        "            issue_colors = {\n",
        "                \"pitch\": \"red\",\n",
        "                \"tempo\": \"blue\",\n",
        "                \"volume\": \"green\",\n",
        "                \"transition\": \"magenta\",\n",
        "                \"tone\": \"brown\"\n",
        "            }\n",
        "            ax_for_agg = axes3[0]  # Example: overlay on ZCR subplot\n",
        "            for group in aggregated_items:\n",
        "                start_t = group[\"start_time\"]\n",
        "                end_t = group[\"end_time\"]\n",
        "                issue_type = group[\"issue_type\"]\n",
        "                color = issue_colors.get(issue_type, \"gray\")\n",
        "                ax_for_agg.axvspan(start_t, end_t, color=color, alpha=0.15)\n",
        "                label_str = f\"{issue_type} - {group.get('issue_level','')}\"\n",
        "                y_lim = ax_for_agg.get_ylim()\n",
        "                ax_for_agg.text(\n",
        "                    start_t,\n",
        "                    y_lim[1]*0.9,  # 10% below top\n",
        "                    label_str,\n",
        "                    color=color,\n",
        "                    fontsize=8,\n",
        "                    alpha=0.8\n",
        "                )\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save Figure 3\n",
        "        fig3_name = f\"{base_name}_ChunkMetrics_{now_str}.png\"\n",
        "        fig3_path = os.path.join(OUTPUT_DIR, fig3_name)\n",
        "        fig3.savefig(fig3_path)\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    # ------------------------------------------------------------\n",
        "    # 4) Advanced vocal features figure (jitter, shimmer, formants, vibrato, etc.)\n",
        "    times_small = [row.get('time_s', 0.0) for row in time_matrix_small]\n",
        "\n",
        "    jitters = [row.get(\"jitter\", 0.0) for row in time_matrix_small]\n",
        "    shimmers = [row.get(\"shimmer\", 0.0) for row in time_matrix_small]\n",
        "    formant_F1 = [row.get(\"formants\", {}).get(\"F1\", 0.0) for row in time_matrix_small]\n",
        "    formant_F2 = [row.get(\"formants\", {}).get(\"F2\", 0.0) for row in time_matrix_small]\n",
        "    formant_F3 = [row.get(\"formants\", {}).get(\"F3\", 0.0) for row in time_matrix_small]\n",
        "    vib_extents = [row.get(\"vibrato_extent\", 0.0) for row in time_matrix_small]\n",
        "    vib_rates = [row.get(\"vibrato_rate\", 0.0) for row in time_matrix_small]\n",
        "\n",
        "    fig_adv, axs_adv = plt.subplots(2, 3, figsize=(15, 10))\n",
        "\n",
        "    # 4.1) Jitter\n",
        "    axs_adv[0, 0].plot(times_small, jitters, marker='o', linestyle='-', color='blue', label='Jitter')\n",
        "    axs_adv[0, 0].set_title(\"Jitter (local) over Time\")\n",
        "    axs_adv[0, 0].set_xlabel(\"Time (s)\")\n",
        "    axs_adv[0, 0].set_ylabel(\"Jitter\")\n",
        "    axs_adv[0, 0].legend()\n",
        "\n",
        "    # 4.2) Shimmer\n",
        "    axs_adv[0, 1].plot(times_small, shimmers, marker='o', linestyle='-', color='orange', label='Shimmer')\n",
        "    axs_adv[0, 1].set_title(\"Shimmer (local) over Time\")\n",
        "    axs_adv[0, 1].set_xlabel(\"Time (s)\")\n",
        "    axs_adv[0, 1].set_ylabel(\"Shimmer\")\n",
        "    axs_adv[0, 1].legend()\n",
        "\n",
        "    # 4.3) Formants\n",
        "    axs_adv[0, 2].plot(times_small, formant_F1, marker='o', linestyle='-', color='green', label='F1')\n",
        "    axs_adv[0, 2].plot(times_small, formant_F2, marker='o', linestyle='-', color='red', label='F2')\n",
        "    axs_adv[0, 2].plot(times_small, formant_F3, marker='o', linestyle='-', color='purple', label='F3')\n",
        "    axs_adv[0, 2].set_title(\"Formant Frequencies over Time\")\n",
        "    axs_adv[0, 2].set_xlabel(\"Time (s)\")\n",
        "    axs_adv[0, 2].set_ylabel(\"Frequency (Hz)\")\n",
        "    axs_adv[0, 2].legend()\n",
        "\n",
        "    # 4.4) Vibrato Extent\n",
        "    axs_adv[1, 0].plot(times_small, vib_extents, marker='o', linestyle='-', color='magenta', label='Vibrato Extent')\n",
        "    axs_adv[1, 0].set_title(\"Vibrato Extent over Time\")\n",
        "    axs_adv[1, 0].set_xlabel(\"Time (s)\")\n",
        "    axs_adv[1, 0].set_ylabel(\"Vibrato Extent (Hz)\")\n",
        "    axs_adv[1, 0].legend()\n",
        "\n",
        "    # 4.5) Vibrato Rate\n",
        "    axs_adv[1, 1].plot(times_small, vib_rates, marker='o', linestyle='-', color='brown', label='Vibrato Rate')\n",
        "    axs_adv[1, 1].set_title(\"Vibrato Rate over Time\")\n",
        "    axs_adv[1, 1].set_xlabel(\"Time (s)\")\n",
        "    axs_adv[1, 1].set_ylabel(\"Vibrato Rate (Hz)\")\n",
        "    axs_adv[1, 1].legend()\n",
        "\n",
        "    # 4.6) Note-level advanced features (attack time, etc.)\n",
        "    if analysis_dict and \"advanced_note_features\" in analysis_dict:\n",
        "        note_feats = analysis_dict[\"advanced_note_features\"]\n",
        "        note_start_times = []\n",
        "        note_attack_times = []\n",
        "        for note in note_feats:\n",
        "            note_time = note.get(\"time_s\", None)\n",
        "            for detail in note.get(\"asd_details\", []):\n",
        "                attack = detail.get(\"attack\", None)\n",
        "                if note_time is not None and attack is not None:\n",
        "                    note_start_times.append(note_time)\n",
        "                    note_attack_times.append(attack)\n",
        "        if note_attack_times:\n",
        "            axs_adv[1, 2].plot(\n",
        "                note_start_times, \n",
        "                note_attack_times, \n",
        "                marker='o', linestyle='-', \n",
        "                color='cyan', \n",
        "                label=\"Attack Time\"\n",
        "            )\n",
        "            axs_adv[1, 2].set_title(\"Note Attack Time over Time\")\n",
        "            axs_adv[1, 2].set_xlabel(\"Time (s)\")\n",
        "            axs_adv[1, 2].set_ylabel(\"Attack Time (s)\")\n",
        "            axs_adv[1, 2].legend()\n",
        "        else:\n",
        "            axs_adv[1, 2].text(0.5, 0.5, \"No Attack Data\", ha='center', va='center')\n",
        "    else:\n",
        "        axs_adv[1, 2].text(0.5, 0.5, \"No Attack Data\", ha='center', va='center')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save Advanced Features Figure\n",
        "    fig_adv_name = f\"{base_name}_AdvancedVocalFeatures_{now_str}.png\"\n",
        "    fig_adv_path = os.path.join(OUTPUT_DIR, fig_adv_name)\n",
        "    fig_adv.savefig(fig_adv_path)\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78c862fd",
      "metadata": {},
      "source": [
        "## 11.5: Classification Summaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96385aed",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 14. Classification Summaries (Global/Track-Level Distribution Counts)\n",
        "from collections import Counter\n",
        "\n",
        "def summarize_classifications(\n",
        "    time_matrix_small,\n",
        "    time_matrix_praat,\n",
        "    time_matrix_tempo_medium,\n",
        "    time_matrix_tempo_large\n",
        "):\n",
        "    \"\"\"\n",
        "    Gathers global classification counts across all relevant 'time_matrix' data.\n",
        "    Returns a dictionary, e.g.:\n",
        "\n",
        "    {\n",
        "      \"pitch_accuracy_category\": {\"perfect\": 12, \"good\": 25, ...},\n",
        "      \"tone_to_noise_category\": {...},\n",
        "      \"transition_category\": {...},\n",
        "      \"rms_db_category\": {...},\n",
        "      \"lufs_category\": {...},\n",
        "      \"tempo_bpm_category_small\": {...},\n",
        "      \"tempo_bpm_category_medium\": {...},\n",
        "      \"tempo_bpm_category_large\": {...},\n",
        "      \"praat_hnr_category\": {...}\n",
        "    }\n",
        "\n",
        "    You can adjust or expand these counts as needed.\n",
        "    \"\"\"\n",
        "\n",
        "    # Helper function to safely gather a given field from a list of dictionaries\n",
        "    def gather_categories(matrix, field_name):\n",
        "        # returns e.g. [\"perfect\", \"good\", \"fair\", ...]\n",
        "        cat_list = []\n",
        "        for row in matrix:\n",
        "            val = row.get(field_name, None)\n",
        "            if val is not None:\n",
        "                cat_list.append(val)\n",
        "        return cat_list\n",
        "\n",
        "    # 1) pitch_accuracy_category from time_matrix_small\n",
        "    pitch_categories = gather_categories(time_matrix_small, \"pitch_accuracy_category\")\n",
        "\n",
        "    # 2) tone_to_noise_category from time_matrix_small\n",
        "    tone_categories = gather_categories(time_matrix_small, \"tone_to_noise_category\")\n",
        "\n",
        "    # 3) transition_category from time_matrix_small\n",
        "    transition_categories = gather_categories(time_matrix_small, \"transition_category\")\n",
        "\n",
        "    # 4) RMS & LUFS from time_matrix_small\n",
        "    rms_categories = gather_categories(time_matrix_small, \"rms_db_category\")\n",
        "    lufs_categories = gather_categories(time_matrix_small, \"lufs_category\")\n",
        "\n",
        "    # 5) tempo BPM categories (512) from time_matrix_small\n",
        "    tempo_small_categories = gather_categories(time_matrix_small, \"tempo_bpm_category\")\n",
        "\n",
        "    # 6) medium-tempo matrix\n",
        "    tempo_medium_categories = gather_categories(time_matrix_tempo_medium, \"tempo_bpm_category\")\n",
        "\n",
        "    # 7) large-tempo matrix\n",
        "    tempo_large_categories = gather_categories(time_matrix_tempo_large, \"tempo_bpm_category\")\n",
        "\n",
        "    # 8) Praat HNR\n",
        "    praat_categories = gather_categories(time_matrix_praat, \"praat_hnr_category\")\n",
        "\n",
        "    # Build a dictionary of counts for each classification dimension\n",
        "    classification_summary = {\n",
        "        \"pitch_accuracy_category\": dict(Counter(pitch_categories)),\n",
        "        \"tone_to_noise_category\": dict(Counter(tone_categories)),\n",
        "        \"transition_category\": dict(Counter(transition_categories)),\n",
        "        \"rms_db_category\": dict(Counter(rms_categories)),\n",
        "        \"lufs_category\": dict(Counter(lufs_categories)),\n",
        "        \"tempo_bpm_category_small\": dict(Counter(tempo_small_categories)),\n",
        "        \"tempo_bpm_category_medium\": dict(Counter(tempo_medium_categories)),\n",
        "        \"tempo_bpm_category_large\": dict(Counter(tempo_large_categories)),\n",
        "        \"praat_hnr_category\": dict(Counter(praat_categories))\n",
        "    }\n",
        "\n",
        "    return classification_summary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b99d9b0",
      "metadata": {},
      "source": [
        "## 15. Generate Actionable Feedback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a8a6812",
      "metadata": {},
      "outputs": [],
      "source": [
        "###################################################\n",
        "# 15. Actionable Feedback Generation (New Module)\n",
        "###################################################\n",
        "\n",
        "def generate_detailed_feedback(analysis_dict):\n",
        "    \"\"\"\n",
        "    Loop over 'time_matrix_small' and produce chunk-level feedback items in a structured format.\n",
        "    E.g. a list of:\n",
        "      {\n",
        "        \"start_time\": float,\n",
        "        \"end_time\": float,\n",
        "        \"issue_type\": \"pitch\" or \"tempo\" or \"volume\" etc.,\n",
        "        \"issue_level\": \"very poor\" or \"too fast\", etc.,\n",
        "        \"message\": \"Time ~2.54s: pitch accuracy was very poor...\"\n",
        "      }\n",
        "    We'll later use this for more advanced aggregation.\n",
        "    \"\"\"\n",
        "    time_matrix_small = analysis_dict.get(\"time_matrix_small\", [])\n",
        "    sr = analysis_dict[\"sample_rate\"]  # or from track-level\n",
        "    hop_duration = float(512) / sr     # each chunk is ~0.0116s at 44.1kHz, but you might have a bigger chunk\n",
        "\n",
        "    feedback_items = []\n",
        "\n",
        "    for chunk in time_matrix_small:\n",
        "        start_time = chunk.get(\"time_s\", 0.0)\n",
        "        end_time = start_time + hop_duration\n",
        "\n",
        "        pitch_cat = chunk.get(\"pitch_accuracy_category\", \"unknown\")\n",
        "        rms_cat = chunk.get(\"rms_db_category\", \"unknown\")\n",
        "        tempo_cat = chunk.get(\"tempo_bpm_category\", \"unknown\")\n",
        "        transition_cat = chunk.get(\"transition_category\", \"unknown\")\n",
        "        tone_cat = chunk.get(\"tone_to_noise_category\", \"unknown\")\n",
        "\n",
        "        # 1) Pitch Issues\n",
        "        if pitch_cat in [\"poor\", \"very poor\"]:\n",
        "            feedback_items.append({\n",
        "                \"start_time\": start_time,\n",
        "                \"end_time\": end_time,\n",
        "                \"issue_type\": \"pitch\",\n",
        "                \"issue_level\": pitch_cat,\n",
        "                \"message\": f\"Time ~{start_time:.2f}s: pitch accuracy was {pitch_cat}.\"\n",
        "            })\n",
        "\n",
        "        # 2) Volume Issues (RMS)\n",
        "        if rms_cat in [\"very loud\", \"very soft\"]:\n",
        "            feedback_items.append({\n",
        "                \"start_time\": start_time,\n",
        "                \"end_time\": end_time,\n",
        "                \"issue_type\": \"volume\",\n",
        "                \"issue_level\": rms_cat,\n",
        "                \"message\": f\"Time ~{start_time:.2f}s: volume is {rms_cat}.\"\n",
        "            })\n",
        "\n",
        "        # 3) Tempo Issues\n",
        "        if tempo_cat in [\"fast\", \"very fast\", \"slow\", \"very slow\"]:\n",
        "            feedback_items.append({\n",
        "                \"start_time\": start_time,\n",
        "                \"end_time\": end_time,\n",
        "                \"issue_type\": \"tempo\",\n",
        "                \"issue_level\": tempo_cat,\n",
        "                \"message\": f\"Time ~{start_time:.2f}s: tempo is {tempo_cat}.\"\n",
        "            })\n",
        "\n",
        "        # 4) Transition Issues\n",
        "        if transition_cat == \"abrupt\":\n",
        "            feedback_items.append({\n",
        "                \"start_time\": start_time,\n",
        "                \"end_time\": end_time,\n",
        "                \"issue_type\": \"transition\",\n",
        "                \"issue_level\": \"abrupt\",\n",
        "                \"message\": f\"Time ~{start_time:.2f}s: pitch transition was abrupt.\"\n",
        "            })\n",
        "\n",
        "        # 5) Tone-Noisy\n",
        "        if tone_cat in [\"noisy\", \"very noisy\"]:\n",
        "            feedback_items.append({\n",
        "                \"start_time\": start_time,\n",
        "                \"end_time\": end_time,\n",
        "                \"issue_type\": \"tone\",\n",
        "                \"issue_level\": tone_cat,\n",
        "                \"message\": f\"Time ~{start_time:.2f}s: tone is {tone_cat}.\"\n",
        "            })\n",
        "\n",
        "    return feedback_items\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def aggregate_feedback(feedback_items, max_gap=0.5):\n",
        "    \"\"\"\n",
        "    Combine consecutive issues of the same type & level into a single aggregated message.\n",
        "    :param feedback_items: list of dicts from generate_detailed_feedback.\n",
        "    :param max_gap: max allowed gap (in seconds) to still merge consecutive issues.\n",
        "    :return: a list of aggregated feedback dicts.\n",
        "    \"\"\"\n",
        "    if not feedback_items:\n",
        "        return []\n",
        "\n",
        "    # Sort by start time\n",
        "    feedback_sorted = sorted(feedback_items, key=lambda x: x[\"start_time\"])\n",
        "    aggregated = []\n",
        "\n",
        "    # We'll accumulate consecutive items in a \"current group\" if they match.\n",
        "    current_group = None\n",
        "\n",
        "    for item in feedback_sorted:\n",
        "        if current_group is None:\n",
        "            # start a new group\n",
        "            current_group = {\n",
        "                \"issue_type\": item[\"issue_type\"],\n",
        "                \"issue_level\": item[\"issue_level\"],\n",
        "                \"start_time\": item[\"start_time\"],\n",
        "                \"end_time\": item[\"end_time\"],\n",
        "            }\n",
        "        else:\n",
        "            # check if we can merge with the current group\n",
        "            same_issue = (\n",
        "                (item[\"issue_type\"] == current_group[\"issue_type\"]) and\n",
        "                (item[\"issue_level\"] == current_group[\"issue_level\"])\n",
        "            )\n",
        "            small_gap = (item[\"start_time\"] - current_group[\"end_time\"]) <= max_gap\n",
        "\n",
        "            if same_issue and small_gap:\n",
        "                # expand current group end_time\n",
        "                current_group[\"end_time\"] = item[\"end_time\"]\n",
        "            else:\n",
        "                # finalize the current group\n",
        "                aggregated.append(current_group)\n",
        "                # start a new group\n",
        "                current_group = {\n",
        "                    \"issue_type\": item[\"issue_type\"],\n",
        "                    \"issue_level\": item[\"issue_level\"],\n",
        "                    \"start_time\": item[\"start_time\"],\n",
        "                    \"end_time\": item[\"end_time\"],\n",
        "                }\n",
        "\n",
        "    # finalize the last group if it exists\n",
        "    if current_group:\n",
        "        aggregated.append(current_group)\n",
        "\n",
        "    return aggregated\n",
        "\n",
        "\n",
        "\n",
        "def generate_aggregate_messages(aggregated_items):\n",
        "    \"\"\"\n",
        "    Convert the aggregated dictionary items into text messages for the user.\n",
        "    \"\"\"\n",
        "    messages = []\n",
        "    for group in aggregated_items:\n",
        "        issue_type = group[\"issue_type\"]\n",
        "        issue_level = group[\"issue_level\"]\n",
        "        start_t = group[\"start_time\"]\n",
        "        end_t = group[\"end_time\"]\n",
        "\n",
        "        # You can tailor the text per issue_type\n",
        "        if issue_type == \"pitch\":\n",
        "            msg = (\n",
        "                f\"From {start_t:.2f}s to {end_t:.2f}s, your pitch accuracy was consistently {issue_level}. \"\n",
        "                \"Focus on matching the target pitch in this section.\"\n",
        "            )\n",
        "        elif issue_type == \"tempo\":\n",
        "            msg = (\n",
        "                f\"From {start_t:.2f}s to {end_t:.2f}s, your tempo was {issue_level}. \"\n",
        "                \"Try adjusting your speed in that phrase.\"\n",
        "            )\n",
        "        elif issue_type == \"volume\":\n",
        "            msg = (\n",
        "                f\"From {start_t:.2f}s to {end_t:.2f}s, your volume was {issue_level}. \"\n",
        "                \"Consider moderating your dynamics during that passage.\"\n",
        "            )\n",
        "        elif issue_type == \"transition\":\n",
        "            msg = (\n",
        "                f\"From {start_t:.2f}s to {end_t:.2f}s, transitions were {issue_level}. \"\n",
        "                \"Practice smoother note changes.\"\n",
        "            )\n",
        "        elif issue_type == \"tone\":\n",
        "            msg = (\n",
        "                f\"From {start_t:.2f}s to {end_t:.2f}s, tone was {issue_level}. \"\n",
        "                \"Work on clarity and resonance for a more refined sound.\"\n",
        "            )\n",
        "        else:\n",
        "            msg = (\n",
        "                f\"From {start_t:.2f}s to {end_t:.2f}s, there was an issue with {issue_type} ({issue_level}).\"\n",
        "            )\n",
        "\n",
        "        messages.append(msg)\n",
        "\n",
        "    return messages\n",
        "\n",
        "\n",
        "\n",
        "###################################################\n",
        "# High-Level Aggregation (New Module)\n",
        "###################################################\n",
        "\n",
        "def aggregate_feedback_high_level(feedback_items, max_gap=5.0):\n",
        "    \"\"\"\n",
        "    Further aggregate detailed feedback items over larger time spans.\n",
        "    \n",
        "    Groups feedback items by the same issue_type and issue_level if the gap between them is less than max_gap seconds.\n",
        "    Returns a list of aggregated items where each includes:\n",
        "      - issue_type, issue_level, start_time, end_time, and a count of events in that interval.\n",
        "    \"\"\"\n",
        "    if not feedback_items:\n",
        "        return []\n",
        "\n",
        "    # Sort the detailed feedback by start time.\n",
        "    sorted_items = sorted(feedback_items, key=lambda x: x[\"start_time\"])\n",
        "    high_level = []\n",
        "    current_group = None\n",
        "\n",
        "    for item in sorted_items:\n",
        "        if current_group is None:\n",
        "            current_group = {\n",
        "                \"issue_type\": item[\"issue_type\"],\n",
        "                \"issue_level\": item[\"issue_level\"],\n",
        "                \"start_time\": item[\"start_time\"],\n",
        "                \"end_time\": item[\"end_time\"],\n",
        "                \"count\": 1,\n",
        "            }\n",
        "        else:\n",
        "            # Merge if same issue and gap is less than max_gap.\n",
        "            if (item[\"issue_type\"] == current_group[\"issue_type\"] and \n",
        "                item[\"issue_level\"] == current_group[\"issue_level\"] and \n",
        "                (item[\"start_time\"] - current_group[\"end_time\"]) <= max_gap):\n",
        "                current_group[\"end_time\"] = item[\"end_time\"]\n",
        "                current_group[\"count\"] += 1\n",
        "            else:\n",
        "                high_level.append(current_group)\n",
        "                current_group = {\n",
        "                    \"issue_type\": item[\"issue_type\"],\n",
        "                    \"issue_level\": item[\"issue_level\"],\n",
        "                    \"start_time\": item[\"start_time\"],\n",
        "                    \"end_time\": item[\"end_time\"],\n",
        "                    \"count\": 1,\n",
        "                }\n",
        "    if current_group:\n",
        "        high_level.append(current_group)\n",
        "\n",
        "    return high_level\n",
        "\n",
        "\n",
        "def aggregate_feedback_actionable(feedback_items, max_gap=5.0, min_duration=3.0, min_count=3):\n",
        "    \"\"\"\n",
        "    Further aggregate detailed feedback items over longer time spans and filter out groups that are too short or isolated.\n",
        "    \n",
        "    - max_gap: gap in seconds to merge consecutive feedback items.\n",
        "    - min_duration: minimum duration (in seconds) for the group to be considered actionable.\n",
        "    - min_count: minimum number of occurrences within the group.\n",
        "    \n",
        "    Returns only those groups that meet the thresholds.\n",
        "    \"\"\"\n",
        "    high_level = aggregate_feedback_high_level(feedback_items, max_gap=max_gap)\n",
        "    actionable = []\n",
        "    for group in high_level:\n",
        "        duration = group[\"end_time\"] - group[\"start_time\"]\n",
        "        if duration >= min_duration and group[\"count\"] >= min_count:\n",
        "            actionable.append(group)\n",
        "    return actionable\n",
        "\n",
        "\n",
        "def generate_high_level_messages(aggregated_high_items):\n",
        "    \"\"\"\n",
        "    Convert the high-level aggregated feedback items into human-friendly summary messages.\n",
        "    \"\"\"\n",
        "    messages = []\n",
        "    for group in aggregated_high_items:\n",
        "        issue_type = group[\"issue_type\"]\n",
        "        issue_level = group[\"issue_level\"]\n",
        "        start_t = group[\"start_time\"]\n",
        "        end_t = group[\"end_time\"]\n",
        "        count = group[\"count\"]\n",
        "\n",
        "        if issue_type == \"pitch\":\n",
        "            msg = (f\"Between {start_t:.2f}s and {end_t:.2f}s, you had pitch issues \"\n",
        "                   f\"({issue_level}) on {count} separate occasions.\")\n",
        "        elif issue_type == \"tempo\":\n",
        "            msg = (f\"Between {start_t:.2f}s and {end_t:.2f}s, your tempo was {issue_level} \"\n",
        "                   f\"on {count} separate occasions.\")\n",
        "        elif issue_type == \"volume\":\n",
        "            msg = (f\"Between {start_t:.2f}s and {end_t:.2f}s, your volume was {issue_level} \"\n",
        "                   f\"on {count} separate occasions.\")\n",
        "        elif issue_type == \"transition\":\n",
        "            msg = (f\"Between {start_t:.2f}s and {end_t:.2f}s, transitions were {issue_level} \"\n",
        "                   f\"on {count} occasions.\")\n",
        "        elif issue_type == \"tone\":\n",
        "            msg = (f\"Between {start_t:.2f}s and {end_t:.2f}s, your tone was {issue_level} \"\n",
        "                   f\"on {count} separate occasions.\")\n",
        "        else:\n",
        "            msg = (f\"Between {start_t:.2f}s and {end_t:.2f}s, there were {count} instances of {issue_type} issues ({issue_level}).\")\n",
        "        messages.append(msg)\n",
        "    return messages\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e78cdc2",
      "metadata": {},
      "source": [
        "## 19.5: Some helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3c142c6",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_advanced_stats(time_matrix):\n",
        "    \"\"\"Computes averages and standard deviations for advanced vocal features, replacing NaNs.\"\"\"\n",
        "    def safe_stat(values):\n",
        "        \"\"\"Returns mean and std deviation, replacing NaNs with 0.0.\"\"\"\n",
        "        values = np.array(values, dtype=np.float32)\n",
        "        values = values[~np.isnan(values)]  # Remove NaNs before computation\n",
        "        if len(values) == 0:\n",
        "            return 0.0, 0.0  # Avoid NaNs in stats\n",
        "        return float(np.mean(values)), float(np.std(values))\n",
        "\n",
        "    jitter_vals = [row.get(\"jitter\", 0) for row in time_matrix]\n",
        "    shimmer_vals = [row.get(\"shimmer\", 0) for row in time_matrix]\n",
        "    F1_vals = [row.get(\"formants\", {}).get(\"F1\", 0) for row in time_matrix]\n",
        "    F2_vals = [row.get(\"formants\", {}).get(\"F2\", 0) for row in time_matrix]\n",
        "    F3_vals = [row.get(\"formants\", {}).get(\"F3\", 0) for row in time_matrix]\n",
        "    vib_extents = [row.get(\"vibrato_extent\", 0) for row in time_matrix]\n",
        "    vib_rates = [row.get(\"vibrato_rate\", 0) for row in time_matrix]\n",
        "\n",
        "    return {\n",
        "        \"avg_jitter\": safe_stat(jitter_vals)[0],\n",
        "        \"std_jitter\": safe_stat(jitter_vals)[1],\n",
        "        \"avg_shimmer\": safe_stat(shimmer_vals)[0],\n",
        "        \"std_shimmer\": safe_stat(shimmer_vals)[1],\n",
        "        \"avg_F1\": safe_stat(F1_vals)[0],\n",
        "        \"std_F1\": safe_stat(F1_vals)[1],\n",
        "        \"avg_F2\": safe_stat(F2_vals)[0],\n",
        "        \"std_F2\": safe_stat(F2_vals)[1],\n",
        "        \"avg_F3\": safe_stat(F3_vals)[0],\n",
        "        \"std_F3\": safe_stat(F3_vals)[1],\n",
        "        \"avg_vibrato_extent\": safe_stat(vib_extents)[0],\n",
        "        \"std_vibrato_extent\": safe_stat(vib_extents)[1],\n",
        "        \"avg_vibrato_rate\": safe_stat(vib_rates)[0],\n",
        "        \"std_vibrato_rate\": safe_stat(vib_rates)[1],\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8ca8cdf",
      "metadata": {},
      "source": [
        "## 20. Quantum Computing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11f3b26e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import logging\n",
        "from qiskit import QuantumCircuit, transpile\n",
        "from qiskit_aer import AerSimulator  # Updated import for Qiskit Aer\n",
        "\n",
        "# Configure logging to show only INFO-level messages.\n",
        "# logging.basicConfig(level=logging.INFO,\n",
        "#                    format='[%(levelname)s] %(asctime)s - %(message)s',\n",
        "#                    datefmt='%H:%M:%S')\n",
        "\n",
        "\n",
        "# --- Helper function to encode a detected raga ---\n",
        "def encode_raga(raga):\n",
        "    \"\"\"\n",
        "    Encodes a raga string into a numerical value.\n",
        "    A simple encoding: if raga is a string, compute the sum of the Unicode code points \n",
        "    modulo 100 and normalize to [0, 1]. Replace this with a domain-specific encoding as needed.\n",
        "    \"\"\"\n",
        "    if raga is None:\n",
        "        return 0.0\n",
        "    if isinstance(raga, str):\n",
        "        return (sum(ord(c) for c in raga) % 100) / 100.0\n",
        "    try:\n",
        "        return float(raga)\n",
        "    except Exception:\n",
        "        return 0.0\n",
        "\n",
        "\n",
        "def scale_features_individually(features, lower_bounds, upper_bounds):\n",
        "    \"\"\"\n",
        "    Scales each feature individually from its own [lower, upper] range to [0, 2*pi].\n",
        "    - features: list of raw feature values.\n",
        "    - lower_bounds: list of lower bounds for each feature.\n",
        "    - upper_bounds: list of upper bounds for each feature.\n",
        "    Returns a list of scaled angles.\n",
        "    \"\"\"\n",
        "    scaled = []\n",
        "    for f, l, u in zip(features, lower_bounds, upper_bounds):\n",
        "        spread = u - l if u != l else 1\n",
        "        scaled_angle = ((f - l) / spread) * 2 * np.pi\n",
        "        scaled.append(scaled_angle)\n",
        "    return scaled\n",
        "\n",
        "\n",
        "def nonlinear_scale_features(features):\n",
        "    \"\"\"\n",
        "    Scales a list of features into rotation angles in [0, 2π] using an arctan transformation.\n",
        "    The transformation centers the features around their median and compresses them based on\n",
        "    the median absolute deviation (MAD).\n",
        "    \n",
        "    The formula for each feature f is:\n",
        "       angle = [arctan(scaling_factor * (f - median)) + π/2] * 2\n",
        "    where scaling_factor = 1 / MAD (with a fallback if MAD is zero).\n",
        "    \"\"\"\n",
        "    features = np.asarray(features, dtype=float)\n",
        "    median_val = np.median(features)\n",
        "    mad = np.median(np.abs(features - median_val))\n",
        "    if mad == 0:\n",
        "        mad = np.std(features) if np.std(features) > 0 else 1.0\n",
        "    scaling_factor = 1.0 / mad\n",
        "    \n",
        "    scaled = []\n",
        "    for f in features:\n",
        "        angle = (np.arctan(scaling_factor * (f - median_val)) + (np.pi / 2)) * 2\n",
        "        scaled.append(angle)\n",
        "    return scaled\n",
        "\n",
        "\n",
        "# --- Function to compute quantum features from analysis_dict ---\n",
        "def _compute_quantum_features_from_analysis(analysis_dict):\n",
        "    \"\"\"\n",
        "    Extracts metrics from analysis_dict and returns a feature vector.\n",
        "    \n",
        "    The full feature set normally includes (1 to 25):\n",
        "      1)  avg_dev_cents,\n",
        "      2)  std_dev_cents,\n",
        "      3)  avg_tnr,\n",
        "      4)  avg_praat_hnr,\n",
        "      5)  rms_db_mean,\n",
        "      6)  lufs_mean,\n",
        "      7)  mean_pitch,\n",
        "      8)  mean_praat_hnr_tmatrix,\n",
        "      9)  detected_raga,\n",
        "      10) off_pitch_count,\n",
        "      11) avg_tempo,\n",
        "      12) avg_jitter,\n",
        "      13) std_jitter,\n",
        "      14) avg_shimmer,\n",
        "      15) std_shimmer,\n",
        "      16) avg_F1,\n",
        "      17) std_F1,\n",
        "      18) avg_F2,\n",
        "      19) std_F2,\n",
        "      20) avg_F3,\n",
        "      21) std_F3,\n",
        "      22) avg_vibrato_extent,\n",
        "      23) std_vibrato_extent,\n",
        "      24) avg_vibrato_rate,\n",
        "      25) std_vibrato_rate.\n",
        "\n",
        "    We're commenting out certain features per your request:\n",
        "      - avg_dev_cents (1)\n",
        "      - rms_db_mean (5)\n",
        "      - lufs_mean (6)\n",
        "      - mean_pitch (7)\n",
        "      - avg_tempo (11)\n",
        "      - avg_jitter (12)\n",
        "      - avg_shimmer (14)\n",
        "      - avg_F1 (16)\n",
        "      - avg_F2 (18)\n",
        "      - avg_F3 (20)\n",
        "      - avg_vibrato_extent (22)\n",
        "      - avg_vibrato_rate (24)\n",
        "    \"\"\"\n",
        "    results = analysis_dict.get(\"results\", {})\n",
        "    avg_dev_cents = results.get(\"average_dev_cents\", 0.0)\n",
        "    std_dev_cents = results.get(\"std_dev_cents\", 0.0)\n",
        "    avg_tnr = results.get(\"avg_tnr\", 0.0)\n",
        "    avg_hnr = results.get(\"avg_praat_hnr\", 0.0)\n",
        "\n",
        "    dyn_sum = analysis_dict.get(\"dynamics_summary\", {})\n",
        "    rms_db_mean = dyn_sum.get(\"rms_db\", {}).get(\"mean\", -40.0)\n",
        "    lufs_mean = dyn_sum.get(\"lufs\", {}).get(\"mean\", -35.0)\n",
        "\n",
        "    time_matrix_small = analysis_dict.get(\"time_matrix_small\", [])\n",
        "    time_matrix_praat = analysis_dict.get(\"time_matrix_praat\", [])\n",
        "    pitch_vals = [row.get(\"pitch_hz\", 0) for row in time_matrix_small if row.get(\"pitch_hz\", 0) > 0]\n",
        "    mean_pitch = float(np.mean(pitch_vals)) if pitch_vals else 0.0\n",
        "    hnr_vals = [row.get(\"praat_hnr\", 0) for row in time_matrix_praat if row.get(\"praat_hnr\", 0) > 0]\n",
        "    mean_praat_hnr_tmatrix = float(np.mean(hnr_vals)) if hnr_vals else 0.0\n",
        "\n",
        "    detected_raga_raw = analysis_dict.get(\"detected_raga\", None)\n",
        "    detected_raga = encode_raga(detected_raga_raw)\n",
        "    \n",
        "    off_pitch_count = analysis_dict.get(\"off_pitch_count\", 0)\n",
        "    avg_tempo = analysis_dict.get(\"avg_tempo\", 0.0)\n",
        "    \n",
        "    # Advanced vocal stats\n",
        "    adv_stats = analysis_dict.get(\"advanced_vocal_stats\", {})\n",
        "    # We'll explicitly log the advanced features:\n",
        "    #logging.info(f\"Advanced features: {adv_stats}\")\n",
        "\n",
        "    avg_jitter = adv_stats.get(\"avg_jitter\", 0.0)\n",
        "    std_jitter = adv_stats.get(\"std_jitter\", 0.0)\n",
        "    avg_shimmer = adv_stats.get(\"avg_shimmer\", 0.0)\n",
        "    std_shimmer = adv_stats.get(\"std_shimmer\", 0.0)\n",
        "    avg_F1 = adv_stats.get(\"avg_F1\", 0.0)\n",
        "    std_F1 = adv_stats.get(\"std_F1\", 0.0)\n",
        "    avg_F2 = adv_stats.get(\"avg_F2\", 0.0)\n",
        "    std_F2 = adv_stats.get(\"std_F2\", 0.0)\n",
        "    avg_F3 = adv_stats.get(\"avg_F3\", 0.0)\n",
        "    std_F3 = adv_stats.get(\"std_F3\", 0.0)\n",
        "    avg_vib_extent = adv_stats.get(\"avg_vibrato_extent\", 0.0)\n",
        "    std_vib_extent = adv_stats.get(\"std_vibrato_extent\", 0.0)\n",
        "    avg_vib_rate = adv_stats.get(\"avg_vibrato_rate\", 0.0)\n",
        "    std_vib_rate = adv_stats.get(\"std_vibrato_rate\", 0.0)\n",
        "    \n",
        "    # Build the feature vector, with certain features commented out\n",
        "    feature_vector = [\n",
        "        avg_dev_cents,  # (1)\n",
        "        std_dev_cents,    # (2)\n",
        "        avg_tnr,          # (3)\n",
        "        avg_hnr,          # (4)\n",
        "        rms_db_mean,    # (5)\n",
        "        # lufs_mean,      # (6)\n",
        "        # mean_pitch,     # (7)\n",
        "        mean_praat_hnr_tmatrix,  # (8)\n",
        "        detected_raga,    # (9)\n",
        "        off_pitch_count,  # (10)\n",
        "        # avg_tempo,      # (11)\n",
        "        avg_jitter,     # (12)\n",
        "        std_jitter,       # (13)\n",
        "        # avg_shimmer,    # (14)\n",
        "        std_shimmer,      # (15)\n",
        "        # avg_F1,         # (16)\n",
        "        # std_F1,           # (17)\n",
        "        # avg_F2,         # (18)\n",
        "        # std_F2,           # (19)\n",
        "        # avg_F3,         # (20)\n",
        "        # std_F3,           # (21)\n",
        "        # avg_vib_extent, # (22)\n",
        "        std_vib_extent,   # (23)\n",
        "        # avg_vib_rate,   # (24)\n",
        "        std_vib_rate      # (25)\n",
        "    ]\n",
        "    \n",
        "    # Only print the final feature vector\n",
        "    #logging.info(f\"Final feature vector: {feature_vector}\")\n",
        "    return feature_vector\n",
        "\n",
        "\n",
        "def quantum_pipeline_complex(feature_vector):\n",
        "    \"\"\"\n",
        "    Builds and runs a more complex quantum circuit with scaled features.\n",
        "    \"\"\"\n",
        "    num_qubits = len(feature_vector)\n",
        "    \n",
        "    # Scale the features\n",
        "    scaled_features = nonlinear_scale_features(feature_vector)\n",
        "    \n",
        "    # Print all scaled angles\n",
        "    #for i, angle in enumerate(scaled_features):\n",
        "    #    logging.info(f\"Feature {i}: scaled_angle = {angle:.4f}\")\n",
        "\n",
        "    qc = QuantumCircuit(num_qubits, num_qubits)\n",
        "\n",
        "    # 1. Initial Hadamard layer\n",
        "    for q in range(num_qubits):\n",
        "        qc.h(q)\n",
        "\n",
        "    # 2. First data-encoding layer (Ry)\n",
        "    for q, angle in enumerate(scaled_features):\n",
        "        qc.ry(angle, q)\n",
        "\n",
        "    # 3. First entangling layer: ring of CX\n",
        "    for q in range(num_qubits):\n",
        "        qc.cx(q, (q + 1) % num_qubits)\n",
        "\n",
        "    # 4. Second data-encoding layer (Rz)\n",
        "    for q, angle in enumerate(scaled_features):\n",
        "        qc.rz(angle, q)\n",
        "\n",
        "    # 5. Second entangling layer: ring (reverse)\n",
        "    for q in range(num_qubits):\n",
        "        qc.cx((q + 1) % num_qubits, q)\n",
        "\n",
        "    # 6. Third data-encoding layer (Rx with half angles)\n",
        "    for q, angle in enumerate(scaled_features):\n",
        "        qc.rx(angle * 0.5, q)\n",
        "\n",
        "    # 7. Nearest-neighbor chain\n",
        "    for q in range(num_qubits - 1):\n",
        "        qc.cx(q, q + 1)\n",
        "\n",
        "    # 8. Measure\n",
        "    qc.measure(range(num_qubits), range(num_qubits))\n",
        "\n",
        "    # Print the circuit\n",
        "    #logging.info(\"Quantum circuit (complex version):\")\n",
        "    #logging.info(\"\\n\" + str(qc.draw(output='text')))\n",
        "\n",
        "    # Execute\n",
        "    backend = AerSimulator()\n",
        "    transpiled_qc = transpile(qc, backend)\n",
        "    job = backend.run(transpiled_qc, shots=8192)\n",
        "    result = job.result()\n",
        "    counts = result.get_counts()\n",
        "\n",
        "    # Print measurement counts\n",
        "    #logging.info(f\"Measurement counts: {counts}\")\n",
        "    return scaled_features, counts\n",
        "\n",
        "\n",
        "def plot_quantum_counts(counts):\n",
        "    \"\"\"\n",
        "    Plots the measurement distribution.\n",
        "    \"\"\"\n",
        "    outcomes = list(counts.keys())\n",
        "    frequencies = list(counts.values())\n",
        "    fig, ax = plt.subplots(figsize=(7, 4))\n",
        "    ax.bar(outcomes, frequencies, color='skyblue')\n",
        "    ax.set_xlabel(\"Measurement Outcome\")\n",
        "    ax.set_ylabel(\"Counts\")\n",
        "    ax.set_title(\"Quantum Measurement Distribution\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def run_and_store_quantum_analysis(analysis_dict):\n",
        "    \"\"\"\n",
        "    1) Compute the feature vector (with some features commented out).\n",
        "    2) Run the complex quantum circuit.\n",
        "    3) Plot the measurement distribution.\n",
        "    4) Store results in analysis_dict.\n",
        "    \"\"\"\n",
        "\n",
        "    # Example of retrieving advanced stats, if you want them in your quantum section:\n",
        "    advanced_stats = analysis_dict.get(\"advanced_vocal_stats\", {})\n",
        "\n",
        "    feature_vector = _compute_quantum_features_from_analysis(analysis_dict)\n",
        "    scaled_features, measurement_counts = quantum_pipeline_complex(feature_vector)\n",
        "    plot_quantum_counts(measurement_counts)\n",
        "    \n",
        "    analysis_dict[\"quantum_analysis\"] = {\n",
        "        \"feature_vector\": feature_vector,\n",
        "        \"scaled_angles\": scaled_features,\n",
        "        \"advanced_stats\": advanced_stats,\n",
        "        \"measurement_counts\": dict(measurement_counts)\n",
        "    }\n",
        "    \n",
        "    # Comment out or remove other debug messages; keep the essential final line if you want:\n",
        "    # logging.info(\"Quantum analysis complete (complex circuit).\")\n",
        "\n",
        "    return analysis_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mod12-classical",
      "metadata": {},
      "source": [
        "## 12. Pipeline - both classical AND quantum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "classical-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def _replace_nan_with_none(value):\n",
        "    if isinstance(value, float) and np.isnan(value):\n",
        "        return None\n",
        "    elif isinstance(value, list):\n",
        "        return [_replace_nan_with_none(v) for v in value]\n",
        "    elif isinstance(value, tuple):\n",
        "        return tuple(_replace_nan_with_none(v) for v in value)\n",
        "    elif isinstance(value, dict):\n",
        "        return {k: _replace_nan_with_none(v) for k, v in value.items()}\n",
        "    else:\n",
        "        return value\n",
        "\n",
        "def grade_single_file(file_name):\n",
        "    \"\"\"\n",
        "    1) Preprocess => audio_data\n",
        "    2) Noise reduction => clean_audio\n",
        "    3) pitch => times, pitches\n",
        "    4) map pitch => note_names/freqs/deviations\n",
        "    5) stats => dev cents\n",
        "    6) build time_matrix_small (512)\n",
        "    7) Module 7 (Revised): analyze_dynamics_module7 => uses LUF_CHUNK_SIZE for RMS & LUFS.\n",
        "    8) analyze => spectral flatness => tone_to_noise, transition_score\n",
        "    9) advanced features => zcr, centroid, rolloff...\n",
        "    10) small-chunk tempo => 512\n",
        "    11) build time_matrix_praat => 2048 => hnr\n",
        "    12) build time_matrix_tempo_medium => 4096, time_matrix_tempo_large => 16384\n",
        "    13) store in DB => analysis_dict, then plot (including RMS & LUFS)\n",
        "    \"\"\"\n",
        "    input_path = os.path.join(INPUT_DIR, file_name)\n",
        "\n",
        "    # 1. Preprocessing\n",
        "    audio_data, sr = preprocess_audio(input_path)\n",
        "\n",
        "    # 2. Noise reduction\n",
        "    clean_audio = simple_noise_reduction(audio_data, sr)\n",
        "\n",
        "    ##############################################\n",
        "    # PATCH 2: Remove Percussion from Clean Audio\n",
        "    ##############################################\n",
        "    harmonic_audio = remove_percussive_components(clean_audio, sr, margin=(1.0, 1.0))\n",
        "    # now to pass this everywhere \n",
        "    \n",
        "\n",
        "    # 3. pitch\n",
        "    times, pitches, confidences = extract_pitch_contour(harmonic_audio, sr)\n",
        "\n",
        "    # 4. map pitch => note\n",
        "    # note_names, note_freqs, deviations = map_pitches_to_notes(pitches)\n",
        "    note_names, note_freqs, deviations, sruti_labels = map_pitches_to_notes_22sruti(pitches)\n",
        "\n",
        "\n",
        "    # 5. stats\n",
        "    valid_devs = [d for d in deviations if d is not None]\n",
        "    if len(valid_devs) > 0:\n",
        "        avg_deviation_cents = float(np.mean(np.abs(valid_devs)))\n",
        "        std_deviation_cents = float(np.std(valid_devs))\n",
        "    else:\n",
        "        avg_deviation_cents = 0.0\n",
        "        std_deviation_cents = 0.0\n",
        "\n",
        "    # 6. build time_matrix_small\n",
        "    time_matrix_small = []\n",
        "    for t, p, nf, d, nn, sruti in zip(times, pitches, note_freqs, deviations, note_names, sruti_labels):\n",
        "        dev_flag_int = 1 if d is not None and abs(d) > DEVIATION_THRESHOLD else 0\n",
        "        time_matrix_small.append({\n",
        "            \"time_s\": float(t),\n",
        "            \"pitch_hz\": p,\n",
        "            \"note_name\": nn,\n",
        "            \"note_freq_hz\": nf,\n",
        "            \"deviation_cents\": d,\n",
        "            \"dev_flag\": dev_flag_int,\n",
        "            \"sruti_class\": sruti,  # store which śruti class it belongs to\n",
        "            \"pitch_accuracy_category\": classify_pitch_deviation(d)\n",
        "        })\n",
        "\n",
        "    # 7. DYNAMICS => RMS & LUFS (with bigger chunk => LUF_CHUNK_SIZE)\n",
        "    time_matrix_small, dyn_summary = analyze_dynamics_module7(\n",
        "        harmonic_audio, sr, time_matrix_small\n",
        "    )\n",
        "\n",
        "    # 8. analyze => tone_to_noise = spectral flatness, transition_score\n",
        "    time_matrix_small = analyze_note_transitions(harmonic_audio, sr, time_matrix_small)\n",
        "\n",
        "    # 9. advanced features => skip RMS here, we do zcr, centroid, rolloff...\n",
        "    time_matrix_small = update_time_matrix_small_with_advanced_features(harmonic_audio, sr, time_matrix_small)\n",
        "\n",
        "    # 10. small-chunk tempo => 512\n",
        "    time_matrix_small = analyze_tempo_adherence(harmonic_audio, sr, time_matrix_small)\n",
        "\n",
        "    # 11. build time_matrix_praat => 2048-chunk\n",
        "    time_matrix_praat = build_praat_time_matrix(harmonic_audio, sr)\n",
        "    for row in time_matrix_praat:\n",
        "        row[\"praat_hnr_category\"] = classify_praat_hnr(row[\"praat_hnr\"])\n",
        "\n",
        "    # 12. build time_matrix_tempo_medium => 4096, time_matrix_tempo_large => 16384\n",
        "    time_matrix_tempo_medium = build_larger_tempo_matrix(\n",
        "        harmonic_audio, sr, chunk_size=TEMPO_CHUNK_SIZE_MEDIUM, overlap=0.5\n",
        "    )\n",
        "    time_matrix_tempo_large = build_larger_tempo_matrix(\n",
        "        harmonic_audio, sr, chunk_size=TEMPO_CHUNK_SIZE_LARGE, overlap=0.5\n",
        "    )\n",
        "\n",
        "    # gather final stats => from time_matrix_small\n",
        "    transition_scores = [row.get('transition_score', 0.0) for row in time_matrix_small]\n",
        "    tone_to_noise_vals = [row.get('tone_to_noise', 0.0) for row in time_matrix_small]\n",
        "\n",
        "    if tone_to_noise_vals:\n",
        "        avg_tnr = float(np.mean(tone_to_noise_vals))\n",
        "        std_tnr = float(np.std(tone_to_noise_vals))\n",
        "    else:\n",
        "        avg_tnr = 0.0\n",
        "        std_tnr = 0.0\n",
        "\n",
        "    if transition_scores:\n",
        "        avg_transition_score = float(np.mean(transition_scores))\n",
        "        std_transition_score = float(np.std(transition_scores))\n",
        "    else:\n",
        "        avg_transition_score = 0.0\n",
        "        std_transition_score = 0.0\n",
        "\n",
        "    # gather praat HNR stats\n",
        "    praat_hnr_vals = [row['praat_hnr'] for row in time_matrix_praat]\n",
        "    if praat_hnr_vals:\n",
        "        avg_praat_hnr = float(np.mean(praat_hnr_vals))\n",
        "        std_praat_hnr = float(np.std(praat_hnr_vals))\n",
        "    else:\n",
        "        avg_praat_hnr = 0.0\n",
        "        std_praat_hnr = 0.0\n",
        "\n",
        "    # Compute advanced note features over the whole file (or using your existing note segmentation)\n",
        "    note_segments = detect_notes(clean_audio, sr)  # Or use your preferred segmentation method\n",
        "    advanced_note_features = update_note_transition_analysis_with_advanced_features(clean_audio, sr, note_segments)\n",
        "\n",
        "    advanced_vocal_stats = compute_advanced_stats(time_matrix_small)\n",
        "\n",
        "\n",
        "    analysis_dict = {\n",
        "        \"file_name\": file_name,\n",
        "        \"sample_rate\": sr,\n",
        "        \"results\": {\n",
        "            \"average_dev_cents\": avg_deviation_cents,\n",
        "            \"std_dev_cents\": std_deviation_cents,\n",
        "            \"deviation_threshold\": DEVIATION_THRESHOLD,\n",
        "            \"avg_transition_score\": avg_transition_score,\n",
        "            \"std_transition_score\": std_transition_score,\n",
        "            \"avg_tnr\": avg_tnr,\n",
        "            \"std_tnr\": std_tnr,\n",
        "            \"avg_praat_hnr\": avg_praat_hnr,\n",
        "            \"std_praat_hnr\": std_praat_hnr\n",
        "        },\n",
        "        \"time_matrix_small\": time_matrix_small,\n",
        "        \"time_matrix_praat\": time_matrix_praat,\n",
        "        \"dynamics_summary\": dyn_summary,\n",
        "        \"time_matrix_tempo_medium\": time_matrix_tempo_medium,\n",
        "        \"time_matrix_tempo_large\": time_matrix_tempo_large,\n",
        "        \"advanced_note_features\": advanced_note_features,\n",
        "        \"advanced_vocal_stats\": advanced_vocal_stats  # New key with aggregated advanced features\n",
        "    }\n",
        "\n",
        "\n",
        "    ########################################\n",
        "    # NEW: Summarize classification counts\n",
        "    ########################################\n",
        "    classification_summary = summarize_classifications(\n",
        "        time_matrix_small=time_matrix_small,\n",
        "        time_matrix_praat=time_matrix_praat,\n",
        "        time_matrix_tempo_medium=time_matrix_tempo_medium,\n",
        "        time_matrix_tempo_large=time_matrix_tempo_large\n",
        "    )\n",
        "    analysis_dict[\"classification_summary\"] = classification_summary\n",
        "\n",
        "    # ====== NEW QUANTUM STEP ======\n",
        "    analysis_dict = run_and_store_quantum_analysis(analysis_dict)\n",
        "    # ====== END NEW QUANTUM STEP ======\n",
        "\n",
        "    # Generate feedback\n",
        "    analysis_dict[\"detailed_feedback_items\"] = generate_detailed_feedback(analysis_dict)\n",
        "\n",
        "    # Aggregate (basic level)\n",
        "    aggregated_items = aggregate_feedback(analysis_dict[\"detailed_feedback_items\"], max_gap=2.0)\n",
        "    # Then generate textual messages\n",
        "    aggregated_messages = generate_aggregate_messages(aggregated_items)\n",
        "    analysis_dict[\"aggregate_feedback\"] = aggregated_messages\n",
        "\n",
        "    # High-level actionable aggregation:\n",
        "    # Only groups with a duration of at least 3 seconds and at least 3 occurrences are retained.\n",
        "    high_level_feedback = aggregate_feedback_actionable(\n",
        "        analysis_dict[\"detailed_feedback_items\"],\n",
        "        max_gap=5.0,\n",
        "        min_duration=3.0,\n",
        "        min_count=3\n",
        "    )\n",
        "    high_level_messages = generate_high_level_messages(high_level_feedback)\n",
        "    analysis_dict[\"high_level_feedback\"] = high_level_messages\n",
        "    \n",
        "\n",
        "    # Raga detection\n",
        "    # Build a list of notes:\n",
        "    detected_notes = [row[\"note_name\"] for row in time_matrix_small if row.get(\"note_name\")]\n",
        "    raga_info = detect_raga_from_notes(detected_notes, raga_db)\n",
        "    analysis_dict[\"raga_detection\"] = raga_info\n",
        "    # raga_info has keys: best_raga, score, explanation, all_scores\n",
        "\n",
        "    # --- Display Indian Note Transitions ---\n",
        "    # --- Display Indian Note Transitions with Proper Names ---\n",
        "    # print(\"\\nIndian Note Transitions:\")\n",
        "\n",
        "    # Define the note cycle: 8 items (7 distinct notes plus the higher Sa)\n",
        "    notes = [\"Sa\", \"Re\", \"Ga\", \"Ma\", \"Pa\", \"Dha\", \"Ni\", \"Sa\"]\n",
        "\n",
        "    prev_indian_note = None\n",
        "    for entry in time_matrix_small:\n",
        "        # Get the raw sruti value (e.g., \"sruti_10\")\n",
        "        current_sruti = entry.get(\"sruti_class\")\n",
        "        # Convert the raw sruti to a proper note using our mapping logic.\n",
        "        if current_sruti is not None and \"_\" in current_sruti:\n",
        "            try:\n",
        "                # Extract the numeric part.\n",
        "                num = int(current_sruti.split('_')[1])\n",
        "                # Map to the index in our notes list.\n",
        "                # Here we assume sruti_10 should map to \"Sa\", so subtract 10.\n",
        "                note_index = (num - 10) % 8\n",
        "                current_indian_note = notes[note_index]\n",
        "            except Exception as e:\n",
        "                # In case of any error, fall back to the raw sruti.\n",
        "                current_indian_note = current_sruti\n",
        "        else:\n",
        "            current_indian_note = current_sruti\n",
        "\n",
        "        time_s = entry.get(\"time_s\")\n",
        "        if current_indian_note != prev_indian_note:\n",
        "        #    if prev_indian_note is None:\n",
        "        #        print(f\"Start at {current_indian_note} at {time_s:.2f} s\")\n",
        "        #    else:\n",
        "        #        print(f\"Transition: {prev_indian_note} -> {current_indian_note} at {time_s:.2f} s\")\n",
        "            prev_indian_note = current_indian_note\n",
        "\n",
        "\n",
        "\n",
        "    sanitized_analysis_dict = _replace_nan_with_none(analysis_dict)\n",
        "\n",
        "\n",
        "    if SAVE_TO_DB:\n",
        "        db = QuantumMusicDB()\n",
        "        db.create_tables()\n",
        "        rec_id = db.insert_analysis(\n",
        "            file_name=file_name,\n",
        "            sample_rate=sr,\n",
        "            analysis_data=sanitized_analysis_dict\n",
        "        )\n",
        "        #print(f\"Inserted analysis record with ID: {rec_id}\")\n",
        "        db.close()\n",
        "\n",
        "    print(\"\\nNow playing the processed audio (bandpass, normalized, trimmed):\")\n",
        "    display(Audio(data=clean_audio, rate=sr))\n",
        "\n",
        "    print(\"\\\\nNow playing the processed audio with percussion removed:\")\n",
        "    display(Audio(data=harmonic_audio, rate=sr))\n",
        "\n",
        "    print(\"\\\\nNow playing the processed audio with tanpura removed:\")\n",
        "    tanpuraremoved_audio = remove_drone(harmonic_audio, sr)\n",
        "    display(Audio(data=tanpuraremoved_audio, rate=sr))\n",
        "\n",
        "\n",
        "    # Print the best raga (which may be 'Kedar' if that has the highest score).\n",
        "    print(f\"Detected Raga: {raga_info['best_raga']}, reason: {raga_info['explanation']}\")\n",
        "\n",
        "    # Print the top 3 detected ragas.\n",
        "    print(\"\\nTop 3 detected ragas:\")\n",
        "    for index, raga in enumerate(raga_info['top_ragas'], start=1):\n",
        "        print(f\"{index}. {raga['raga']} (Score: {raga['score']:.2f}) - {raga['explanation']}\")\n",
        "\n",
        "    plot_all_metrics(\n",
        "        time_matrix_small,\n",
        "        time_matrix_praat,\n",
        "        audio_data=harmonic_audio,  # Using harmonic audio\n",
        "        sr=sr,\n",
        "        time_matrix_tempo_medium=time_matrix_tempo_medium,\n",
        "        time_matrix_tempo_large=time_matrix_tempo_large,\n",
        "        aggregated_items=aggregated_items\n",
        "    )\n",
        "\n",
        "    # Print the basic aggregated feedback.\n",
        "    #print(\"\\nAGGREGATED FEEDBACK\")\n",
        "    #for msg in analysis_dict[\"aggregate_feedback\"]:\n",
        "    #    print(msg)\n",
        "\n",
        "    # Print the high-level (actionable) feedback summary.\n",
        "    #print(\"\\nHigh-Level Feedback Summary:\")\n",
        "    #for msg in analysis_dict[\"high_level_feedback\"]:\n",
        "    #    print(msg)\n",
        "\n",
        "\n",
        "    return sanitized_analysis_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mod13-quantum",
      "metadata": {},
      "source": [
        "## 13. Quantum Pipeline (Placeholder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "quantum-placeholder-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 13. QUANTUM PIPELINE (PLACEHOLDER)\n",
        "def quantum_pipeline_placeholder(feature_matrix):\n",
        "    #print(\"[Quantum Pipeline Placeholder] Feature matrix received.\")\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb0edca1",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import librosa\n",
        "from scipy.signal import iirnotch, filtfilt\n",
        "\n",
        "def remove_drone(audio_data, sr, search_duration=2.0, q_factor=30.0):\n",
        "    \"\"\"\n",
        "    Identify and remove a strong background drone at a specific pitch.\n",
        "    \n",
        "    :param audio_data: 1D numpy array of the audio samples\n",
        "    :param sr: sampling rate\n",
        "    :param search_duration: length (in seconds) of audio to analyze for the drone freq\n",
        "    :param q_factor: notch filter Q-factor (sharpness). Higher => narrower notch.\n",
        "    :return: drone_removed_audio (numpy array)\n",
        "    \"\"\"\n",
        "    # 1) Analyze a short portion of the audio (up to search_duration seconds) \n",
        "    #    to estimate the main drone frequency.\n",
        "    max_samples = min(len(audio_data), int(search_duration * sr))\n",
        "    snippet = audio_data[:max_samples]\n",
        "\n",
        "    # 2) Compute an FFT-based power spectrum.\n",
        "    #    We'll find the frequency bin with the highest magnitude (excluding DC).\n",
        "    fft_spectrum = np.fft.rfft(snippet)\n",
        "    freqs = np.fft.rfftfreq(len(snippet), d=1.0/sr)\n",
        "    \n",
        "    # Exclude DC (freq=0) to avoid picking silence or offset as the drone\n",
        "    fft_spectrum[0] = 0.0  # zero out DC component\n",
        "    power_spectrum = np.abs(fft_spectrum)\n",
        "    \n",
        "    # 3) Identify the peak frequency\n",
        "    peak_index = np.argmax(power_spectrum)\n",
        "    drone_freq = freqs[peak_index]\n",
        "    print(f\"Identified drone frequency ~ {drone_freq:.2f} Hz\")\n",
        "    \n",
        "    # 4) Design a narrow notch filter around that frequency\n",
        "    # w0 = (target freq) / (nyquist freq)\n",
        "    nyquist = 0.5 * sr\n",
        "    w0 = drone_freq / nyquist\n",
        "    b, a = iirnotch(w0, Q=q_factor)  # scipy.signal.iirnotch\n",
        "    \n",
        "    # 5) Apply the notch filter to the entire audio using filtfilt\n",
        "    drone_removed = filtfilt(b, a, audio_data)\n",
        "    \n",
        "    return drone_removed"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "execution-example",
      "metadata": {},
      "source": [
        "# Execution Example\n",
        "Below code cell uses `rohan3.wav` from your `input/` folder.\n",
        "You will see an audio player just before the graphs for quick listening.\n",
        "\n",
        "We now have:\n",
        "- **Module 7** using **LUFS_CHUNK_SIZE** (22050 samples => ~0.5s) for RMS & LUFS\n",
        "- Multi-chunk tempo (512, 4096, 16384)\n",
        "- Plots for RMS(dB) and LUFS in the advanced line plot.\n",
        "- **Short-chunk** fixes for spectral feature calls so no warnings from Librosa.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "execution-example-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "result = grade_single_file(\"rohan3.wav\")\n",
        "# result  # Uncomment to see full analysis dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d4cb568",
      "metadata": {},
      "outputs": [],
      "source": [
        "import psycopg2\n",
        "from psycopg2.extras import Json\n",
        "\n",
        "# ====== Quantum Comparison Helper Functions ======\n",
        "\n",
        "import math\n",
        "\n",
        "def _counts_to_probability(measure_counts):\n",
        "    \"\"\"\n",
        "    Convert measurement_counts dict -> probability dict,\n",
        "    e.g. {'0010': 5, '1110': 10} -> {'0010': 0.3333, '1110': 0.6667}.\n",
        "    \"\"\"\n",
        "    total_shots = sum(measure_counts.values())\n",
        "    if total_shots == 0:\n",
        "        return {}\n",
        "    return {state: c / total_shots for (state, c) in measure_counts.items()}\n",
        "\n",
        "def _kl_div(p, q):\n",
        "    \"\"\"\n",
        "    Kullback–Leibler divergence: sum( p(x) log2 [p(x)/q(x)] ), ignoring zero terms.\n",
        "    \"\"\"\n",
        "    div = 0.0\n",
        "    for key, p_val in p.items():\n",
        "        if p_val > 0:\n",
        "            q_val = q.get(key, 0.0)\n",
        "            if q_val > 0:\n",
        "                div += p_val * math.log(p_val / q_val, 2)\n",
        "    return div\n",
        "\n",
        "def _js_distance(p, q):\n",
        "    \"\"\"\n",
        "    Jensen–Shannon distance = sqrt(JS divergence).\n",
        "    JS divergence = 0.5*KL(p||m) + 0.5*KL(q||m), where m = 0.5*(p+q).\n",
        "    \"\"\"\n",
        "    all_keys = set(p.keys()) | set(q.keys())\n",
        "    m = {}\n",
        "    for key in all_keys:\n",
        "        p_val = p.get(key, 0.0)\n",
        "        q_val = q.get(key, 0.0)\n",
        "        m[key] = 0.5*(p_val + q_val)\n",
        "    \n",
        "    js_div = 0.5*_kl_div(p, m) + 0.5*_kl_div(q, m)\n",
        "    return math.sqrt(js_div)\n",
        "\n",
        "def _euclidean_distance(v1, v2):\n",
        "    \"\"\"\n",
        "    Simple L2 distance between two lists. If lengths differ, compare up to min(len).\n",
        "    \"\"\"\n",
        "    n = min(len(v1), len(v2))\n",
        "    dist_sq = 0.0\n",
        "    for i in range(n):\n",
        "        diff = (v1[i] - v2[i])\n",
        "        dist_sq += diff*diff\n",
        "    return math.sqrt(dist_sq)\n",
        "\n",
        "# ----- Deeper Helper Subroutines -----\n",
        "\n",
        "def _compare_bitstring_probabilities(p1, p2, top_n=5):\n",
        "    \"\"\"\n",
        "    Return a list of the top-N bitstrings with the largest absolute difference in probability.\n",
        "    Each item is (bitstring, pval1, pval2, abs_diff).\n",
        "    \"\"\"\n",
        "    all_keys = set(p1.keys()) | set(p2.keys())\n",
        "    diffs = []\n",
        "    for bs in all_keys:\n",
        "        val1 = p1.get(bs, 0.0)\n",
        "        val2 = p2.get(bs, 0.0)\n",
        "        diff = abs(val1 - val2)\n",
        "        diffs.append((bs, val1, val2, diff))\n",
        "    # Sort by absolute difference descending\n",
        "    diffs.sort(key=lambda x: x[3], reverse=True)\n",
        "    return diffs[:top_n]\n",
        "\n",
        "def _compare_scaled_angles(angles1, angles2, top_n=5):\n",
        "    \"\"\"\n",
        "    Return the top-N angles with the largest absolute difference.\n",
        "    Each item is (angle_index, a1, a2, abs_diff).\n",
        "    \"\"\"\n",
        "    n = min(len(angles1), len(angles2))\n",
        "    diffs = []\n",
        "    for i in range(n):\n",
        "        a1 = angles1[i]\n",
        "        a2 = angles2[i]\n",
        "        diff = abs(a1 - a2)\n",
        "        diffs.append((i, a1, a2, diff))\n",
        "    diffs.sort(key=lambda x: x[3], reverse=True)\n",
        "    return diffs[:top_n]\n",
        "\n",
        "def _compare_advanced_stats(adv_stats_1, adv_stats_2):\n",
        "    \"\"\"\n",
        "    Compare advanced_stats dictionary. Return a list of (stat_name, val1, val2, abs_diff).\n",
        "    For example: \"avg_jitter\", \"std_jitter\", etc. \n",
        "    \"\"\"\n",
        "    all_stats = set(adv_stats_1.keys()) | set(adv_stats_2.keys())\n",
        "    diffs = []\n",
        "    for stat in all_stats:\n",
        "        val1 = adv_stats_1.get(stat, 0.0)\n",
        "        val2 = adv_stats_2.get(stat, 0.0)\n",
        "        diff = abs(val1 - val2)\n",
        "        if diff > 0:\n",
        "            diffs.append((stat, val1, val2, diff))\n",
        "    # Sort descending by absolute difference\n",
        "    diffs.sort(key=lambda x: x[3], reverse=True)\n",
        "    return diffs\n",
        "\n",
        "\n",
        "# --- Database Retrieval ---\n",
        "\n",
        "def get_analysis_from_db(record_id):\n",
        "    \"\"\"\n",
        "    Fetch the analysis record for a given record_id from your PostgreSQL database,\n",
        "    then return the analysis_data (a dictionary stored as JSONB).\n",
        "    \"\"\"\n",
        "    # Instantiate your database connection.\n",
        "    db = QuantumMusicDB()  # Assumes your QuantumMusicDB class is defined and imported.\n",
        "    \n",
        "    row = db.fetch_analysis(record_id)\n",
        "    db.close()\n",
        "    \n",
        "    if row:\n",
        "        # Expected row format: (id, file_name, sample_rate, analysis_data)\n",
        "        analysis_data = row[3]\n",
        "        return analysis_data\n",
        "    else:\n",
        "        raise ValueError(f\"Record with ID {record_id} not found.\")\n",
        "\n",
        "\n",
        "# --- Comparison Module ---\n",
        "\n",
        "def compare_recordings(analysis1, analysis2):\n",
        "    \"\"\"\n",
        "    Compare two analysis dictionaries on several metrics and produce a detailed report.\n",
        "    The comparison includes:\n",
        "      - Raga detection: Are the detected ragas the same or different?\n",
        "      - Aggregated feedback: Which recording has more (or fewer) aggregated feedback items?\n",
        "      - High-level feedback: A comparison of the number of high-level (actionable) issues.\n",
        "      - Pitch accuracy: If available, a comparison of the average pitch accuracy.\n",
        "    \n",
        "    Returns a dictionary with a detailed report and individual comparisons.\n",
        "    \"\"\"\n",
        "    comparison = {}\n",
        "    \n",
        "    # 1. Raga Detection Comparison\n",
        "    raga_info1 = analysis1.get(\"raga_info\", {})\n",
        "    raga_info2 = analysis2.get(\"raga_info\", {})\n",
        "    \n",
        "    if raga_info1 and raga_info2:\n",
        "        best1 = raga_info1.get(\"best_raga\", \"Unknown\")\n",
        "        best2 = raga_info2.get(\"best_raga\", \"Unknown\")\n",
        "        if best1 == best2:\n",
        "            raga_comp = f\"Both recordings were detected as '{best1}'.\"\n",
        "        else:\n",
        "            raga_comp = (f\"Recording 1 was detected as '{best1}', while Recording 2 was detected as '{best2}'.\")\n",
        "    else:\n",
        "        raga_comp = \"Insufficient raga detection data for one or both recordings.\"\n",
        "    comparison[\"raga_comparison\"] = raga_comp\n",
        "    \n",
        "    # 2. Basic Aggregated Feedback Comparison\n",
        "    agg_feedback1 = analysis1.get(\"aggregate_feedback\", [])\n",
        "    agg_feedback2 = analysis2.get(\"aggregate_feedback\", [])\n",
        "    count1 = len(agg_feedback1)\n",
        "    count2 = len(agg_feedback2)\n",
        "    \n",
        "    if count1 < count2:\n",
        "        agg_comp = (f\"Recording 1 appears to have fewer performance issues \"\n",
        "                    f\"({count1} aggregated feedback messages) compared to Recording 2 ({count2}).\")\n",
        "    elif count1 > count2:\n",
        "        agg_comp = (f\"Recording 2 appears to have fewer performance issues \"\n",
        "                    f\"({count2} aggregated feedback messages) compared to Recording 1 ({count1}).\")\n",
        "    else:\n",
        "        agg_comp = f\"Both recordings have a similar number of aggregated feedback items ({count1}).\"\n",
        "    comparison[\"aggregated_feedback_comparison\"] = agg_comp\n",
        "    \n",
        "    # 3. High-Level Feedback Comparison (actionable issues)\n",
        "    high_feedback1 = analysis1.get(\"high_level_feedback\", [])\n",
        "    high_feedback2 = analysis2.get(\"high_level_feedback\", [])\n",
        "    high_count1 = len(high_feedback1)\n",
        "    high_count2 = len(high_feedback2)\n",
        "    \n",
        "    if high_count1 < high_count2:\n",
        "        high_comp = (f\"Recording 1 has fewer sustained performance issues \"\n",
        "                     f\"({high_count1} high-level issues) compared to Recording 2 ({high_count2}).\")\n",
        "    elif high_count1 > high_count2:\n",
        "        high_comp = (f\"Recording 2 has fewer sustained performance issues \"\n",
        "                     f\"({high_count2} high-level issues) compared to Recording 1 ({high_count1}).\")\n",
        "    else:\n",
        "        high_comp = f\"Both recordings have a similar number of high-level issues ({high_count1}).\"\n",
        "    comparison[\"high_level_feedback_comparison\"] = high_comp\n",
        "    \n",
        "    # 4. Pitch Accuracy Comparison (if available)\n",
        "    avg_pitch1 = analysis1.get(\"average_pitch_accuracy\")\n",
        "    avg_pitch2 = analysis2.get(\"average_pitch_accuracy\")\n",
        "    if avg_pitch1 is not None and avg_pitch2 is not None:\n",
        "        if avg_pitch1 > avg_pitch2:\n",
        "            pitch_comp = (f\"Recording 1 has a higher average pitch accuracy ({avg_pitch1:.2f}) \"\n",
        "                          f\"than Recording 2 ({avg_pitch2:.2f}).\")\n",
        "        elif avg_pitch1 < avg_pitch2:\n",
        "            pitch_comp = (f\"Recording 2 has a higher average pitch accuracy ({avg_pitch2:.2f}) \"\n",
        "                          f\"than Recording 1 ({avg_pitch1:.2f}).\")\n",
        "        else:\n",
        "            pitch_comp = f\"Both recordings have similar average pitch accuracy ({avg_pitch1:.2f}).\"\n",
        "    else:\n",
        "        pitch_comp = \"Pitch accuracy data is missing for one or both recordings.\"\n",
        "    comparison[\"pitch_comparison\"] = pitch_comp\n",
        "    \n",
        "    # Combine the individual comparisons into a detailed report\n",
        "    detailed_report = (\n",
        "        \"=== Detailed Comparison Report ===\\n\\n\"\n",
        "        f\"Raga Detection:\\n{comparison['raga_comparison']}\\n\\n\"\n",
        "        f\"Aggregated Feedback:\\n{comparison['aggregated_feedback_comparison']}\\n\\n\"\n",
        "        f\"High-Level Feedback:\\n{comparison['high_level_feedback_comparison']}\\n\\n\"\n",
        "        f\"Pitch Accuracy:\\n{comparison['pitch_comparison']}\\n\"\n",
        "    )\n",
        "    comparison[\"detailed_report\"] = detailed_report\n",
        "    return comparison\n",
        "\n",
        "\n",
        "def compare_quantum_recordings(analysis1, analysis2, top_n=5):\n",
        "    \"\"\"\n",
        "    Compare two recordings' quantum analyses with more detail:\n",
        "      1) Convert measurement_counts -> probability, compute JS distance.\n",
        "      2) Compare scaled angles with Euclidean distance.\n",
        "      3) Identify the top-N bitstrings that differ most in probability.\n",
        "      4) Identify which scaled angles differ most.\n",
        "      5) Compare advanced stats differences, if present.\n",
        "    \n",
        "    Returns a dict with distances, top differences, and a summary.\n",
        "    \"\"\"\n",
        "    qa1 = analysis1.get(\"quantum_analysis\", {})\n",
        "    qa2 = analysis2.get(\"quantum_analysis\", {})\n",
        "\n",
        "    # If either is missing quantum data, bail out\n",
        "    if not qa1 or not qa2:\n",
        "        return {\n",
        "            \"quantum_summary\": \"One or both analyses have no quantum_analysis data.\",\n",
        "            \"quantum_js_distance\": None,\n",
        "            \"angle_distance\": None,\n",
        "            \"bitstring_differences\": [],\n",
        "            \"angle_differences\": [],\n",
        "            \"advanced_stat_differences\": []\n",
        "        }\n",
        "\n",
        "    # 1) Probability distributions\n",
        "    mc1 = qa1.get(\"measurement_counts\", {})\n",
        "    mc2 = qa2.get(\"measurement_counts\", {})\n",
        "    p1 = _counts_to_probability(mc1)\n",
        "    p2 = _counts_to_probability(mc2)\n",
        "\n",
        "    # JS distance\n",
        "    js_dist = _js_distance(p1, p2)\n",
        "\n",
        "    # 2) Compare scaled angles\n",
        "    angles1 = qa1.get(\"scaled_angles\", [])\n",
        "    angles2 = qa2.get(\"scaled_angles\", [])\n",
        "    angle_dist = _euclidean_distance(angles1, angles2)\n",
        "\n",
        "    # 3) Identify top-N bitstring differences\n",
        "    bitstring_diffs = _compare_bitstring_probabilities(p1, p2, top_n=top_n)\n",
        "\n",
        "    # 4) Identify largest angle differences\n",
        "    angle_diffs = _compare_scaled_angles(angles1, angles2, top_n=top_n)\n",
        "\n",
        "    # 5) Compare advanced stats if present\n",
        "    adv_stats_1 = qa1.get(\"advanced_stats\", {})\n",
        "    adv_stats_2 = qa2.get(\"advanced_stats\", {})\n",
        "    adv_stat_diffs = _compare_advanced_stats(adv_stats_1, adv_stats_2)\n",
        "\n",
        "    # Build a textual summary\n",
        "    summary = (\n",
        "        f\"=== Quantum Comparison ===\\n\"\n",
        "        f\"Jensen–Shannon distance: {js_dist:.4f}\\n\"\n",
        "        f\"Euclidean distance of scaled angles: {angle_dist:.4f}\\n\"\n",
        "        \"Lower is more similar.\\n\\n\"\n",
        "        \"Top Bitstring Probability Differences:\\n\"\n",
        "    )\n",
        "    for i, (bs, pval1, pval2, diff) in enumerate(bitstring_diffs, 1):\n",
        "        summary += f\"  {i}) '{bs}': p1={pval1:.4f}, p2={pval2:.4f}, abs diff={diff:.4f}\\n\"\n",
        "\n",
        "    summary += \"\\nLargest Angle Differences:\\n\"\n",
        "    for i, (idx, a1, a2, d) in enumerate(angle_diffs, 1):\n",
        "        summary += f\"  Angle {idx}: rec1={a1:.4f}, rec2={a2:.4f}, diff={d:.4f}\\n\"\n",
        "\n",
        "    if adv_stat_diffs:\n",
        "        summary += \"\\nAdvanced Stats Differences:\\n\"\n",
        "        for stat_name, val1, val2, diff in adv_stat_diffs:\n",
        "            summary += f\"  {stat_name}: rec1={val1:.4f}, rec2={val2:.4f}, diff={diff:.4f}\\n\"\n",
        "    else:\n",
        "        summary += \"\\nNo advanced stats differences found or advanced_stats missing.\\n\"\n",
        "\n",
        "    return {\n",
        "        \"quantum_js_distance\": js_dist,\n",
        "        \"angle_distance\": angle_dist,\n",
        "        \"bitstring_differences\": bitstring_diffs,\n",
        "        \"angle_differences\": angle_diffs,\n",
        "        \"advanced_stat_differences\": adv_stat_diffs,\n",
        "        \"quantum_summary\": summary\n",
        "    }\n",
        "\n",
        "\n",
        "def compare_analysis_results(record_id1, record_id2):\n",
        "    \"\"\"\n",
        "    Given two recording IDs, fetch their analysis results from the database and compare them.\n",
        "    Returns a dictionary containing the detailed comparison report.\n",
        "    \"\"\"\n",
        "    analysis1 = get_analysis_from_db(record_id1)\n",
        "    analysis2 = get_analysis_from_db(record_id2)\n",
        "    \n",
        "    comparison = compare_recordings(analysis1, analysis2)\n",
        "\n",
        "        # Also do a quantum analysis comparison if quantum data is present\n",
        "    quantum_comp = compare_quantum_recordings(analysis1, analysis2)\n",
        "    \n",
        "    # Merge quantum comparison into the final 'comparison' dict\n",
        "    comparison[\"quantum_comparison\"] = quantum_comp\n",
        "    \n",
        "    # Optionally, build a final combined or 'detailed_report' that includes quantum summary\n",
        "    if quantum_comp[\"quantum_summary\"]:\n",
        "        comparison[\"detailed_report\"] += f\"\\n\\nQuantum Analysis:\\n{quantum_comp['quantum_summary']}\\n\"\n",
        "    \n",
        "    return comparison\n",
        "\n",
        "\n",
        "# --- Example Usage ---\n",
        "\n",
        "# Manually set the recording IDs here (replace these with your actual record IDs).\n",
        "# record_id1 = 81  # e.g., 1\n",
        "# record_id2 = 82  # e.g., 2\n",
        "\n",
        "# Fetch the analysis from the database and compare the recordings.\n",
        "# comparison_result = compare_analysis_results(record_id1, record_id2)\n",
        "# print(comparison_result[\"detailed_report\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3484906",
      "metadata": {},
      "source": [
        "## 50 DATA PROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c71b234f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# If not installed, you may need:\n",
        "# !pip install pydub gspread oauth2client yt-dlp\n",
        "\n",
        "import os\n",
        "import time\n",
        "import shutil\n",
        "import subprocess\n",
        "from pydub import AudioSegment\n",
        "\n",
        "# Google Sheets\n",
        "import gspread\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "\n",
        "##################################################\n",
        "# A) GOOGLE SHEET CONFIG & AUTH\n",
        "##################################################\n",
        "\n",
        "SERVICE_ACCOUNT_JSON = os.path.join(\"data\", \"quantummusic-8dbce27ed321.json\")\n",
        "SCOPE = [\n",
        "    \"https://www.googleapis.com/auth/spreadsheets\",\n",
        "    \"https://www.googleapis.com/auth/drive\"\n",
        "]\n",
        "\n",
        "creds = ServiceAccountCredentials.from_json_keyfile_name(SERVICE_ACCOUNT_JSON, scopes=SCOPE)\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "SPREADSHEET_URL = \"https://docs.google.com/spreadsheets/d/1oD1Oyp5inbcog9eUQTl8xnmYi6y-ZMln8Iaxz9xHEtY/edit#gid=780967666\"\n",
        "workbook = gc.open_by_url(SPREADSHEET_URL)\n",
        "\n",
        "metadata_sheet       = workbook.worksheet(\"metadata\")\n",
        "tempogroups_sheet    = workbook.worksheet(\"tempogroups\")\n",
        "outputchunks_sheet   = workbook.worksheet(\"outputchunks\")\n",
        "\n",
        "##################################################\n",
        "# B) HELPER: Find column index by name\n",
        "##################################################\n",
        "def find_col_index(col_name, header_list):\n",
        "    \"\"\"\n",
        "    Locate column index (0-based) for 'col_name' in 'header_list' (case-insensitive).\n",
        "    Returns None if not found.\n",
        "    \"\"\"\n",
        "    col_name_lower = col_name.lower()\n",
        "    for i, h in enumerate(header_list):\n",
        "        if h.lower() == col_name_lower:\n",
        "            return i\n",
        "    return None\n",
        "\n",
        "\n",
        "##################################################\n",
        "# C) FIRST PASS: Download from YouTube if Processed = -1\n",
        "##################################################\n",
        "def first_pass_downloads():\n",
        "    print(\"=== FIRST PASS: Downloading from YouTube for rows with Processed = -1 ===\")\n",
        "\n",
        "    # 1) Read entire \"metadata\" sheet\n",
        "    all_values = metadata_sheet.get_all_values()\n",
        "    if not all_values:\n",
        "        print(\"No data in 'metadata' sheet. Nothing to do.\")\n",
        "        return  # no data\n",
        "\n",
        "    header = all_values[0]\n",
        "    data_rows = all_values[1:]\n",
        "\n",
        "    # Identify relevant columns\n",
        "    tracksource_idx = find_col_index(\"TrackSource\", header)\n",
        "    processed_idx   = find_col_index(\"Processed\", header)\n",
        "    filename_idx    = find_col_index(\"FileName\", header)\n",
        "\n",
        "    if tracksource_idx is None or processed_idx is None or filename_idx is None:\n",
        "        raise ValueError(\"Could not find 'TrackSource', 'FileName', or 'Processed' columns in metadata header.\")\n",
        "\n",
        "    # 2) Collect rows where Processed = -1\n",
        "    rows_to_download = []\n",
        "    for i, row in enumerate(data_rows):\n",
        "        sheet_row = i + 2  # 1-based offset for header\n",
        "        if len(row) <= max(tracksource_idx, processed_idx, filename_idx):\n",
        "            continue\n",
        "        \n",
        "        tracksource_val = row[tracksource_idx].strip()\n",
        "        processed_val   = row[processed_idx].strip()\n",
        "        file_name_in_sheet = row[filename_idx].strip()\n",
        "\n",
        "        if processed_val == \"-1\" and tracksource_val:\n",
        "            rows_to_download.append((sheet_row, tracksource_val, file_name_in_sheet))\n",
        "\n",
        "    print(f\"Found {len(rows_to_download)} YouTube URLs to download (Processed=-1).\")\n",
        "\n",
        "    download_folder = os.path.join(\"data\", \"rawunprocessed\")\n",
        "    os.makedirs(download_folder, exist_ok=True)\n",
        "\n",
        "    # We'll store final statuses in a dict: row -> new_processed_val\n",
        "    status_updates = {}\n",
        "\n",
        "    for (sheet_row_number, youtube_url, file_name_in_sheet) in rows_to_download:\n",
        "        print(f\"Downloading row {sheet_row_number}, URL: {youtube_url}\")\n",
        "        \n",
        "        # Ensure the final filename ends with .mp3\n",
        "        # (If user already put .mp3, we won't double-append it.)\n",
        "        if not file_name_in_sheet.lower().endswith(\".mp3\"):\n",
        "            file_name_in_sheet += \".mp3\"\n",
        "\n",
        "        # Full path for output\n",
        "        output_path = os.path.join(download_folder, file_name_in_sheet)\n",
        "\n",
        "        cmd = [\n",
        "            \"yt-dlp\",\n",
        "            \"-f\", \"bestaudio/best\",\n",
        "            \"--extract-audio\",\n",
        "            \"--audio-format\", \"mp3\",\n",
        "            \"--audio-quality\", \"320K\",\n",
        "            \"-o\", output_path,\n",
        "            youtube_url\n",
        "        ]\n",
        "        try:\n",
        "            result = subprocess.run(cmd, capture_output=True, text=True)\n",
        "            if result.returncode == 0:\n",
        "                print(\"Download succeeded.\")\n",
        "                # Mark processed=0 so it can be chunked in second pass\n",
        "                status_updates[sheet_row_number] = \"11\"\n",
        "            else:\n",
        "                print(f\"Download error (returncode={result.returncode}): {result.stderr}\")\n",
        "                # Mark processed=-2 to indicate error\n",
        "                status_updates[sheet_row_number] = \"-2\"\n",
        "        except Exception as e:\n",
        "            print(f\"Exception in download: {e}\")\n",
        "            status_updates[sheet_row_number] = \"-2\"\n",
        "        \n",
        "        # Wait 10 seconds before next\n",
        "        time.sleep(10)\n",
        "    \n",
        "    # 3) Batch update sheet for these rows\n",
        "    if status_updates:\n",
        "        col_number = processed_idx + 1  # 1-based for Sheets\n",
        "        cell_updates = []\n",
        "        for row_idx, new_val in status_updates.items():\n",
        "            cell_updates.append({\n",
        "                'range': f\"R{row_idx}C{col_number}\",\n",
        "                'values': [[new_val]]\n",
        "            })\n",
        "        metadata_sheet.batch_update(cell_updates)\n",
        "        print(f\"Updated {len(cell_updates)} rows in 'metadata' after YouTube downloads.\")\n",
        "\n",
        "\n",
        "##################################################\n",
        "# D) SECOND PASS: Chunk if Processed = 0\n",
        "##################################################\n",
        "\n",
        "def second_pass_chunking():\n",
        "    print(\"=== SECOND PASS: Chunking files for rows with Processed = 0 ===\")\n",
        "\n",
        "    # 1) Re-read \"metadata\" to see the updated states\n",
        "    metadata_all_values = metadata_sheet.get_all_values()\n",
        "    if not metadata_all_values:\n",
        "        print(\"No data in 'metadata' sheet.\")\n",
        "        return\n",
        "\n",
        "    metadata_header = metadata_all_values[0]\n",
        "    metadata_data_rows = metadata_all_values[1:] if len(metadata_all_values) > 1 else []\n",
        "\n",
        "    file_col_idx = find_col_index(\"FileName\", metadata_header)\n",
        "    processed_col_idx = find_col_index(\"Processed\", metadata_header)\n",
        "    if file_col_idx is None or processed_col_idx is None:\n",
        "        raise ValueError(\"Could not find 'FileName' or 'Processed' column in metadata header.\")\n",
        "\n",
        "    # Build list of (sheet_row_number, file_name_in_sheet) where processed=0\n",
        "    unprocessed_entries = []\n",
        "    for i, row in enumerate(metadata_data_rows):\n",
        "        sheet_row_number = i + 2\n",
        "        if len(row) <= max(file_col_idx, processed_col_idx):\n",
        "            continue\n",
        "        \n",
        "        file_name_in_sheet = row[file_col_idx].strip()\n",
        "        processed_val = row[processed_col_idx].strip()\n",
        "        if processed_val == \"0\":\n",
        "            unprocessed_entries.append((sheet_row_number, file_name_in_sheet))\n",
        "\n",
        "    print(f\"Found {len(unprocessed_entries)} files marked Processed=0 in metadata for chunking.\")\n",
        "\n",
        "    # 2) Build set of base names for these\n",
        "    unprocessed_base_names = set()\n",
        "    for _, file_name_in_sheet in unprocessed_entries:\n",
        "        base_name = os.path.splitext(file_name_in_sheet)[0]\n",
        "        unprocessed_base_names.add(base_name)\n",
        "\n",
        "    # 3) Read tempogroups, store only for these base names\n",
        "    tempogroups_all = tempogroups_sheet.get_all_records()\n",
        "    tempo_dict = {}\n",
        "    for row in tempogroups_all:\n",
        "        fname = row.get(\"FileName\", \"\").strip()\n",
        "        base_ = os.path.splitext(fname)[0]\n",
        "        if base_ not in unprocessed_base_names:\n",
        "            continue\n",
        "        start_dur = float(row.get(\"StartDuration\", 0))\n",
        "        end_dur   = float(row.get(\"EndDuration\", 0))\n",
        "        tempo     = str(row.get(\"Tempo\", \"Unknown\"))\n",
        "        if base_ not in tempo_dict:\n",
        "            tempo_dict[base_] = []\n",
        "        tempo_dict[base_].append({\n",
        "            \"start\": start_dur,\n",
        "            \"end\": end_dur,\n",
        "            \"tempo\": tempo\n",
        "        })\n",
        "\n",
        "    # Folders & settings\n",
        "    data_folder      = \"data\"\n",
        "    input_folder     = os.path.join(data_folder, \"rawunprocessed\")\n",
        "    output_folder    = os.path.join(data_folder, \"trainingdata\")\n",
        "    processed_folder = os.path.join(data_folder, \"rawprocessed\")\n",
        "    error_folder     = os.path.join(data_folder, \"rawerror\")\n",
        "\n",
        "    target_sample_rate = 44100\n",
        "    chunk_duration_sec = 20\n",
        "\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    os.makedirs(processed_folder, exist_ok=True)\n",
        "    os.makedirs(error_folder, exist_ok=True)\n",
        "\n",
        "    # Helper functions\n",
        "    def process_audio_file(file_path, is_mp3):\n",
        "        \"\"\"Load MP3 or WAV, convert to target SR mono.\"\"\"\n",
        "        if is_mp3:\n",
        "            audio = AudioSegment.from_file(file_path, format=\"mp3\")\n",
        "        else:\n",
        "            audio = AudioSegment.from_file(file_path, format=\"wav\")\n",
        "        audio = audio.set_frame_rate(target_sample_rate).set_channels(1)\n",
        "        duration_sec = len(audio) / 1000.0\n",
        "        return audio, duration_sec\n",
        "\n",
        "    def chunk_audio_by_tempo(audio, base_name, file_name_sheet, tempo_info):\n",
        "        rows_for_sheet = []\n",
        "        for tinfo in tempo_info:\n",
        "            tempo_start = float(tinfo[\"start\"])\n",
        "            tempo_end   = float(tinfo[\"end\"])\n",
        "            tempo_label = tinfo[\"tempo\"]\n",
        "            chunk_start_sec = tempo_start\n",
        "\n",
        "            while chunk_start_sec < tempo_end:\n",
        "                chunk_end_sec = chunk_start_sec + chunk_duration_sec\n",
        "                if chunk_end_sec > tempo_end:\n",
        "                    actual_length_sec = tempo_end - chunk_start_sec\n",
        "                    padded_sec = chunk_duration_sec - actual_length_sec\n",
        "                    start_ms = int(chunk_start_sec * 1000)\n",
        "                    end_ms   = int(tempo_end * 1000)\n",
        "                    audio_chunk = audio[start_ms:end_ms]\n",
        "                    if padded_sec > 0:\n",
        "                        silence_chunk = AudioSegment.silent(duration=int(padded_sec * 1000))\n",
        "                        audio_chunk = audio_chunk + silence_chunk\n",
        "\n",
        "                    chunk_filename = f\"{base_name}_{int(chunk_start_sec)}s_{int(chunk_end_sec)}s_{tempo_label}_padded.wav\"\n",
        "                    padded_flag = \"Y\"\n",
        "                else:\n",
        "                    start_ms = int(chunk_start_sec * 1000)\n",
        "                    end_ms   = int(chunk_end_sec * 1000)\n",
        "                    audio_chunk = audio[start_ms:end_ms]\n",
        "                    chunk_filename = f\"{base_name}_{int(chunk_start_sec)}s_{int(chunk_end_sec)}s_{tempo_label}.wav\"\n",
        "                    padded_flag = \"N\"\n",
        "\n",
        "                out_path = os.path.join(output_folder, chunk_filename)\n",
        "                audio_chunk.export(out_path, format=\"wav\")\n",
        "\n",
        "                rows_for_sheet.append({\n",
        "                    \"FileName\": file_name_sheet,\n",
        "                    \"ChunkFileName\": chunk_filename,\n",
        "                    \"Start Duration\": f\"{chunk_start_sec:.2f}\",\n",
        "                    \"End Duration\": f\"{min(chunk_end_sec, tempo_end):.2f}\",\n",
        "                    \"Tempo\": tempo_label,\n",
        "                    \"Padded\": padded_flag\n",
        "                })\n",
        "\n",
        "                chunk_start_sec += chunk_duration_sec\n",
        "\n",
        "        return rows_for_sheet\n",
        "\n",
        "    all_output_rows = []\n",
        "    status_updates_for_chunk = {}\n",
        "\n",
        "    # 4) Loop over entries with Processed=0\n",
        "    for (sheet_row_number, file_name_in_sheet) in unprocessed_entries:\n",
        "        local_path = os.path.join(input_folder, file_name_in_sheet)\n",
        "        lower_name = file_name_in_sheet.lower()\n",
        "\n",
        "        if not os.path.exists(local_path):\n",
        "            print(f\"Error: File '{file_name_in_sheet}' not found in rawunprocessed. Skipping.\")\n",
        "            # Mark processed = -3\n",
        "            status_updates_for_chunk[sheet_row_number] = \"-3\"\n",
        "            continue\n",
        "\n",
        "        if not (lower_name.endswith(\".mp3\") or lower_name.endswith(\".wav\")):\n",
        "            print(f\"Error: File '{file_name_in_sheet}' is not .mp3 or .wav. Skipping.\")\n",
        "            status_updates_for_chunk[sheet_row_number] = \"-3\"\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            is_mp3 = lower_name.endswith(\".mp3\")\n",
        "            audio, total_sec = process_audio_file(local_path, is_mp3)\n",
        "            base_name = os.path.splitext(file_name_in_sheet)[0]\n",
        "            if base_name in tempo_dict and tempo_dict[base_name]:\n",
        "                tempo_info = tempo_dict[base_name]\n",
        "            else:\n",
        "                tempo_info = [{\n",
        "                    \"start\": 0,\n",
        "                    \"end\": total_sec,\n",
        "                    \"tempo\": \"Unknown\"\n",
        "                }]\n",
        "\n",
        "            chunk_rows = chunk_audio_by_tempo(audio, base_name, file_name_in_sheet, tempo_info)\n",
        "            all_output_rows.extend(chunk_rows)\n",
        "\n",
        "            # Move original file to processed\n",
        "            shutil.move(local_path, os.path.join(processed_folder, file_name_in_sheet))\n",
        "            status_updates_for_chunk[sheet_row_number] = \"1\"  # success\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {file_name_in_sheet}: {e}\")\n",
        "            if os.path.exists(local_path):\n",
        "                shutil.move(local_path, os.path.join(error_folder, file_name_in_sheet))\n",
        "            status_updates_for_chunk[sheet_row_number] = \"-3\"\n",
        "\n",
        "    # 5) Single append to 'outputchunks'\n",
        "    if all_output_rows:\n",
        "        rows_to_append = []\n",
        "        for row in all_output_rows:\n",
        "            rows_to_append.append([\n",
        "                row[\"FileName\"],\n",
        "                row[\"ChunkFileName\"],\n",
        "                row[\"Start Duration\"],\n",
        "                row[\"End Duration\"],\n",
        "                row[\"Tempo\"],\n",
        "                row[\"Padded\"]\n",
        "            ])\n",
        "        outputchunks_sheet.append_rows(rows_to_append, value_input_option='RAW')\n",
        "        print(f\"Appended {len(rows_to_append)} rows to 'outputchunks' in second pass.\")\n",
        "\n",
        "    # 6) Batch update \"Processed\" for chunk results\n",
        "    if status_updates_for_chunk:\n",
        "        col_number = processed_col_idx + 1\n",
        "        cell_updates = []\n",
        "        for row_idx, new_val in status_updates_for_chunk.items():\n",
        "            cell_updates.append({\n",
        "                'range': f\"R{row_idx}C{col_number}\",\n",
        "                'values': [[new_val]]\n",
        "            })\n",
        "        metadata_sheet.batch_update(cell_updates)\n",
        "        print(f\"Updated {len(cell_updates)} rows in 'metadata' after chunking pass.\")\n",
        "\n",
        "\n",
        "##################################################\n",
        "# E) SINGLE CELL MAIN LOGIC\n",
        "##################################################\n",
        "\n",
        "def run_all_passes():\n",
        "    \"\"\"\n",
        "    1) First pass: download from YouTube where Processed=-1, set them to 0 or -2\n",
        "    2) Second pass: chunk audio where Processed=0, set them to 1 or -3\n",
        "    \"\"\"\n",
        "    first_pass_downloads()\n",
        "    second_pass_chunking()\n",
        "    print(\"All passes completed.\")\n",
        "\n",
        "# Finally, just call run_all_passes():\n",
        "if __name__ == \"__main__\":\n",
        "    run_all_passes()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4b1ecbb",
      "metadata": {},
      "source": [
        "## Machine Learning Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79bf9d4c",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import librosa\n",
        "import librosa.display\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torchaudio.transforms import MelSpectrogram\n",
        "from transformers import ASTModel, ASTConfig\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import psycopg2\n",
        "from psycopg2.extras import Json\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "\n",
        "# === SETTINGS ===\n",
        "TRAINING_DIR = \"data/trainingdata\"\n",
        "OUTPUT_DIR = \"data/trainingdataoutput\"\n",
        "DB_NAME = \"quantummusic\"\n",
        "DB_HOST = \"localhost\"\n",
        "DB_USER = \"postgres\"\n",
        "DB_PASSWORD = \"postgres\"\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 0.0001\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "# === 1. Process and Store Features from Audio Files ===\n",
        "def process_all_files():\n",
        "    \"\"\"Processes all .wav files in trainingdata using grade_single_file, stores results in DB, and moves files to trainingdataoutput.\"\"\"\n",
        "    db = QuantumMusicDB()\n",
        "    db.create_tables()\n",
        "\n",
        "    if not os.path.exists(OUTPUT_DIR):\n",
        "        os.makedirs(OUTPUT_DIR)\n",
        "\n",
        "    files = [f for f in os.listdir(TRAINING_DIR) if f.endswith(\".wav\")]\n",
        "    \n",
        "    if not files:\n",
        "        print(\"No new files found in trainingdata. Proceeding to fetch data from DB.\")\n",
        "        db.close()\n",
        "        return\n",
        "\n",
        "    for file in files:\n",
        "        file_path = os.path.join(TRAINING_DIR, file)\n",
        "        print(f\"Processing {file}...\")\n",
        "\n",
        "        try:\n",
        "            analysis = grade_single_file(file)\n",
        "\n",
        "            # Insert into DB\n",
        "            rec_id = db.insert_analysis(\n",
        "                file_name=file,\n",
        "                sample_rate=analysis[\"sample_rate\"],\n",
        "                analysis_data=analysis,\n",
        "            )\n",
        "            print(f\"Stored in DB with ID: {rec_id}\")\n",
        "\n",
        "            # Move processed file to trainingdataoutput\n",
        "            shutil.move(file_path, os.path.join(OUTPUT_DIR, file))\n",
        "            print(f\"Moved {file} to {OUTPUT_DIR}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {file}: {e}\")\n",
        "\n",
        "    db.close()\n",
        "    print(\"All files processed and stored in DB.\")\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "# === 2. Fetch Features from Database ===\n",
        "def fetch_training_data():\n",
        "    \"\"\"Retrieves processed data from the database, preprocesses it, and returns formatted datasets.\"\"\"\n",
        "    db = QuantumMusicDB()\n",
        "    conn = db.conn\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    cursor.execute(\"SELECT analysis_data FROM audio_analysis\")\n",
        "    records = cursor.fetchall()\n",
        "    \n",
        "    if not records:\n",
        "        print(\"No data available in the database for training.\")\n",
        "        db.close()\n",
        "        return None, None, None, None\n",
        "\n",
        "    audio_features, scalar_features, quantum_features, labels = [], [], [], []\n",
        "\n",
        "    for row in records:\n",
        "        analysis_data = row[0]  # JSON data from DB\n",
        "\n",
        "        # 1) Load Spectrogram Representation\n",
        "        file_path = os.path.join(OUTPUT_DIR, analysis_data[\"file_name\"])\n",
        "        if not os.path.exists(file_path):\n",
        "            continue  # Skip missing files\n",
        "\n",
        "        y, sr = librosa.load(file_path, sr=None)\n",
        "        mel_spec = MelSpectrogram()(torch.tensor(y).float()).numpy()\n",
        "        audio_features.append(mel_spec)\n",
        "\n",
        "        # 2) Extract Scalar Features\n",
        "        features = analysis_data[\"results\"]\n",
        "        scalar_feat = [\n",
        "            features[\"average_dev_cents\"],\n",
        "            features[\"std_dev_cents\"],\n",
        "            features[\"avg_praat_hnr\"],\n",
        "            features[\"avg_tnr\"],\n",
        "            features[\"avg_vibrato_extent\"],\n",
        "            features[\"avg_vibrato_rate\"],\n",
        "        ]\n",
        "        scalar_features.append(scalar_feat)\n",
        "\n",
        "        # 3) Extract Quantum Features\n",
        "        quantum_analysis = analysis_data.get(\"quantum_analysis\", {})\n",
        "        if quantum_analysis:\n",
        "            quantum_feat = quantum_analysis.get(\"scaled_angles\", [])\n",
        "            quantum_features.append(quantum_feat)\n",
        "\n",
        "        # 4) Use an overall rating or class label (e.g., quality metric)\n",
        "        labels.append(random.randint(0, 1))  # Placeholder for now\n",
        "\n",
        "    db.close()\n",
        "    return (\n",
        "        np.array(audio_features),\n",
        "        np.array(scalar_features),\n",
        "        np.array(quantum_features),\n",
        "        np.array(labels),\n",
        "    )\n",
        "\n",
        "\n",
        "# === 3. Build Dataset for AST Model ===\n",
        "class AudioDataset(Dataset):\n",
        "    \"\"\"Custom dataset for handling audio + scalar + quantum features.\"\"\"\n",
        "\n",
        "    def __init__(self, audio_features, scalar_features, quantum_features, labels):\n",
        "        self.audio_features = torch.tensor(audio_features, dtype=torch.float32)\n",
        "        self.scalar_features = torch.tensor(scalar_features, dtype=torch.float32)\n",
        "        self.quantum_features = torch.tensor(quantum_features, dtype=torch.float32)\n",
        "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (\n",
        "            self.audio_features[idx],\n",
        "            self.scalar_features[idx],\n",
        "            self.quantum_features[idx],\n",
        "            self.labels[idx],\n",
        "        )\n",
        "\n",
        "\n",
        "# === 4. Define AST-Based Multi-Input Model ===\n",
        "class HybridASTModel(nn.Module):\n",
        "    def __init__(self, scalar_dim, quantum_dim, output_dim=2):\n",
        "        super(HybridASTModel, self).__init__()\n",
        "\n",
        "        # AST Model\n",
        "        self.ast = ASTModel.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
        "\n",
        "        # Fully connected layers for scalar and quantum inputs\n",
        "        self.scalar_fc = nn.Linear(scalar_dim, 128)\n",
        "        self.quantum_fc = nn.Linear(quantum_dim, 128)\n",
        "\n",
        "        # Final classification layer\n",
        "        self.final_fc = nn.Linear(768 + 128 + 128, output_dim)\n",
        "\n",
        "    def forward(self, audio_input, scalar_input, quantum_input):\n",
        "        ast_output = self.ast(audio_input).pooler_output\n",
        "        scalar_embed = torch.relu(self.scalar_fc(scalar_input))\n",
        "        quantum_embed = torch.relu(self.quantum_fc(quantum_input))\n",
        "\n",
        "        # Concatenate embeddings\n",
        "        fused = torch.cat((ast_output, scalar_embed, quantum_embed), dim=1)\n",
        "\n",
        "        return self.final_fc(fused)\n",
        "\n",
        "\n",
        "# === 5. Train Model ===\n",
        "def train_model():\n",
        "    \"\"\"Train the AST-based model with scalar and quantum feature fusion.\"\"\"\n",
        "    audio_features, scalar_features, quantum_features, labels = fetch_training_data()\n",
        "\n",
        "    if audio_features is None:\n",
        "        print(\"No training data available. Exiting training.\")\n",
        "        return None\n",
        "\n",
        "    # Normalize Scalar Features\n",
        "    scaler = StandardScaler()\n",
        "    scalar_features = scaler.fit_transform(scalar_features)\n",
        "\n",
        "    # Split into Train/Test\n",
        "    X_train, X_test, S_train, S_test, Q_train, Q_test, Y_train, Y_test = train_test_split(\n",
        "        audio_features, scalar_features, quantum_features, labels, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Create Datasets\n",
        "    train_dataset = AudioDataset(X_train, S_train, Q_train, Y_train)\n",
        "    test_dataset = AudioDataset(X_test, S_test, Q_test, Y_test)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    # Define Model\n",
        "    model = HybridASTModel(scalar_dim=S_train.shape[1], quantum_dim=Q_train.shape[1])\n",
        "    model.to(DEVICE)\n",
        "\n",
        "    # Loss and Optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    # Training Loop\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for audio, scalar, quantum, label in train_loader:\n",
        "            audio, scalar, quantum, label = (\n",
        "                audio.to(DEVICE),\n",
        "                scalar.to(DEVICE),\n",
        "                quantum.to(DEVICE),\n",
        "                label.to(DEVICE),\n",
        "            )\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(audio, scalar, quantum)\n",
        "            loss = criterion(output, label)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "    print(\"Training Complete.\")\n",
        "    return model\n",
        "\n",
        "\n",
        "# === 6. Run Everything ===\n",
        "if __name__ == \"__main__\":\n",
        "    process_all_files()  # Step 1: Process and store features\n",
        "    trained_model = train_model()  # Step 2: Train the AST-based model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9e76d16",
      "metadata": {},
      "source": [
        "## Attempting local llama models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "ed78a0ff",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model='llama3.2-vision:90b' created_at='2025-02-18T02:01:52.990838Z' done=True done_reason='stop' total_duration=121465478959 load_duration=2096872042 prompt_eval_count=18 prompt_eval_duration=60243000000 eval_count=131 eval_duration=58830000000 message=Message(role='assistant', content=\"The image depicts a baby sitting in a blue tub, with the focus on the baby's face and upper body. The baby has dark hair and eyes, and is looking directly at the camera with a neutral expression. The baby's skin tone appears to be light brown or tan.\\n\\nIn the background, there are various objects that suggest the scene is set in a bathroom or nursery. A white wall with a few framed pictures hangs behind the tub, and a small table or shelf holds some toiletries such as shampoo, conditioner, and lotion. The overall atmosphere of the image is calm and serene, suggesting a peaceful moment between the baby and their caregiver.\", images=None, tool_calls=None)\n"
          ]
        }
      ],
      "source": [
        "import ollama\n",
        "\n",
        "response = ollama.chat(\n",
        "    model='llama3.2-vision:90b',\n",
        "    messages=[{\n",
        "        'role': 'user',\n",
        "        'content': 'What is in this image?',\n",
        "        'images': ['IMG_3310.jpg']\n",
        "    }]\n",
        ")\n",
        "\n",
        "print(response)"
      ]
    }
  ],
  "metadata": {
    "authors": [
      {
        "name": "Rohan Agarwal"
      }
    ],
    "kernelspec": {
      "display_name": "quantumvenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "name": "QuantumMusic_Unified_LargerLUFS",
    "summary": "A multi-module system for analyzing and grading singing performances with LUFS_CHUNK_SIZE for RMS & LUFS, multi-chunk tempo, and short-chunk n_fft fixes."
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
