{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "intro",
      "metadata": {},
      "source": [
        "# QuantumMusic - Unified Version\n",
        "\n",
        "### Introduction\n",
        "\n",
        "This is the Python notebook for the quantum music analysis project\n",
        "Started this as a single, all encompassing notebook, but then added more pieces for different functionality!\n",
        "Author: Rohan Agarwal\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "constants",
      "metadata": {},
      "source": [
        "## 1. Constants & Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "code-constants",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. CONSTANTS & IMPORTS\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import psycopg2\n",
        "from psycopg2.extras import Json\n",
        "import numpy as np\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import re\n",
        "from matplotlib import colormaps\n",
        "import scipy.signal  # (if not already imported)\n",
        "from scipy.signal import find_peaks\n",
        "from scipy.signal import iirnotch, filtfilt\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import logging\n",
        "from qiskit import QuantumCircuit, transpile\n",
        "from qiskit_aer import AerSimulator  # Updated import for Qiskit Aer\n",
        "from qiskit.circuit import ParameterVector\n",
        "from qiskit.visualization import plot_histogram\n",
        "import optuna\n",
        "import concurrent.futures\n",
        "from scipy.signal import butter, filtfilt\n",
        "from qiskit import QuantumCircuit, transpile\n",
        "from qiskit_aer import AerSimulator  # modern Aer simulator from qiskit-aer\n",
        "\n",
        "\n",
        "# Directory constants\n",
        "INPUT_DIR = \"data/trainingdata\"\n",
        "OUTPUT_DIR = \"output\"\n",
        "\n",
        "# Database constants\n",
        "DB_NAME = \"quantummusic\"\n",
        "DB_HOST = \"localhost\"\n",
        "DB_USER = \"postgres\"  # placeholder\n",
        "DB_PASSWORD = \"postgres\"  # placeholder\n",
        "\n",
        "# Audio processing constants\n",
        "STANDARD_SR = 44100  # Standard sampling rate\n",
        "SILENCE_THRESHOLD_DB = 30  # dB threshold for silence trimming\n",
        "\n",
        "# Band-pass filter constants\n",
        "LOW_FREQ = 80.0\n",
        "HIGH_FREQ = 3000.0\n",
        "\n",
        "# Visualization constants\n",
        "FIG_SIZE = (10, 4)\n",
        "\n",
        "# Save-to-DB constant\n",
        "SAVE_TO_DB = True\n",
        "\n",
        "# Frame-based approach for pitch detection\n",
        "FRAME_SIZE = 2048\n",
        "HOP_LENGTH = 512  # normal frames\n",
        "# Praat-based chunk size\n",
        "PRAAT_CHUNK_SIZE = 2048  # for Praat\n",
        "\n",
        "# Praat-based HNR\n",
        "import parselmouth\n",
        "from parselmouth.praat import call\n",
        "\n",
        "# audio playback in Jupyter\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "# For LUFS-based loudness\n",
        "try:\n",
        "    import pyloudnorm as pyln\n",
        "    LOUDNORM_AVAILABLE = True\n",
        "except ImportError:\n",
        "    LOUDNORM_AVAILABLE = False\n",
        "    print(\"Warning: pyloudnorm not installed, LUFS computations will be skipped.\")\n",
        "\n",
        "\n",
        "# Deviation threshold in cents, for dev_flag\n",
        "DEVIATION_THRESHOLD = 50.0\n",
        "\n",
        "# Multi-chunk tempo analysis\n",
        "TEMPO_CHUNK_SIZE_MEDIUM = 4096\n",
        "TEMPO_CHUNK_SIZE_LARGE = 16384\n",
        "\n",
        "#  constants for advanced vocal feature extraction\n",
        "VOCAL_FEATURE_CHUNK_SIZE = 16384       #  chunk size for advanced feature extraction (samples)\n",
        "VOCAL_FEATURE_CHUNK_HOP = 4096        # Hop size for overlapping chunks (e.g., 50% overlap)\n",
        "\n",
        "#  constant for LUFS calculations (0.5s @ 44.1kHz)\n",
        "LUFS_CHUNK_SIZE = 22050\n",
        "\n",
        "# ---  CONSTANTS FOR ADVANCED VOCAL FEATURE EXTRACTION ---\n",
        "\n",
        "# Formant analysis parameters\n",
        "FORMANT_ANALYSIS_TIME = 0.1            # Time (in seconds) at which to extract formants\n",
        "FORMANT_TIME_STEP = 0.01               # Time step for formant analysis\n",
        "MAX_NUMBER_OF_FORMANTS = 5             # Maximum number of formants computed by Praat\n",
        "MAXIMUM_FORMANT_FREQUENCY = 5500       # Maximum frequency considered for formant analysis\n",
        "NUM_FORMANTS_TO_EXTRACT = 3            # Number of formants to extract (e.g., F1, F2, F3)\n",
        "\n",
        "# Pitch / jitter / shimmer parameters\n",
        "MIN_F0 = 75                          # Minimum expected fundamental frequency (Hz)\n",
        "MAX_F0 = 500                         # Maximum expected fundamental frequency (Hz)\n",
        "JITTER_TIME_STEP = 0.0001             # Time step for jitter computation\n",
        "JITTER_MIN_PERIOD = 0.02              # Minimum period threshold for jitter\n",
        "JITTER_MAX_PERIOD = 1.3               # Maximum period threshold factor for jitter\n",
        "SHIMMER_MIN_AMPLITUDE = 0.0001         # Minimum amplitude threshold for shimmer\n",
        "SHIMMER_MAX_AMPLITUDE = 0.02           # Maximum amplitude threshold for shimmer\n",
        "SHIMMER_FACTOR = 1.6                 # Shimmer factor (per Praat defaults)\n",
        "\n",
        "# Vibrato analysis parameters\n",
        "VIBRATO_MIN_HZ = 3                   # LoIr bound for vibrato rate (Hz)\n",
        "VIBRATO_MAX_HZ = 10                  # Upper bound for vibrato rate (Hz)\n",
        "MEDIAN_FILTER_KERNEL_SIZE = 9        # Kernel size for median filtering pitch contours\n",
        "\n",
        "\n",
        "# Ensure output directories exist\n",
        "os.makedirs(INPUT_DIR, exist_ok=True)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "def test_imports():\n",
        "    print(\"Imports tested, all modules are available.\")\n",
        "\n",
        "test_imports()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25acf27a",
      "metadata": {},
      "source": [
        "## 2 - Converting Numbers to Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4c12516",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# : Classification Levels\n",
        "# : My own numbers here; come back to make changes as needed later\n",
        "\n",
        "\n",
        "def classify_pitch_deviation(dev_cents):\n",
        "    \"\"\"\n",
        "    Classify pitch deviation into categories based on absolute distance from nearest note.\n",
        "    \"\"\"\n",
        "    if dev_cents is None or np.isnan(dev_cents):\n",
        "        return \"unknown\"\n",
        "    dev_abs = abs(dev_cents)\n",
        "    if dev_abs < 10:\n",
        "        return \"good\"\n",
        "    elif dev_abs < 30:\n",
        "        return \"fair\"\n",
        "    elif dev_abs < 50:\n",
        "        return \"poor\"\n",
        "    elif dev_abs < 100:\n",
        "        return \"very poor\"\n",
        "    else:\n",
        "        return \"completely missing the note\"\n",
        "\n",
        "\n",
        "def classify_tone_to_noise(sf_val):\n",
        "    \"\"\"\n",
        "    Classify spectral flatness into a rough 'tonal vs. noisy' scale.\n",
        "    Typical spectral flatness range is ~0 to 1.\n",
        "    \"\"\"\n",
        "    if np.isnan(sf_val):\n",
        "        return \"unknown\"\n",
        "    if sf_val < 0.02:\n",
        "        return \"very tonal\"\n",
        "    elif sf_val < 0.08:\n",
        "        return \"mostly tonal\"\n",
        "    elif sf_val < 0.15:\n",
        "        return \"mixed\"\n",
        "    elif sf_val < 0.3:\n",
        "        return \"noisy\"\n",
        "    else:\n",
        "        return \"very noisy\"\n",
        "\n",
        "\n",
        "def classify_transition_score(score):\n",
        "    \"\"\"\n",
        "    Classify how smoothly the pitch transitions from the previous chunk.\n",
        "    Score ~ 0..1, with 1 = no pitch jump, 0 = extremely large jump\n",
        "    \"\"\"\n",
        "    if np.isnan(score):\n",
        "        return \"unknown\"\n",
        "    if score >= 0.8:\n",
        "        return \"smooth\"\n",
        "    elif score >= 0.5:\n",
        "        return \"moderate\"\n",
        "    else:\n",
        "        return \"abrupt\"\n",
        "\n",
        "\n",
        "def classify_rms_db(rms_db):\n",
        "    \"\"\"\n",
        "    Classify RMS in dB into categories from very soft => very loud\n",
        "    \"\"\"\n",
        "    if np.isnan(rms_db):\n",
        "        return \"unknown\"\n",
        "    if rms_db < -40:\n",
        "        return \"very soft\"\n",
        "    elif rms_db < -20:\n",
        "        return \"soft\"\n",
        "    elif rms_db < -10:\n",
        "        return \"moderate\"\n",
        "    elif rms_db < -2:\n",
        "        return \"loud\"\n",
        "    else:\n",
        "        return \"very loud\"\n",
        "\n",
        "\n",
        "def classify_lufs(lufs_val):\n",
        "    \"\"\"\n",
        "    Classify LUFS values into rough loudness categories.\n",
        "    Typically, loIr (more negative) = quieter, higher = louder.\n",
        "    \"\"\"\n",
        "    if lufs_val is None or np.isnan(lufs_val):\n",
        "        return \"unknown\"\n",
        "    if lufs_val < -40:\n",
        "        return \"very soft\"\n",
        "    elif lufs_val < -23:\n",
        "        return \"soft\"\n",
        "    elif lufs_val < -14:\n",
        "        return \"moderate\"\n",
        "    elif lufs_val < -5:\n",
        "        return \"loud\"\n",
        "    else:\n",
        "        return \"very loud\"\n",
        "\n",
        "\n",
        "def classify_praat_hnr(hnr_val):\n",
        "    \"\"\"\n",
        "    Classify Praat HNR (dB). Typical range: 0..35 dB for normal voices.\n",
        "    Higher = more harmonic, loIr = more noise.\n",
        "    \"\"\"\n",
        "    if np.isnan(hnr_val):\n",
        "        return \"unknown\"\n",
        "    if hnr_val < 5:\n",
        "        return \"very noisy\"\n",
        "    elif hnr_val < 15:\n",
        "        return \"noisy\"\n",
        "    elif hnr_val < 25:\n",
        "        return \"moderately harmonic\"\n",
        "    else:\n",
        "        return \"very harmonic\"\n",
        "\n",
        "\n",
        "def classify_tempo_bpm(bpm_val):\n",
        "    \"\"\"\n",
        "    Classify BPM values into slow, moderate, fast, etc.\n",
        "    \"\"\"\n",
        "    if np.isnan(bpm_val):\n",
        "        return \"unknown\"\n",
        "    if bpm_val < 40:\n",
        "        return \"very slow\"\n",
        "    elif bpm_val < 70:\n",
        "        return \"slow\"\n",
        "    elif bpm_val < 110:\n",
        "        return \"moderate\"\n",
        "    elif bpm_val < 160:\n",
        "        return \"fast\"\n",
        "    else:\n",
        "        return \"very fast\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db-scaffolding",
      "metadata": {},
      "source": [
        "## 3. Database Scaffolding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "code-database",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. DATABASE SCAFFOLDING\n",
        "\n",
        "class QuantumMusicDB:\n",
        "    \"\"\"\n",
        "    Connection to the PostgreSQL database and basic CRUD operations.\n",
        "    \"\"\"\n",
        "    def __init__(self, db_name=DB_NAME, host=DB_HOST, user=DB_USER, password=DB_PASSWORD):\n",
        "        self.db_name = \"quantummusic\"\n",
        "        self.host = \"localhost\"\n",
        "        self.user = \"postgres\"\n",
        "        self.password = \"postgres\"\n",
        "        self.conn = None\n",
        "        self.connect()\n",
        "\n",
        "    def connect(self):\n",
        "        try:\n",
        "            self.conn = psycopg2.connect(\n",
        "                dbname=self.db_name,\n",
        "                host=self.host,\n",
        "                user=self.user,\n",
        "                password=self.password\n",
        "            )\n",
        "            print(f\"Connected to database {self.db_name} successfully.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error connecting to database: {e}\")\n",
        "\n",
        "    def create_tables(self):\n",
        "        create_table_query = \"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS audio_analysis (\n",
        "            id SERIAL PRIMARY KEY,\n",
        "            file_name VARCHAR(255),\n",
        "            upload_date TIMESTAMP DEFAULT NOW(),\n",
        "            sample_rate INT,\n",
        "            analysis_data JSONB\n",
        "        );\n",
        "        \"\"\"\n",
        "        with self.conn.cursor() as cur:\n",
        "            cur.execute(create_table_query)\n",
        "            self.conn.commit()\n",
        "        print(\"Tables ensured.\")\n",
        "\n",
        "    def insert_analysis(self, file_name, sample_rate, analysis_data):\n",
        "        \"\"\"\n",
        "        Insert a  analysis record into the DB.\n",
        "        analysis_data is stored as a JSONB column using psycopg2.extras.Json.\n",
        "        \"\"\"\n",
        "        insert_query = \"\"\"\n",
        "        INSERT INTO audio_analysis(file_name, sample_rate, analysis_data)\n",
        "        VALUES (%s, %s, %s)\n",
        "        RETURNING id;\n",
        "        \"\"\"\n",
        "        with self.conn.cursor() as cur:\n",
        "            cur.execute(insert_query, (file_name, sample_rate, Json(analysis_data)))\n",
        "            _id = cur.fetchone()[0]\n",
        "            self.conn.commit()\n",
        "        return _id\n",
        "\n",
        "    def fetch_analysis(self, record_id):\n",
        "        \"\"\"\n",
        "        Fetch a specific analysis record by ID.\n",
        "        \"\"\"\n",
        "        select_query = \"\"\"\n",
        "        SELECT id, file_name, sample_rate, analysis_data\n",
        "        FROM audio_analysis\n",
        "        WHERE id=%s;\n",
        "        \"\"\"\n",
        "        with self.conn.cursor() as cur:\n",
        "            cur.execute(select_query, (record_id,))\n",
        "            row = cur.fetchone()\n",
        "        return row\n",
        "\n",
        "    def close(self):\n",
        "        if self.conn:\n",
        "            self.conn.close()\n",
        "            print(\"Database connection closed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "preprocessing-mod",
      "metadata": {},
      "source": [
        "## 4. Module 1: Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "preprocessing-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. MODULE 1: PREPROCESSING\n",
        "\n",
        "def preprocess_audio(file_path,\n",
        "                     target_sr=STANDARD_SR,\n",
        "                     silence_threshold_db=SILENCE_THRESHOLD_DB):\n",
        "    audio_data, sr = librosa.load(file_path, sr=None)\n",
        "    if sr != target_sr:\n",
        "        audio_data = librosa.resample(audio_data, orig_sr=sr, target_sr=target_sr)\n",
        "        sr = target_sr\n",
        "\n",
        "    peak = np.max(np.abs(audio_data))\n",
        "    if peak > 0:\n",
        "        audio_data = audio_data / peak\n",
        "\n",
        "    audio_data, _ = librosa.effects.trim(audio_data, top_db=silence_threshold_db)\n",
        "    return audio_data, sr\n",
        "\n",
        "\n",
        "\n",
        "def remove_percussive_components(audio_data, sr, margin=(1.0, 3.0)):\n",
        "    \"\"\"\n",
        "    Harmonic-percussive source separation (HPSS) to split audio into harmonic and percussive components.\n",
        "    \n",
        "    :param audio_data: np.array containing the audio samples\n",
        "    :param sr: sampling rate\n",
        "    :param margin: A tuple specifying HPSS margin for harmonic/percussive. \n",
        "                   Larger margins can better isolate percussive vs. harmonic content, \n",
        "                   but it may remove more from the harmonic portion.\n",
        "    :return: np.array containing only the harmonic component\n",
        "    \"\"\"\n",
        "    # Perform HPSS with Librosa\n",
        "    # margin=(harmonic_margin, percussive_margin)\n",
        "    # Adjust margins if you have a strongly percussive track\n",
        "    harmonic_part, percussive_part = librosa.decompose.hpss(\n",
        "        librosa.stft(audio_data), \n",
        "        margin=margin\n",
        "    )\n",
        "    # Convert back from STFT to time-domain\n",
        "    harmonic_audio = librosa.istft(harmonic_part)\n",
        "    \n",
        "    return harmonic_audio\n",
        "\n",
        "\n",
        "def remove_drone(audio_data, sr, search_duration=2.0, q_factor=30.0):\n",
        "    \"\"\"\n",
        "    Identify and remove a background drone at a specific pitch.\n",
        "    Not really working so Ill!!\n",
        "    \n",
        "    :param audio_data: 1D numpy array of the audio samples\n",
        "    :param sr: sampling rate\n",
        "    :param search_duration: length (in seconds) of audio to analyze for the drone freq\n",
        "    :param q_factor: notch filter Q-factor (sharpness). Higher => narroIr notch.\n",
        "    :return: drone_removed_audio (numpy array)\n",
        "    \"\"\"\n",
        "    # 1) Analyze a short portion of the audio (up to search_duration seconds) \n",
        "    #    to estimate the main drone frequency.\n",
        "    max_samples = min(len(audio_data), int(search_duration * sr))\n",
        "    snippet = audio_data[:max_samples]\n",
        "\n",
        "    # 2) Compute an FFT-based poIr spectrum.\n",
        "    #    I'll find the frequency bin with the highest magnitude (excluding DC).\n",
        "    fft_spectrum = np.fft.rfft(snippet)\n",
        "    freqs = np.fft.rfftfreq(len(snippet), d=1.0/sr)\n",
        "    \n",
        "    # Exclude DC (freq=0) to avoid picking silence or offset as the drone\n",
        "    fft_spectrum[0] = 0.0  # zero out DC component\n",
        "    poIr_spectrum = np.abs(fft_spectrum)\n",
        "    \n",
        "    # 3) Identify the peak frequency\n",
        "    peak_index = np.argmax(poIr_spectrum)\n",
        "    drone_freq = freqs[peak_index]\n",
        "    print(f\"Identified drone frequency ~ {drone_freq:.2f} Hz\")\n",
        "    \n",
        "    # 4) Design a narrow notch filter around that frequency\n",
        "    # w0 = (target freq) / (nyquist freq)\n",
        "    nyquist = 0.5 * sr\n",
        "    w0 = drone_freq / nyquist\n",
        "    b, a = iirnotch(w0, Q=q_factor)  # scipy.signal.iirnotch\n",
        "    \n",
        "    # 5) Apply the notch filter to the entire audio using filtfilt\n",
        "    drone_removed = filtfilt(b, a, audio_data)\n",
        "    \n",
        "    return drone_removed\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "noise-mod",
      "metadata": {},
      "source": [
        "## 5. Module 2: Noise Identification & Reduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "noise-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. MODULE 2: NOISE IDENTIFICATION & REDUCTION\n",
        "# I should make this more advanced later on\n",
        "\n",
        "def bandpass_filter(audio_data, sr, low_freq=LOW_FREQ, high_freq=HIGH_FREQ):\n",
        "    nyquist = 0.5 * sr\n",
        "    low = low_freq / nyquist\n",
        "    high = high_freq / nyquist\n",
        "    b, a = butter(N=4, Wn=[low, high], btype='band')\n",
        "    filtered_audio = filtfilt(b, a, audio_data)\n",
        "    return filtered_audio\n",
        "\n",
        "def simple_noise_reduction(audio_data, sr):\n",
        "    filtered = bandpass_filter(audio_data, sr)\n",
        "    return filtered"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pitch-mod",
      "metadata": {},
      "source": [
        "## 6. Module 3: Pitch Contour Extraction and Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pitch-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "NOTE_NAMES = [\"C\", \"C#\", \"D\", \"D#\", \"E\", \"F\", \"F#\", \"G\", \"G#\", \"A\", \"A#\", \"B\"]\n",
        "\n",
        "def extract_pitch_contour(audio_data, sr):\n",
        "    \"\"\"\n",
        "    Uses librosa.pyin to extract pitch contour.\n",
        "    Returns (times, pitches, confidences).\n",
        "    \"\"\"\n",
        "    fmin = librosa.note_to_hz('C2')\n",
        "    fmax = librosa.note_to_hz('C7')\n",
        "    # Check if audio_data is stereo, if so, take the mean?\n",
        "    # TODO: Check if I want to take audio_data or harmonic_data here?\n",
        "\n",
        "    # If stereo, down-mix to mono for pitch detection\n",
        "    if audio_data.ndim > 1:\n",
        "        audio_data = np.mean(audio_data, axis=1)\n",
        "\n",
        "\n",
        "    pitches, voiced_flags, confidences = librosa.pyin(\n",
        "        y=audio_data,\n",
        "        fmin=fmin,\n",
        "        fmax=fmax,\n",
        "        sr=sr,\n",
        "        frame_length=FRAME_SIZE,\n",
        "        hop_length=HOP_LENGTH\n",
        "    )\n",
        "    times = librosa.times_like(pitches, sr=sr, hop_length=HOP_LENGTH)\n",
        "    return times, pitches, voiced_flags, confidences\n",
        "\n",
        "def freq_to_closest_note(freq):\n",
        "    \"\"\"\n",
        "    Convert freq in Hz to the nearest semitone and compute the difference in cents.\n",
        "    Returns (note_name, note_freq, deviation_cents) or (None,None,None) if invalid.\n",
        "    My first attempt - I should have just used pyin, but it was fun to look at frequency calcs\n",
        "    \"\"\"\n",
        "    if freq is None or freq <= 0 or np.isnan(freq):\n",
        "        return (None, None, None)\n",
        "    note_num = 69 + 12 * np.log2(freq / 440.0)\n",
        "    if np.isnan(note_num) or np.isinf(note_num):\n",
        "        return (None, None, None)\n",
        "    rounded_note = int(round(note_num))\n",
        "    closest_note_freq = 440.0 * (2.0 ** ((rounded_note - 69)/12.0))\n",
        "    deviation_cents = 1200 * np.log2(freq / closest_note_freq)\n",
        "    note_name = NOTE_NAMES[rounded_note % 12]\n",
        "    octave = (rounded_note // 12) - 1\n",
        "    full_note_name = f\"{note_name}{octave}\"\n",
        "    return (full_note_name, closest_note_freq, deviation_cents)\n",
        "\n",
        "def map_pitches_to_notes(pitches):\n",
        "    \"\"\"\n",
        "    For an array of pitch frames, map each to (note_name, note_freq, deviation_cents).\n",
        "    Return arrays: note_names, note_freqs, deviations.\n",
        "    \"\"\"\n",
        "    note_names = []\n",
        "    note_freqs = []\n",
        "    deviations = []\n",
        "    for p in pitches:\n",
        "        name, freq, dev = freq_to_closest_note(p)\n",
        "        note_names.append(name)\n",
        "        note_freqs.append(freq)\n",
        "        deviations.append(dev)\n",
        "    return note_names, note_freqs, deviations\n",
        "\n",
        "def estimate_base_pitch_from_contour(pitches, voiced_flags):\n",
        "    \"\"\"\n",
        "    Estimate the base pitch (e.g., tanpura / drone) by building a histogram\n",
        "    of all voiced pitches and selecting the peak bin.\n",
        "    \"\"\"\n",
        "    valid_pitches = pitches[voiced_flags & ~np.isnan(pitches)]\n",
        "    if len(valid_pitches) == 0:\n",
        "        return None\n",
        "\n",
        "    hist, bin_edges = np.histogram(\n",
        "        valid_pitches,\n",
        "        bins=200,\n",
        "        range=(valid_pitches.min(), valid_pitches.max())\n",
        "    )\n",
        "    max_bin_idx = np.argmax(hist)\n",
        "    base_pitch = 0.5 * (bin_edges[max_bin_idx] + bin_edges[max_bin_idx + 1])\n",
        "    return base_pitch\n",
        "\n",
        "\n",
        "def map_pitches_with_librosa_functions(pitches, base_pitch=None):\n",
        "    \"\"\"\n",
        "    Map each pitch in Hz to:\n",
        "      1) Istern note name (e.g. 'C4', 'F#3', etc.)\n",
        "      2) Standard Hindustani note (e.g. 'Sa_4')\n",
        "\n",
        "    I loop over pitches individually to avoid passing any NaN to librosa.hz_to_note,\n",
        "    which would cause ValueError in some versions of librosa.\n",
        "    \"\"\"\n",
        "    if base_pitch is None or np.isnan(base_pitch):\n",
        "        base_pitch = 220.0  # fallback reference if the drone was not detected\n",
        "\n",
        "    Istern_notes = []\n",
        "    hindustani_notes = []\n",
        "\n",
        "    # Process each pitch safely\n",
        "    for freq in pitches:\n",
        "        # If freq is invalid, store None\n",
        "        if freq is None or np.isnan(freq) or freq <= 0:\n",
        "            Istern_notes.append(None)\n",
        "            hindustani_notes.append(None)\n",
        "            continue\n",
        "\n",
        "        # I can pass a single-element list to hz_to_note() or hz_to_svara_h()\n",
        "        w_note = librosa.hz_to_note([freq])[0]\n",
        "        h_note = librosa.hz_to_svara_h([freq], Sa=base_pitch, octave=True)[0]\n",
        "\n",
        "        Istern_notes.append(w_note)\n",
        "        hindustani_notes.append(h_note)\n",
        "\n",
        "    return Istern_notes, hindustani_notes\n",
        "\n",
        "\n",
        "\n",
        "# 22 Śrutis Microtonal Classification\n",
        "# an interesting attempt; I'll have to ponder this deeper to see how I can really use this\n",
        "\n",
        "def classify_sruti_offset(offset_cents):\n",
        "    \"\"\"\n",
        "    Attempt to classify the pitch offset into one of 22 śrutis (approx).\n",
        "    Treat an octave as 1200 cents, split into 22 equal segments (~54.5454 cents each).\n",
        "    Also an interesting attempt - I take this for granted when singing!\n",
        "\n",
        "    \n",
        "    offset_cents can be negative or positive. I'll do a rough approach:\n",
        "      - shift offset range by +600 so that 0..1200 covers a full sruti scale\n",
        "      - clamp <0 to 0, >1200 to 1200\n",
        "      - compute sruti index = int(shifted / sruti_size)\n",
        "    \"\"\"\n",
        "    if offset_cents is None or np.isnan(offset_cents):\n",
        "        return \"sruti_unknown\"\n",
        "\n",
        "    sruti_size = 1200.0 / 22.0  # ~54.5454 cents per śruti\n",
        "    shifted = offset_cents + 600.0  # shift so -600 -> 0, +600 -> 1200\n",
        "\n",
        "    if shifted < 0:\n",
        "        shifted = 0\n",
        "    elif shifted > 1200:\n",
        "        shifted = 1200\n",
        "\n",
        "    sruti_index = int(shifted // sruti_size)\n",
        "    if sruti_index > 21:\n",
        "        sruti_index = 21\n",
        "\n",
        "    return f\"sruti_{sruti_index}\"\n",
        "\n",
        "\n",
        "def detect_microtone_offset_22sruti(freq, base_freq):\n",
        "    \"\"\"\n",
        "    Return (offset_cents, sruti_label).\n",
        "      offset_cents: how many cents above or below base_freq\n",
        "      sruti_label: the 22-sruti classification, e.g. 'sruti_0'..'sruti_21'\n",
        "    \"\"\"\n",
        "    if freq <= 0 or base_freq <= 0:\n",
        "        return (None, \"sruti_unknown\")\n",
        "\n",
        "    offset_cents = 1200.0 * np.log2(freq / base_freq)\n",
        "    sruti_label = classify_sruti_offset(offset_cents)\n",
        "    return (offset_cents, sruti_label)\n",
        "\n",
        "\n",
        "def map_pitches_to_notes_22sruti(pitches, tonic=240.0):\n",
        "    \"\"\"\n",
        "    Maps a list of pitch values (in Hz) to Indian musical note names and sruti labels.\n",
        "    \n",
        "    For each pitch:\n",
        "      - Compute the cents difference relative to the tonic.\n",
        "      - Wrap the cents into one octave (0–1200 cents).\n",
        "      - Quantize that range into 22 equal intervals.\n",
        "      - Use the quantized index to assign a sruti label (e.g. \"sruti_X\").\n",
        "      - Map the 22 intervals to the 7-note scale (Sa, Re, Ga, Ma, Pa, Dha, Ni).\n",
        "      - Compute the center frequency of the quantized bin.\n",
        "      - Compute the deviation (in cents) from the center.\n",
        "      - Again, this is interesting, I took the tonic at 240 Hz, but I think I can use the base pitch instead.\n",
        "    \n",
        "    Returns four lists:\n",
        "      - note_names: The mapped note names (e.g. \"Sa\", \"Re\", etc.).\n",
        "      - note_freqs: The frequency corresponding to the center of the quantized note.\n",
        "      - deviations: The deviation (in cents) of the pitch from that center.\n",
        "      - sruti_labels: The raw sruti label (e.g. \"sruti_0\", \"sruti_1\", ..., \"sruti_21\").\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "\n",
        "    note_names = []\n",
        "    note_freqs = []\n",
        "    deviations = []\n",
        "    sruti_labels = []\n",
        "    \n",
        "    # Define the basic note names for the 7-note scale.\n",
        "    basic_notes = [\"Sa\", \"Re\", \"Ga\", \"Ma\", \"Pa\", \"Dha\", \"Ni\"]\n",
        "    \n",
        "    # Each sruti interval spans:\n",
        "    sruti_width = 1200 / 22.0  # approximately 54.55 cents\n",
        "    \n",
        "    for p in pitches:\n",
        "        # Check for invalid pitches: None, <= 0, or NaN.\n",
        "        if p is None or p <= 0 or np.isnan(p):\n",
        "            note_names.append(None)\n",
        "            note_freqs.append(None)\n",
        "            deviations.append(None)\n",
        "            sruti_labels.append(None)\n",
        "            continue\n",
        "        \n",
        "        # Compute the cents difference from the tonic.\n",
        "        cents_diff = 1200 * np.log2(p / tonic)\n",
        "        # Wrap the difference into one octave (0 to 1200 cents).\n",
        "        cents_diff_mod = cents_diff % 1200\n",
        "        \n",
        "        # Quantize into one of 22 sruti intervals.\n",
        "        sruti_index = int(round(cents_diff_mod / sruti_width)) % 22\n",
        "        sruti_label = f\"sruti_{sruti_index}\"\n",
        "        sruti_labels.append(sruti_label)\n",
        "        \n",
        "        # Map the 22 intervals to the 7-note scale via a simple linear mapping.\n",
        "        note_index = int(sruti_index * 7 / 22)\n",
        "        note_index = min(note_index, len(basic_notes) - 1)\n",
        "        note_name = basic_notes[note_index]\n",
        "        note_names.append(note_name)\n",
        "        \n",
        "        # Compute the center of the sruti bin (in cents).\n",
        "        center_cents = sruti_index * sruti_width\n",
        "        # Convert the center cents back to frequency.\n",
        "        center_freq = tonic * (2 ** (center_cents / 1200))\n",
        "        note_freqs.append(center_freq)\n",
        "        \n",
        "        # Compute the deviation in cents from the center.\n",
        "        deviation = cents_diff_mod - center_cents\n",
        "        if deviation > sruti_width / 2:\n",
        "            deviation -= 1200\n",
        "        elif deviation < -sruti_width / 2:\n",
        "            deviation += 1200\n",
        "        deviations.append(deviation)\n",
        "    \n",
        "    return note_names, note_freqs, deviations, sruti_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "praat-hnr",
      "metadata": {},
      "source": [
        "## 7. Module 4: Utility for Praat HNR (larger chunk), plus separate time_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "praat-hnr-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_praat_hnr(audio_chunk, sr):\n",
        "    if len(audio_chunk) == 0:\n",
        "        return 0.0\n",
        "    required_length_sec = 3.0 / 86.1329  # ~0.0348 s\n",
        "    required_length_samples = int(required_length_sec * sr)\n",
        "    if len(audio_chunk) < required_length_samples:\n",
        "        return 0.0\n",
        "\n",
        "    try:\n",
        "        sound = parselmouth.Sound(audio_chunk, sr)\n",
        "        harmonicity = call(sound, \"To Harmonicity (cc)\", 0.01, 86.1329, 0.1, 1.0)\n",
        "        hnr = call(harmonicity, \"Get mean\", 0, 0)\n",
        "        if np.isnan(hnr) or np.isinf(hnr):\n",
        "            return 0.0\n",
        "        return float(hnr)\n",
        "    except:\n",
        "        return 0.0\n",
        "\n",
        "def build_praat_time_matrix(audio_data, sr):\n",
        "    matrix = []\n",
        "    num_samples = len(audio_data)\n",
        "    idx = 0\n",
        "    chunk_counter = 0\n",
        "    chunk_hop = PRAAT_CHUNK_SIZE\n",
        "\n",
        "    while idx < num_samples:\n",
        "        end = idx + chunk_hop\n",
        "        if end > num_samples:\n",
        "            end = num_samples\n",
        "        chunk_data = audio_data[idx:end]\n",
        "        start_time = idx / sr\n",
        "        hnr_value = compute_praat_hnr(chunk_data, sr)\n",
        "\n",
        "        matrix.append({\n",
        "            \"chunk_index\": chunk_counter,\n",
        "            \"start_time_s\": float(start_time),\n",
        "            \"praat_hnr\": hnr_value\n",
        "        })\n",
        "\n",
        "        idx += chunk_hop\n",
        "        chunk_counter += 1\n",
        "\n",
        "    return matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mod7-dynamics-again",
      "metadata": {},
      "source": [
        "## 8. Module 5: Dynamics (RMS & LUFS) with Larger Chunk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dynamics-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_rms_energy_advanced(audio_chunk):\n",
        "    if len(audio_chunk) == 0:\n",
        "        return 0.0\n",
        "    n_fft_local = min(len(audio_chunk), LUFS_CHUNK_SIZE)\n",
        "    rms = librosa.feature.rms(\n",
        "        y=audio_chunk,\n",
        "        frame_length=n_fft_local,\n",
        "        hop_length=n_fft_local\n",
        "    )\n",
        "    return float(rms.mean())\n",
        "\n",
        "def compute_lufs(audio_chunk, sr):\n",
        "    if not LOUDNORM_AVAILABLE:\n",
        "        return None\n",
        "    if len(audio_chunk) == 0:\n",
        "        return None\n",
        "\n",
        "    # Convert to mono if multi-channel\n",
        "    if audio_chunk.ndim > 1:\n",
        "        audio_chunk = np.mean(audio_chunk, axis=1)\n",
        "\n",
        "    # Make sure chunk is long enough for integrated loudness:\n",
        "    meter = pyln.Meter(sr)  # by default block_size=0.4\n",
        "    min_required = int(meter.block_size * sr)  # typically 0.4 * 44100 = 17640\n",
        "    if len(audio_chunk) < min_required:\n",
        "        return 0.0\n",
        "\n",
        "    loudness_val = meter.integrated_loudness(audio_chunk)\n",
        "    return float(loudness_val)\n",
        "\n",
        "def analyze_dynamics_module7(audio_data, sr, time_matrix):\n",
        "    \"\"\"\n",
        "    Now I use LUFS_CHUNK_SIZE (~0.5s) for integrated loudness.\n",
        "    For each row in time_matrix, I gather the start_time and read ~0.5s of audio.\n",
        "    Then store RMS(dB) and LUFS.\n",
        "    Also store summary in the final row.\n",
        "    \"\"\"\n",
        "    rms_values_db = []\n",
        "    lufs_values = []\n",
        "\n",
        "    for i, row in enumerate(time_matrix):\n",
        "        start_time = row['time_s']\n",
        "        start_sample = int(start_time * sr)\n",
        "        end_sample = start_sample + LUFS_CHUNK_SIZE\n",
        "        if end_sample > len(audio_data):\n",
        "            end_sample = len(audio_data)\n",
        "        chunk_data = audio_data[start_sample:end_sample]\n",
        "        # TODO: Grab harmonic data instead?\n",
        "\n",
        "        raw_rms = compute_rms_energy_advanced(chunk_data)\n",
        "        rms_db = 20.0 * np.log10(raw_rms + 1e-12)\n",
        "        row['rms_db'] = rms_db\n",
        "        ########\n",
        "        # : Store RMS category\n",
        "        ########\n",
        "        row['rms_db_category'] = classify_rms_db(rms_db)\n",
        "        rms_values_db.append(rms_db)\n",
        "\n",
        "        lufs_val = compute_lufs(chunk_data, sr)\n",
        "        if lufs_val is None:\n",
        "            lufs_val = 0.0\n",
        "        row['lufs'] = lufs_val\n",
        "        ########\n",
        "        # : Store LUFS category\n",
        "        ########\n",
        "        row['lufs_category'] = classify_lufs(lufs_val)\n",
        "\n",
        "        lufs_values.append(lufs_val)\n",
        "\n",
        "    # Summaries\n",
        "    valid_rms = [x for x in rms_values_db if not np.isnan(x) and not np.isinf(x)]\n",
        "    if valid_rms:\n",
        "        mean_rms = float(np.mean(valid_rms))\n",
        "        med_rms = float(np.median(valid_rms))\n",
        "        min_rms = float(np.min(valid_rms))\n",
        "        max_rms = float(np.max(valid_rms))\n",
        "        range_rms = max_rms - min_rms\n",
        "        std_rms = float(np.std(valid_rms))\n",
        "        dyn_range_rms = range_rms\n",
        "    else:\n",
        "        mean_rms = med_rms = min_rms = max_rms = range_rms = std_rms = dyn_range_rms = 0.0\n",
        "\n",
        "    valid_lufs = [x for x in lufs_values if not np.isnan(x) and not np.isinf(x)]\n",
        "    if valid_lufs:\n",
        "        mean_lufs = float(np.mean(valid_lufs))\n",
        "        med_lufs = float(np.median(valid_lufs))\n",
        "        min_lufs = float(np.min(valid_lufs))\n",
        "        max_lufs = float(np.max(valid_lufs))\n",
        "        range_lufs = max_lufs - min_lufs\n",
        "        std_lufs = float(np.std(valid_lufs))\n",
        "        dyn_range_lufs = range_lufs\n",
        "    else:\n",
        "        mean_lufs = med_lufs = min_lufs = max_lufs = range_lufs = std_lufs = dyn_range_lufs = 0.0\n",
        "\n",
        "    dyn_summary = {\n",
        "        'rms_db': {\n",
        "            'mean': mean_rms,\n",
        "            'median': med_rms,\n",
        "            'min': min_rms,\n",
        "            'max': max_rms,\n",
        "            'range': range_rms,\n",
        "            'std': std_rms,\n",
        "            'dynamic_range': dyn_range_rms\n",
        "        },\n",
        "        'lufs': {\n",
        "            'mean': mean_lufs,\n",
        "            'median': med_lufs,\n",
        "            'min': min_lufs,\n",
        "            'max': max_lufs,\n",
        "            'range': range_lufs,\n",
        "            'std': std_lufs,\n",
        "            'dynamic_range': dyn_range_lufs\n",
        "        }\n",
        "    }\n",
        "\n",
        "    if time_matrix:\n",
        "        time_matrix[-1]['dynamics_summary'] = dyn_summary\n",
        "\n",
        "    return time_matrix, dyn_summary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8-analyze-transitions",
      "metadata": {},
      "source": [
        "## 9. Module 6: Normal time_matrix analysis (tone_to_noise, transitions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "transitions-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_spectral_flatness_chunk(audio_chunk, sr):\n",
        "    if len(audio_chunk) < 2:\n",
        "        return 0.0\n",
        "    # Short-chunk fix:\n",
        "    n_fft_local = min(len(audio_chunk), FRAME_SIZE)\n",
        "    sf = librosa.feature.spectral_flatness(\n",
        "        y=audio_chunk,\n",
        "        n_fft=n_fft_local,\n",
        "        hop_length=HOP_LENGTH\n",
        "    )\n",
        "    return float(np.mean(sf))\n",
        "\n",
        "def analyze_note_transitions(audio_data, sr, time_matrix):\n",
        "    for i, row in enumerate(time_matrix):\n",
        "        start_time = row['time_s']\n",
        "        start_sample = int(start_time * sr)\n",
        "        end_sample = start_sample + HOP_LENGTH\n",
        "        if end_sample > len(audio_data):\n",
        "            end_sample = len(audio_data)\n",
        "        chunk_data = audio_data[start_sample:end_sample]\n",
        "\n",
        "        if len(chunk_data) == 0:\n",
        "            # skip or set default\n",
        "            row['tone_to_noise'] = 0.0\n",
        "            row['tone_to_noise_category'] = \"unknown\"\n",
        "            row['transition_score'] = 0.5\n",
        "            row['transition_category'] = \"moderate\"\n",
        "            continue\n",
        "\n",
        "        # tone_to_noise => spectral flatness\n",
        "        sf_val = compute_spectral_flatness_chunk(chunk_data, sr)\n",
        "        row['tone_to_noise'] = sf_val\n",
        "        # : Store tone category\n",
        "        row['tone_to_noise_category'] = classify_tone_to_noise(sf_val)\n",
        "\n",
        "\n",
        "        # transition_score => pitch difference\n",
        "        if i == 0:\n",
        "            transition_score = 1.0\n",
        "        else:\n",
        "            prev_pitch = time_matrix[i-1].get('pitch_hz', None)\n",
        "            curr_pitch = row.get('pitch_hz', None)\n",
        "            if not prev_pitch or not curr_pitch or prev_pitch <= 0 or curr_pitch <= 0:\n",
        "                transition_score = 0.5\n",
        "            else:\n",
        "                diff_cents = 1200.0 * np.log2(curr_pitch / prev_pitch)\n",
        "                diff_abs = abs(diff_cents)\n",
        "                transition_score = max(0.0, 1.0 - diff_abs/200.0)\n",
        "        row['transition_score'] = transition_score\n",
        "        # : Store transition category\n",
        "        row['transition_category'] = classify_transition_score(transition_score)\n",
        "\n",
        "\n",
        "    return time_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9-spectrogram",
      "metadata": {},
      "source": [
        "## 10. Module 7: Spectrogram Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "spectrogram-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_zero_crossing_rate(audio_chunk):\n",
        "    if len(audio_chunk) == 0:\n",
        "        return 0.0\n",
        "    # Short-chunk fix:\n",
        "    n_fft_local = min(len(audio_chunk), FRAME_SIZE)\n",
        "    zcr = librosa.feature.zero_crossing_rate(\n",
        "        y=audio_chunk,\n",
        "        frame_length=n_fft_local,\n",
        "        hop_length=HOP_LENGTH\n",
        "    )\n",
        "    return float(zcr.mean())\n",
        "\n",
        "def compute_spectral_centroid_advanced(audio_chunk, sr):\n",
        "    if len(audio_chunk) == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    # If the chunk is shorter than FRAME_SIZE, pad it with zeros.\n",
        "    if len(audio_chunk) < FRAME_SIZE:\n",
        "        padding_needed = FRAME_SIZE - len(audio_chunk)\n",
        "        audio_chunk = np.pad(audio_chunk, (0, padding_needed), mode='constant')\n",
        "\n",
        "    n_fft_local = min(len(audio_chunk), FRAME_SIZE)\n",
        "    c = librosa.feature.spectral_centroid(\n",
        "        y=audio_chunk,\n",
        "        sr=sr,\n",
        "        n_fft=n_fft_local,\n",
        "        hop_length=HOP_LENGTH\n",
        "    )\n",
        "    return float(c.mean())\n",
        "\n",
        "def compute_spectral_rolloff_advanced(audio_chunk, sr):\n",
        "\n",
        "    if audio_chunk is None or len(audio_chunk) == 0:\n",
        "        return 0.0\n",
        "    # If the chunk is too short, you can either skip it or pad it.\n",
        "    if len(audio_chunk) < FRAME_SIZE:\n",
        "        # Option A: Return a default value (skip processing)\n",
        "        # return 0.0\n",
        "\n",
        "        # Option B: Pad the chunk so that it meets the minimum length requirement.\n",
        "        padding_needed = FRAME_SIZE - len(audio_chunk)\n",
        "        audio_chunk = np.pad(audio_chunk, (0, padding_needed), mode='constant')\n",
        "\n",
        "\n",
        "    n_fft_local = min(len(audio_chunk), FRAME_SIZE)\n",
        "    r = librosa.feature.spectral_rolloff(\n",
        "        y=audio_chunk,\n",
        "        sr=sr,\n",
        "        n_fft=n_fft_local,\n",
        "        hop_length=HOP_LENGTH\n",
        "    )\n",
        "    return float(r.mean())\n",
        "\n",
        "def compute_spectral_bandwidth_advanced(audio_chunk, sr):\n",
        "    if audio_chunk is None or len(audio_chunk) == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    if len(audio_chunk) < FRAME_SIZE:\n",
        "        # Option A: Return a default value (skip processing)\n",
        "        # return 0.0\n",
        "\n",
        "        # Option B: Pad the chunk so that it meets the minimum length requirement.\n",
        "        padding_needed = FRAME_SIZE - len(audio_chunk)\n",
        "        audio_chunk = np.pad(audio_chunk, (0, padding_needed), mode='constant')\n",
        "\n",
        "    n_fft_local = min(len(audio_chunk), FRAME_SIZE)\n",
        "    bw = librosa.feature.spectral_bandwidth(\n",
        "        y=audio_chunk,\n",
        "        sr=sr,\n",
        "        n_fft=n_fft_local,\n",
        "        hop_length=HOP_LENGTH\n",
        "    )\n",
        "    return float(bw.mean())\n",
        "\n",
        "def compute_mfccs_advanced(audio_chunk, sr, n_mfcc=13):\n",
        "    if audio_chunk is None or len(audio_chunk) == 0:\n",
        "        return [0.0]*n_mfcc    \n",
        "  \n",
        "    # If the audio chunk is too short, pad it with zeros so that it has at least FRAME_SIZE samples\n",
        "    if len(audio_chunk) < FRAME_SIZE:\n",
        "        padding_needed = FRAME_SIZE - len(audio_chunk)\n",
        "        audio_chunk = np.pad(audio_chunk, (0, padding_needed), mode='constant')\n",
        "  \n",
        "    n_fft_local = min(len(audio_chunk), FRAME_SIZE)\n",
        "    mfcc_data = librosa.feature.mfcc(\n",
        "        y=audio_chunk,\n",
        "        sr=sr,\n",
        "        n_mfcc=n_mfcc,\n",
        "        n_fft=n_fft_local,\n",
        "        hop_length=HOP_LENGTH,\n",
        "        fmax=sr/2\n",
        "    )\n",
        "    return mfcc_data.mean(axis=1).tolist()\n",
        "\n",
        "def compute_chroma_advanced(audio_chunk, sr):\n",
        "    if audio_chunk is None or len(audio_chunk) == 0:\n",
        "        return [0.0]*12\n",
        "    \n",
        "    # If the chunk is too short for a single frame, pad it with zeros.\n",
        "    if len(audio_chunk) < FRAME_SIZE:\n",
        "        padding_needed = FRAME_SIZE - len(audio_chunk)\n",
        "        audio_chunk = np.pad(audio_chunk, (0, padding_needed), mode='constant')\n",
        "    \n",
        " \n",
        "    n_fft_local = min(len(audio_chunk), FRAME_SIZE)\n",
        "    c = librosa.feature.chroma_stft(\n",
        "        y=audio_chunk,\n",
        "        sr=sr,\n",
        "        n_fft=n_fft_local,\n",
        "        hop_length=HOP_LENGTH\n",
        "    )\n",
        "    return c.mean(axis=1).tolist()\n",
        "\n",
        "\n",
        "\n",
        "def update_time_matrix_small_with_advanced_features(clean_audio, sr, time_matrix_small):\n",
        "    \"\"\"\n",
        "    For each row in time_matrix_small (which currently are 512-hop chunks), compute overlapping advanced vocal\n",
        "    features on windows of length VOCAL_FEATURE_CHUNK_SIZE, with a hop of VOCAL_FEATURE_CHUNK_HOP.\n",
        "    \n",
        "    Then, for each row in time_matrix_small, find the overlapping advanced feature (computed on the larger window)\n",
        "    whose start time is closest to the row's time and update the row with those advanced metrics.\n",
        "    \n",
        "    Returns:\n",
        "        The updated time_matrix_small, where each row now includes keys such as 'jitter', 'shimmer',\n",
        "        'formants', 'vibrato_extent', 'vibrato_rate', 'asd_details', 'onset_frames', and 'notes'.\n",
        "    \"\"\"\n",
        "    # First, compute overlapping advanced features for the entire audio.\n",
        "    advanced_features_list = []\n",
        "    total_samples = len(clean_audio)\n",
        "    for start_sample in range(0, total_samples - VOCAL_FEATURE_CHUNK_SIZE + 1, VOCAL_FEATURE_CHUNK_HOP):\n",
        "        end_sample = start_sample + VOCAL_FEATURE_CHUNK_SIZE\n",
        "        chunk_data = clean_audio[start_sample:end_sample]\n",
        "        # Compute advanced features on this overlapping chunk.\n",
        "        adv_features = compute_advanced_vocal_features_chunk(chunk_data, sr)\n",
        "        # Record the chunk's start time (in seconds)\n",
        "        adv_features['time_s'] = start_sample / sr\n",
        "        advanced_features_list.append(adv_features)\n",
        "    \n",
        "    # Now, update each row in time_matrix_small.\n",
        "    # For each row (which has a key 'time_s' indicating its start time),\n",
        "    # find the advanced features dictionary whose 'time_s' is closest.\n",
        "    for row in time_matrix_small:\n",
        "        row_time = row['time_s']\n",
        "        # Find the overlapping chunk whose start time is closest to the row's time.\n",
        "        closest = min(advanced_features_list, key=lambda x: abs(x['time_s'] - row_time))\n",
        "        # Update the row with the advanced features from the closest overlapping chunk.\n",
        "        row.update(closest)\n",
        "    \n",
        "    return time_matrix_small"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d2d27df",
      "metadata": {},
      "source": [
        "## 11: Module 8: Advanced features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54c8b2d4",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_jitter_shimmer(audio_data, sr):\n",
        "    \"\"\"\n",
        "    Compute jitter (local) and shimmer (local) on the provided audio array using Praat.\n",
        "    Returns a tuple: (jitter_local, shimmer_local)\n",
        "    \"\"\"\n",
        "    snd = parselmouth.Sound(audio_data, sr)\n",
        "    point_process = call(snd, \"To PointProcess (periodic, cc)\", MIN_F0, MAX_F0)\n",
        "    jitter_local = call(point_process, \"Get jitter (local)\",\n",
        "                          0, 0, JITTER_TIME_STEP, JITTER_MIN_PERIOD, JITTER_MAX_PERIOD)\n",
        "    shimmer_local = call([snd, point_process], \"Get shimmer (local)\",\n",
        "                           0, 0, SHIMMER_MIN_AMPLITUDE, SHIMMER_MAX_AMPLITUDE,\n",
        "                           JITTER_MAX_PERIOD, SHIMMER_FACTOR)\n",
        "    return jitter_local, shimmer_local\n",
        "\n",
        "def compute_formants(audio_data, sr, time=FORMANT_ANALYSIS_TIME, num_formants=NUM_FORMANTS_TO_EXTRACT):\n",
        "    \"\"\"\n",
        "    Extract the first 'num_formants' formant frequencies from the provided audio chunk at a given time.\n",
        "    Returns a dictionary (e.g., {\"F1\": ..., \"F2\": ..., \"F3\": ...}).\n",
        "    \"\"\"\n",
        "    snd = parselmouth.Sound(audio_data, sr)\n",
        "    formant = snd.to_formant_burg(time_step=FORMANT_TIME_STEP,\n",
        "                                  max_number_of_formants=MAX_NUMBER_OF_FORMANTS,\n",
        "                                  maximum_formant=MAXIMUM_FORMANT_FREQUENCY)\n",
        "    extracted = {}\n",
        "    for i in range(1, num_formants+1):\n",
        "        extracted[f\"F{i}\"] = formant.get_value_at_time(i, time)\n",
        "    return extracted\n",
        "\n",
        "def compute_vibrato(audio_data, sr):\n",
        "    \"\"\"\n",
        "    Estimate vibrato extent (std of pitch deviations) and vibrato rate (dominant frequency)\n",
        "    from the pitch contour computed on the provided audio chunk.\n",
        "    Returns (vibrato_extent, vibrato_rate)\n",
        "    \"\"\"\n",
        "    f0, _, _ = librosa.pyin(audio_data, fmin=MIN_F0, fmax=MAX_F0, sr=sr)\n",
        "    valid = ~np.isnan(f0)\n",
        "    if np.sum(valid) == 0:\n",
        "        return None, None\n",
        "    f0 = f0[valid]\n",
        "    smoothed = scipy.signal.medfilt(f0, kernel_size=MEDIAN_FILTER_KERNEL_SIZE)\n",
        "    vibrato_extent = np.std(f0 - smoothed)\n",
        "    deviation = f0 - smoothed\n",
        "    deviation -= np.mean(deviation)\n",
        "    fft_vals = np.fft.rfft(deviation)\n",
        "    freqs = np.fft.rfftfreq(len(deviation), d=HOP_LENGTH/sr)\n",
        "    band = (freqs > VIBRATO_MIN_HZ) & (freqs < VIBRATO_MAX_HZ)\n",
        "    if not np.any(band):\n",
        "        vibrato_rate = None\n",
        "    else:\n",
        "        idx = np.argmax(np.abs(fft_vals[band]))\n",
        "        vibrato_rate = freqs[band][idx]\n",
        "    return vibrato_extent, vibrato_rate\n",
        "\n",
        "def compute_asd(audio_data, sr):\n",
        "    \"\"\"\n",
        "    Compute the RMS envelope and detect onsets for the audio chunk.\n",
        "    Returns (times, rms, onset_frames)\n",
        "    \"\"\"\n",
        "    rms = librosa.feature.rms(y=audio_data, frame_length=FRAME_SIZE, hop_length=HOP_LENGTH)[0]\n",
        "    times = librosa.frames_to_time(np.arange(len(rms)), sr=sr, hop_length=HOP_LENGTH)\n",
        "    onset_frames = librosa.onset.onset_detect(y=audio_data, sr=sr, hop_length=HOP_LENGTH)\n",
        "    return times, rms, onset_frames\n",
        "\n",
        "def segment_note_asd(audio_data, sr, onset_frame, offset_frame):\n",
        "    \"\"\"\n",
        "    For a given note segment (from onset_frame to offset_frame in a chunk), compute:\n",
        "      - Attack time (time to reach peak RMS)\n",
        "      - Sustain duration (duration where RMS >= 90% of peak)\n",
        "      - Decay duration (time from end of sustain to note end)\n",
        "    Returns (attack, sustain, decay)\n",
        "    \"\"\"\n",
        "    note_signal = audio_data[onset_frame * HOP_LENGTH : offset_frame * HOP_LENGTH]\n",
        "    rms = librosa.feature.rms(y=note_signal, frame_length=FRAME_SIZE, hop_length=HOP_LENGTH)[0]\n",
        "    times = librosa.frames_to_time(np.arange(len(rms)), sr=sr, hop_length=HOP_LENGTH)\n",
        "    if len(rms) == 0:\n",
        "        return None, None, None\n",
        "    peak = np.argmax(rms)\n",
        "    attack = times[peak]\n",
        "    sustain_frames = np.where(rms >= 0.9 * rms[peak])[0]\n",
        "    sustain = times[sustain_frames[-1]] - times[sustain_frames[0]] if sustain_frames.size > 0 else 0\n",
        "    decay = times[-1] - times[sustain_frames[-1]] if sustain_frames.size > 0 else 0\n",
        "    return attack, sustain, decay\n",
        "\n",
        "def detect_notes(audio_data, sr):\n",
        "    \"\"\"\n",
        "    Detect onsets and segment the audio chunk into notes.\n",
        "    Returns a list of (start_time, end_time) tuples for the chunk.\n",
        "    \"\"\"\n",
        "    onset_frames = librosa.onset.onset_detect(y=audio_data, sr=sr, hop_length=HOP_LENGTH)\n",
        "    onset_times = librosa.frames_to_time(onset_frames, sr=sr, hop_length=HOP_LENGTH)\n",
        "    notes = []\n",
        "    for i, onset in enumerate(onset_times):\n",
        "        start = onset\n",
        "        end = onset_times[i+1] if i < len(onset_times)-1 else len(audio_data)/sr\n",
        "        notes.append((start, end))\n",
        "    return notes\n",
        "\n",
        "def compute_advanced_vocal_features_chunk(audio_chunk, sr):\n",
        "    \"\"\"\n",
        "    Wrapper: Compute all advanced vocal features on a given audio chunk.\n",
        "    Returns a dictionary containing:\n",
        "      - jitter, shimmer,\n",
        "      - formants (dict with keys F1, F2, F3),\n",
        "      - vibrato_extent, vibrato_rate,\n",
        "      - asd_details (list per detected note),\n",
        "      - onset_frames, and note segmentation (notes)\n",
        "    If the chunk is too short, it is padded.\n",
        "    \"\"\"\n",
        "    if audio_chunk is None or len(audio_chunk) == 0:\n",
        "        return {\n",
        "            \"jitter\": 0.0,\n",
        "            \"shimmer\": 0.0,\n",
        "            \"formants\": {\"F1\": 0.0, \"F2\": 0.0, \"F3\": 0.0},\n",
        "            \"vibrato_extent\": 0.0,\n",
        "            \"vibrato_rate\": 0.0,\n",
        "            \"asd_details\": [],\n",
        "            \"onset_frames\": [],\n",
        "            \"notes\": []\n",
        "        }\n",
        "    # Pad if shorter than VOCAL_FEATURE_CHUNK_SIZE\n",
        "    if len(audio_chunk) < VOCAL_FEATURE_CHUNK_SIZE:\n",
        "        audio_chunk = np.pad(audio_chunk, (0, VOCAL_FEATURE_CHUNK_SIZE - len(audio_chunk)), mode='constant')\n",
        "    \n",
        "    jitter, shimmer = compute_jitter_shimmer(audio_chunk, sr)\n",
        "    formants = compute_formants(audio_chunk, sr)\n",
        "    vibrato_extent, vibrato_rate = compute_vibrato(audio_chunk, sr)\n",
        "    times_asd, rms_asd, onset_frames = compute_asd(audio_chunk, sr)\n",
        "    notes = detect_notes(audio_chunk, sr)\n",
        "    \n",
        "    asd_details = []\n",
        "    for i in range(len(onset_frames)):\n",
        "        onset_frame = onset_frames[i]\n",
        "        offset_frame = onset_frames[i+1] if i < len(onset_frames)-1 else len(rms_asd)-1\n",
        "        attack, sustain, decay = segment_note_asd(audio_chunk, sr, onset_frame, offset_frame)\n",
        "        asd_details.append({\n",
        "            \"attack\": attack,\n",
        "            \"sustain\": sustain,\n",
        "            \"decay\": decay\n",
        "        })\n",
        "    \n",
        "    # At the end of compute_advanced_vocal_features_chunk (for debugging)\n",
        "    features = {\n",
        "        \"jitter\": jitter,\n",
        "        \"shimmer\": shimmer,\n",
        "        \"formants\": formants,\n",
        "        \"vibrato_extent\": vibrato_extent,\n",
        "        \"vibrato_rate\": vibrato_rate,\n",
        "        \"asd_details\": asd_details,\n",
        "        \"onset_frames\": onset_frames.tolist() if isinstance(onset_frames, np.ndarray) else onset_frames,\n",
        "        \"notes\": notes\n",
        "    }\n",
        "    #print(\"Advanced features for chunk:\", features)\n",
        "    return features\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def update_note_transition_analysis_with_advanced_features(clean_audio, sr, note_segments):\n",
        "    \"\"\"\n",
        "    For each note segment (provided as a list of (start_time, end_time) tuples),\n",
        "    compute the advanced vocal features (averaged over the note) and add these\n",
        "    to the note transition analysis dictionary.\n",
        "    \n",
        "    Returns a list of dictionaries, one per note segment.\n",
        "    \"\"\"\n",
        "    advanced_note_metrics = []\n",
        "    for (note_start, note_end) in note_segments:\n",
        "        start_sample = int(note_start * sr)\n",
        "        end_sample = int(note_end * sr)\n",
        "        note_audio = clean_audio[start_sample:end_sample]\n",
        "        # Use the same advanced features functions on the note segment\n",
        "        note_adv = compute_advanced_vocal_features_chunk(note_audio, sr)\n",
        "        # You might also want to compute note-level averages from the ASD details.\n",
        "        advanced_note_metrics.append(note_adv)\n",
        "    return advanced_note_metrics\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mod10-tempo",
      "metadata": {},
      "source": [
        "## 10. Tempo Adherence (512‐hop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tempo512-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_tempo_adherence(clean_audio, sr, time_matrix_small):\n",
        "    \"\"\"\n",
        "    For each chunk in time_matrix_small, estimate local tempo (in BPM)\n",
        "    and store it as row['tempo_bpm']. Uses 512-sample frames.\n",
        "    \"\"\"\n",
        "    import librosa\n",
        "    from librosa.feature import rhythm\n",
        "\n",
        "    for i, row in enumerate(time_matrix_small):\n",
        "        start_time = row['time_s']\n",
        "        start_sample = int(start_time * sr)\n",
        "        end_sample = start_sample + HOP_LENGTH\n",
        "        if end_sample > len(clean_audio):\n",
        "            end_sample = len(clean_audio)\n",
        "        chunk_data = clean_audio[start_sample:end_sample]\n",
        "\n",
        "        if len(chunk_data) == 0 or not np.any(chunk_data):\n",
        "            row['tempo_bpm'] = 0.0\n",
        "            ########\n",
        "            # : Store tempo category\n",
        "            ########\n",
        "            row['tempo_bpm_category'] = \"unknown\"\n",
        "            continue\n",
        "\n",
        "        local_tempo = rhythm.tempo(\n",
        "            y=chunk_data,\n",
        "            sr=sr,\n",
        "            hop_length=HOP_LENGTH,\n",
        "            aggregate=None\n",
        "        )\n",
        "        if (local_tempo is None) or (len(local_tempo) == 0):\n",
        "            row['tempo_bpm'] = 0.0\n",
        "            row['tempo_bpm_category'] = \"unknown\"\n",
        "        else:\n",
        "            ########\n",
        "            # : Store tempo category\n",
        "            ########\n",
        "            tempo_val = float(np.mean(local_tempo))\n",
        "            row['tempo_bpm'] = tempo_val\n",
        "            row['tempo_bpm_category'] = classify_tempo_bpm(tempo_val)\n",
        "\n",
        "\n",
        "    return time_matrix_small"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mod10b-tempo",
      "metadata": {},
      "source": [
        "## 10b. Additional Multi‐chunk Tempo (4096, 16384)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tempo-larger-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_larger_tempo_matrix(clean_audio, sr, chunk_size=4096, overlap=0.5):\n",
        "    \"\"\"\n",
        "    Build a separate time matrix for tempo analysis using a larger chunk.\n",
        "    e.g. chunk_size=4096 or 16384, with default 50% overlap => hop=chunk_size/2.\n",
        "    \"\"\"\n",
        "    import librosa\n",
        "    from librosa.feature import rhythm\n",
        "\n",
        "    matrix = []\n",
        "    num_samples = len(clean_audio)\n",
        "    chunk_hop = int(chunk_size * (1.0 - overlap))\n",
        "    idx = 0\n",
        "    chunk_counter = 0\n",
        "\n",
        "    while idx < num_samples:\n",
        "        end = idx + chunk_size\n",
        "        if end > num_samples:\n",
        "            end = num_samples\n",
        "        chunk_data = clean_audio[idx:end]\n",
        "        start_time = idx / sr\n",
        "\n",
        "        if len(chunk_data) == 0 or not np.any(chunk_data):\n",
        "            tempo_bpm = 0.0\n",
        "            tempo_bpm_category = \"unknown\"\n",
        "\n",
        "        else:\n",
        "            local_tempo = rhythm.tempo(\n",
        "                y=chunk_data,\n",
        "                sr=sr,\n",
        "                hop_length=HOP_LENGTH,\n",
        "                aggregate=None\n",
        "            )\n",
        "            if (local_tempo is None) or (len(local_tempo) == 0):\n",
        "                tempo_bpm = 0.0\n",
        "                tempo_bpm_category = \"unknown\"\n",
        "\n",
        "            else:\n",
        "                tempo_bpm = float(np.mean(local_tempo))\n",
        "                ########\n",
        "                # : Store tempo category\n",
        "                ########\n",
        "                tempo_bpm_category = classify_tempo_bpm(tempo_bpm)\n",
        "\n",
        "\n",
        "        matrix.append({\n",
        "            \"chunk_index\": chunk_counter,\n",
        "            \"start_time_s\": float(start_time),\n",
        "            \"tempo_bpm\": tempo_bpm,\n",
        "            \"tempo_bpm_category\": tempo_bpm_category  # <--- \n",
        "\n",
        "        })\n",
        "\n",
        "        idx += chunk_hop\n",
        "        chunk_counter += 1\n",
        "\n",
        "    return matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95121eae",
      "metadata": {},
      "source": [
        "## 18 - Raga Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68728036",
      "metadata": {},
      "outputs": [],
      "source": [
        "raga_db = [\n",
        "    {\n",
        "        \"name\": \"Yaman\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"N3\", \"R2\", \"G2\", \"M2\", \"P\", \"D2\", \"N3\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"N3\", \"D2\", \"P\", \"M2\", \"G2\", \"R2\", \"S\"]\n",
        "        },\n",
        "        \"Istern_scale\": {\n",
        "            \"aaroh\": [\"B3\", \"D4\", \"E4\", \"F#4\", \"G4\", \"A4\", \"B4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"B4\", \"A4\", \"G4\", \"F#4\", \"E4\", \"D4\", \"B3\"]\n",
        "        },\n",
        "        \"vaadi\": \"G2\",\n",
        "        \"samvaadi\": \"N3\",\n",
        "        \"typical_phrases\": [\n",
        "            \"N3 R2 G2 M2 G2 R2 S\",\n",
        "            \"R2 G2 R2 S\",\n",
        "            \"G2 M2 D2 N3 S'\",\n",
        "            \"M2 G2 R2 S\",\n",
        "            \"N3 D2 P M2 G2\",\n",
        "            \"N3 R2 S\",\n",
        "            \"R2 G2 M2 P D2 N3 S'\",\n",
        "            \"M2 G2 R2 S N3\",\n",
        "            \"R2 S N3 D2 P\",\n",
        "            \"G2 M2 D2 N3 R2 S\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"R2 from S with a gentle glide\", \n",
        "            \"G2 from R2 with slight kampan\",\n",
        "            \"M2 from G2 with a meend\"\n",
        "        ],\n",
        "        \"skipped_swars\": [],\n",
        "        \"microtonal_nuances\": \"Tivra Ma (M2) is emphasized; Ni is shuddha. Subtle meends on Re-Ga.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Bhairav\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"S\", \"r1\", \"G2\", \"M1\", \"P\", \"d1\", \"N2\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"N2\", \"d1\", \"P\", \"M1\", \"G2\", \"r1\", \"S\"]\n",
        "        },\n",
        "        \"Istern_scale\": {\n",
        "            \"aaroh\": [\"C4\", \"Db4\", \"E4\", \"F4\", \"G4\", \"Ab4\", \"Bb4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"Bb4\", \"Ab4\", \"G4\", \"F4\", \"E4\", \"Db4\", \"C4\"]\n",
        "        },\n",
        "        \"vaadi\": \"d1\",\n",
        "        \"samvaadi\": \"r1\",\n",
        "        \"typical_phrases\": [\n",
        "            \"S r1 G2 M1 P\",\n",
        "            \"r1 G2 M1 P d1 N2 S'\",\n",
        "            \"S' N2 d1 P M1 G2 r1 S\",\n",
        "            \"M1 G2 r1 S\",\n",
        "            \"r1 G2 M1\",\n",
        "            \"G2 r1 S r1\",\n",
        "            \"P d1 N2 S'\",\n",
        "            \"S' N2 d1 P\",\n",
        "            \"r1 S r1 G2\",\n",
        "            \"M1 G2 r1 S\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"r1 from S with a slow andolan\",\n",
        "            \"d1 from P with a slight andolan\"\n",
        "        ],\n",
        "        \"skipped_swars\": [],\n",
        "        \"microtonal_nuances\": \"Komal Re (r1) and Komal Dha (d1) each with andolan; Ga is shuddha.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Todi\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"S\", \"r1\", \"g1\", \"M2\", \"P\", \"d1\", \"N2\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"N2\", \"d1\", \"P\", \"M2\", \"g1\", \"r1\", \"S\"]\n",
        "        },\n",
        "        \"Istern_scale\": {\n",
        "            \"aaroh\": [\"C4\", \"Db4\", \"Eb4\", \"F#4\", \"G4\", \"Ab4\", \"Bb4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"Bb4\", \"Ab4\", \"G4\", \"F#4\", \"Eb4\", \"Db4\", \"C4\"]\n",
        "        },\n",
        "        \"vaadi\": \"r1\",\n",
        "        \"samvaadi\": \"g1\",\n",
        "        \"typical_phrases\": [\n",
        "            \"S r1 g1 M2 P d1 N2 S'\",\n",
        "            \"M2 g1 r1 S\",\n",
        "            \"r1 g1 M2 d1 N2 S'\",\n",
        "            \"N2 d1 P M2 g1 r1 S\",\n",
        "            \"P M2 g1 r1\",\n",
        "            \"g1 M2 d1 N2 S'\",\n",
        "            \"r1 S N2 d1 P\",\n",
        "            \"g1 r1 S\",\n",
        "            \"r1 g1 M2 P\",\n",
        "            \"d1 N2 S'\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"r1 is approached with delicate andolan\",\n",
        "            \"g1 from r1 with slow glide\"\n",
        "        ],\n",
        "        \"skipped_swars\": [],\n",
        "        \"microtonal_nuances\": \"Komal Re, Ga, Tivra Ma; Re and Ga often have slow glides.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Bilawal\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"S\", \"R2\", \"G2\", \"M1\", \"P\", \"D2\", \"N2\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"N2\", \"D2\", \"P\", \"M1\", \"G2\", \"R2\", \"S\"]\n",
        "        },\n",
        "        \"Istern_scale\": {\n",
        "            \"aaroh\": [\"C4\", \"D4\", \"E4\", \"F4\", \"G4\", \"A4\", \"B4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"B4\", \"A4\", \"G4\", \"F4\", \"E4\", \"D4\", \"C4\"]\n",
        "        },\n",
        "        \"vaadi\": \"D2\",\n",
        "        \"samvaadi\": \"G2\",\n",
        "        \"typical_phrases\": [\n",
        "            \"S R2 G2 M1 P D2 N2 S'\",\n",
        "            \"P D2 N2 S'\",\n",
        "            \"G2 R2 S\",\n",
        "            \"R2 G2 M1 G2 R2 S\",\n",
        "            \"N2 D2 P M1 G2 R2 S\",\n",
        "            \"M1 P D2 N2 S'\",\n",
        "            \"S' N2 D2 P\",\n",
        "            \"G2 M1 G2 R2 S\",\n",
        "            \"D2 P M1 G2\",\n",
        "            \"R2 S N2 D2 P\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"R2 from S with direct approach\", \n",
        "            \"M1 from G2 with a slight meend\"\n",
        "        ],\n",
        "        \"skipped_swars\": [],\n",
        "        \"microtonal_nuances\": \"All shuddha except M1 can have subtle andolan; typically straightforward.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Kafi\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"S\", \"R2\", \"G2\", \"M1\", \"P\", \"D2\", \"N2\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"N2\", \"D2\", \"P\", \"M1\", \"G2\", \"R2\", \"S\"]\n",
        "        },\n",
        "        \"Istern_scale\": {\n",
        "            \"aaroh\": [\"C4\", \"D4\", \"E4\", \"F4\", \"G4\", \"A4\", \"B4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"B4\", \"A4\", \"G4\", \"F4\", \"E4\", \"D4\", \"C4\"]\n",
        "        },\n",
        "        \"vaadi\": \"P\",\n",
        "        \"samvaadi\": \"R2\",\n",
        "        \"typical_phrases\": [\n",
        "            \"S R2 G2 M1 P D2 N2 S'\",\n",
        "            \"P D2 N2 S'\",\n",
        "            \"N2 S' R2 S\",\n",
        "            \"G2 M1 P D2 N2 S'\",\n",
        "            \"P M1 G2 R2 S\",\n",
        "            \"R2 G2 M1 R2 S\",\n",
        "            \"M1 P D2 N2\",\n",
        "            \"S' N2 D2 P\",\n",
        "            \"R2 S N2 D2 P\",\n",
        "            \"G2 M1 G2 R2 S\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"R2 from S directly\",\n",
        "            \"G2 from R2 with a small glide\"\n",
        "        ],\n",
        "        \"skipped_swars\": [],\n",
        "        \"microtonal_nuances\": \"All shuddha notes; minor kampan on Ga or Ni in folk styles.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Bhupali\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"S\", \"R2\", \"G2\", \"P\", \"D2\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"D2\", \"P\", \"G2\", \"R2\", \"S\"]\n",
        "        },\n",
        "        \"Istern_scale\": {\n",
        "            \"aaroh\": [\"C4\", \"D4\", \"E4\", \"G4\", \"A4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"A4\", \"G4\", \"E4\", \"D4\", \"C4\"]\n",
        "        },\n",
        "        \"vaadi\": \"G2\",\n",
        "        \"samvaadi\": \"D2\",\n",
        "        \"typical_phrases\": [\n",
        "            \"S R2 G2 P D2 S'\",\n",
        "            \"R2 G2 P D2 S'\",\n",
        "            \"S' D2 P G2 R2 S\",\n",
        "            \"R2 S G2 P D2 S'\",\n",
        "            \"P G2 R2 S\",\n",
        "            \"G2 P D2 S'\",\n",
        "            \"S' D2 P G2\",\n",
        "            \"R2 G2 P\",\n",
        "            \"G2 R2 S\",\n",
        "            \"D2 P R2 S\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"R2 from S softly\",\n",
        "            \"G2 from R2 with a meend\"\n",
        "        ],\n",
        "        \"skipped_swars\": [\"M1\", \"N2\"],\n",
        "        \"microtonal_nuances\": \"Shuddha Re, Ga, Dha; typically minimal microtonal oscillation.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Kalyan\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"S\", \"R2\", \"G2\", \"M2\", \"P\", \"D2\", \"N2\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"N2\", \"D2\", \"P\", \"M2\", \"G2\", \"R2\", \"S\"]\n",
        "        },\n",
        "        \"Istern_scale\": {\n",
        "            \"aaroh\": [\"C4\", \"D4\", \"E4\", \"F#4\", \"G4\", \"A4\", \"B4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"B4\", \"A4\", \"G4\", \"F#4\", \"E4\", \"D4\", \"C4\"]\n",
        "        },\n",
        "        \"vaadi\": \"R2\", \n",
        "        \"samvaadi\": \"P\", \n",
        "        \"typical_phrases\": [\n",
        "            \"S R2 G2 M2 P D2 N2 S'\",\n",
        "            \"R2 G2 M2 P\",\n",
        "            \"M2 P D2 N2 S'\",\n",
        "            \"N2 D2 P M2 G2 R2 S\",\n",
        "            \"G2 M2 R2 S\",\n",
        "            \"R2 S N2 D2 P\",\n",
        "            \"M2 G2 R2 S\",\n",
        "            \"R2 G2 M2 G2 R2 S\",\n",
        "            \"D2 N2 S'\",\n",
        "            \"P D2 N2\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"M2 from G2 with a slight meend\",\n",
        "            \"N2 from D2 quickly\"\n",
        "        ],\n",
        "        \"skipped_swars\": [],\n",
        "        \"microtonal_nuances\": \"Tivra Ma (M2), rest shuddha. Subtle meends from Re to Ga.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Bageshree\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"S\", \"G2\", \"M1\", \"D2\", \"N2\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"N2\", \"D2\", \"M1\", \"G2\", \"R2\", \"S\"]\n",
        "        },\n",
        "        \"Istern_scale\": {\n",
        "            \"aaroh\": [\"C4\", \"E4\", \"F4\", \"A4\", \"B4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"B4\", \"A4\", \"F4\", \"E4\", \"D4\", \"C4\"]\n",
        "        },\n",
        "        \"vaadi\": \"M1\",\n",
        "        \"samvaadi\": \"S\",\n",
        "        \"typical_phrases\": [\n",
        "            \"S G2 M1 D2 N2 S'\",\n",
        "            \"S' N2 D2 M1 G2 S\",\n",
        "            \"M1 D2 N2 S'\",\n",
        "            \"N2 S' M1 G2 S\",\n",
        "            \"G2 M1 G2 S\",\n",
        "            \"D2 M1 G2 S\",\n",
        "            \"M1 D2 N2 S'\",\n",
        "            \"S' N2 D2 M1 G2 S\",\n",
        "            \"G2 M1 R2 S\",\n",
        "            \"S G2 M1\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"G2 from S with a slight meend\",\n",
        "            \"M1 from G2 directly\"\n",
        "        ],\n",
        "        \"skipped_swars\": [\"P\"],\n",
        "        \"microtonal_nuances\": \"No Pa in aaroh, subtle andolan on Ga or Dha in slow elaborations.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Durga\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"S\", \"R2\", \"M1\", \"P\", \"D2\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"D2\", \"P\", \"M1\", \"R2\", \"S\"]\n",
        "        },\n",
        "        \"Istern_scale\": {\n",
        "            \"aaroh\": [\"C4\", \"D4\", \"F4\", \"G4\", \"A4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"A4\", \"G4\", \"F4\", \"D4\", \"C4\"]\n",
        "        },\n",
        "        \"vaadi\": \"R2\",\n",
        "        \"samvaadi\": \"D2\",\n",
        "        \"typical_phrases\": [\n",
        "            \"S R2 M1 P D2 S'\",\n",
        "            \"R2 M1 P D2 S'\",\n",
        "            \"S' D2 P M1 R2 S\",\n",
        "            \"M1 R2 S\",\n",
        "            \"P D2 S'\",\n",
        "            \"R2 M1 R2 S\",\n",
        "            \"M1 P D2 S'\",\n",
        "            \"S' D2 P M1\",\n",
        "            \"R2 S D2 P\",\n",
        "            \"M1 P R2 S\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"R2 from S quickly\",\n",
        "            \"M1 from R2\"\n",
        "        ],\n",
        "        \"skipped_swars\": [\"G2\", \"N2\"],\n",
        "        \"microtonal_nuances\": \"All shuddha except no Ga or Ni. Minimal microtonal usage.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Desh\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"S\", \"R2\", \"M1\", \"P\", \"N2\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"N2\", \"D2\", \"P\", \"M1\", \"G2\", \"R2\", \"S\"]\n",
        "        },\n",
        "        \"Istern_scale\": {\n",
        "            \"aaroh\": [\"C4\", \"D4\", \"F4\", \"G4\", \"B4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"B4\", \"A4\", \"G4\", \"F4\", \"E4\", \"D4\", \"C4\"]\n",
        "        },\n",
        "        \"vaadi\": \"R2\",\n",
        "        \"samvaadi\": \"P\",\n",
        "        \"typical_phrases\": [\n",
        "            \"S R2 M1 P N2 S'\",\n",
        "            \"R2 M1 P N2 S'\",\n",
        "            \"S' N2 D2 P\",\n",
        "            \"M1 G2 R2 S\",\n",
        "            \"R2 S N2 D2 P\",\n",
        "            \"M1 P N2 S'\",\n",
        "            \"S' N2 D2 P M1 G2 R2 S\",\n",
        "            \"M1 P N2 S' R2 S\",\n",
        "            \"G2 R2 S\",\n",
        "            \"R2 M1 P\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"R2 from S with mild meend\",\n",
        "            \"N2 from P\"\n",
        "        ],\n",
        "        \"skipped_swars\": [\"G2\"  # partial usage in aaroh\n",
        "        ],\n",
        "        \"microtonal_nuances\": \"Ga touched in avroh. Some variations allow slow meends on N2.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Malkauns\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"S\", \"g1\", \"M1\", \"d1\", \"n1\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"n1\", \"d1\", \"M1\", \"g1\", \"S\"]\n",
        "        },\n",
        "        \"Istern_scale\": {\n",
        "            \"aaroh\": [\"C4\", \"Eb4\", \"F4\", \"Ab4\", \"Bb4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"Bb4\", \"Ab4\", \"F4\", \"Eb4\", \"C4\"]\n",
        "        },\n",
        "        \"vaadi\": \"M1\",\n",
        "        \"samvaadi\": \"S\",\n",
        "        \"typical_phrases\": [\n",
        "            \"S g1 M1 d1 n1 S'\",\n",
        "            \"g1 M1 d1 n1 S'\",\n",
        "            \"S' n1 d1 M1 g1 S\",\n",
        "            \"n1 S' g1 M1 S\",\n",
        "            \"d1 n1 S'\",\n",
        "            \"g1 M1 g1 S\",\n",
        "            \"d1 n1 S' n1 d1\",\n",
        "            \"M1 g1 S\",\n",
        "            \"S g1 M1 d1\",\n",
        "            \"n1 d1 M1 g1 S\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"g1 from S with an andolan\",\n",
        "            \"d1 from M1 with a slow approach\"\n",
        "        ],\n",
        "        \"skipped_swars\": [\"R2\", \"G2\", \"P\", \"D2\", \"N2\"],\n",
        "        \"microtonal_nuances\": \"Komal Ga, Dha, Ni each can have deep andolan style.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Hamsadhwani\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"S\", \"R2\", \"G2\", \"P\", \"N2\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"N2\", \"P\", \"G2\", \"R2\", \"S\"]\n",
        "        },\n",
        "        \"Istern_scale\": {\n",
        "            \"aaroh\": [\"C4\", \"D4\", \"E4\", \"G4\", \"B4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"B4\", \"G4\", \"E4\", \"D4\", \"C4\"]\n",
        "        },\n",
        "        \"vaadi\": \"G2\",\n",
        "        \"samvaadi\": \"N2\",\n",
        "        \"typical_phrases\": [\n",
        "            \"S R2 G2 P N2 S'\",\n",
        "            \"R2 G2 P N2 S'\",\n",
        "            \"S' N2 P G2 R2 S\",\n",
        "            \"R2 S G2 P\",\n",
        "            \"G2 P N2 S'\",\n",
        "            \"N2 S' R2 G2\",\n",
        "            \"P G2 R2 S\",\n",
        "            \"S R2 G2\",\n",
        "            \"R2 G2 P\",\n",
        "            \"G2 P N2 S'\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"R2 from S quickly\",\n",
        "            \"G2 from R2 direct approach\"\n",
        "        ],\n",
        "        \"skipped_swars\": [\"M1\", \"D2\"],\n",
        "        \"microtonal_nuances\": \"Primarily shuddha notes, slight meend R2->G2.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Jaunpuri\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"S\", \"R2\", \"g1\", \"M1\", \"P\", \"d1\", \"N2\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"N2\", \"d1\", \"P\", \"M1\", \"g1\", \"R2\", \"S\"]\n",
        "        },\n",
        "        \"Istern_scale\": {\n",
        "            \"aaroh\": [\"C4\", \"D4\", \"Eb4\", \"F4\", \"G4\", \"Ab4\", \"B4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"B4\", \"Ab4\", \"G4\", \"F4\", \"Eb4\", \"D4\", \"C4\"]\n",
        "        },\n",
        "        \"vaadi\": \"P\",\n",
        "        \"samvaadi\": \"S\",\n",
        "        \"typical_phrases\": [\n",
        "            \"S R2 g1 M1 P d1 N2 S'\",\n",
        "            \"R2 g1 M1 P d1 N2 S'\",\n",
        "            \"S' N2 d1 P\",\n",
        "            \"M1 g1 R2 S\",\n",
        "            \"g1 M1 P d1 N2 S'\",\n",
        "            \"P d1 N2 S'\",\n",
        "            \"R2 S N2 d1 P\",\n",
        "            \"g1 R2 S\",\n",
        "            \"d1 N2 S'\",\n",
        "            \"M1 P d1 N2 S'\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"g1 from R2 with a small meend\",\n",
        "            \"d1 from P\"\n",
        "        ],\n",
        "        \"skipped_swars\": [],\n",
        "        \"microtonal_nuances\": \"Komal Ga, Dha. Some meends from Re to ga.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Alhaiya Bilawal\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"S\", \"R2\", \"G2\", \"M1\", \"P\", \"D2\", \"N2\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"N2\", \"D2\", \"P\", \"M1\", \"G2\", \"R2\", \"S\"]\n",
        "        },\n",
        "        \"Istern_scale\": {\n",
        "            \"aaroh\": [\"C4\", \"D4\", \"E4\", \"F4\", \"G4\", \"A4\", \"B4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"B4\", \"A4\", \"G4\", \"F4\", \"E4\", \"D4\", \"C4\"]\n",
        "        },\n",
        "        \"vaadi\": \"D2\",\n",
        "        \"samvaadi\": \"G2\",\n",
        "        \"typical_phrases\": [\n",
        "            \"S R2 G2 M1 P D2 N2 S'\",\n",
        "            \"R2 G2 M1 P D2 N2 S'\",\n",
        "            \"N2 D2 P M1 G2 R2 S\",\n",
        "            \"G2 M1 G2 R2 S\",\n",
        "            \"M1 P D2 N2 S'\",\n",
        "            \"R2 S N2 D2 P\",\n",
        "            \"S' N2 D2 P\",\n",
        "            \"R2 G2 M1 R2 S\",\n",
        "            \"D2 P M1 G2\",\n",
        "            \"R2 S N2 D2 P\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"R2 from S\",\n",
        "            \"G2 from R2 with meend\"\n",
        "        ],\n",
        "        \"skipped_swars\": [],\n",
        "        \"microtonal_nuances\": \"Similar to Bilawal but with special phrases. Shuddha notes mostly.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Kedar\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"S\", \"M1\", \"M2\", \"P\", \"N2\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"N2\", \"P\", \"M2\", \"M1\", \"G2\", \"R2\", \"S\"]\n",
        "        },\n",
        "        \"Istern_scale\": {\n",
        "            \"aaroh\": [\"C4\", \"F4\", \"F#4\", \"G4\", \"B4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"B4\", \"G4\", \"F#4\", \"F4\", \"E4\", \"D4\", \"C4\"]\n",
        "        },\n",
        "        \"vaadi\": \"M2\",\n",
        "        \"samvaadi\": \"S\",\n",
        "        \"typical_phrases\": [\n",
        "            \"S M1 M2 P N2 S'\",\n",
        "            \"M1 M2 P N2 S'\",\n",
        "            \"S' N2 P M2 M1 G2 R2 S\",\n",
        "            \"M2 P N2 S'\",\n",
        "            \"N2 P M2 M1\",\n",
        "            \"M1 G2 R2 S\",\n",
        "            \"M2 P N2 S'\",\n",
        "            \"P N2 S'\",\n",
        "            \"M1 M2 P\",\n",
        "            \"G2 R2 S\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"M2 from M1 with a quick oscillation\",\n",
        "            \"N2 from P direct\"\n",
        "        ],\n",
        "        \"skipped_swars\": [],\n",
        "        \"microtonal_nuances\": \"Both Ma shuddha (M1) and tivra (M2). Dramatic meends on Ma.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Patdeep\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"N2\", \"S\", \"G2\", \"M1\", \"P\", \"N2\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"N2\", \"P\", \"M1\", \"G2\", \"R2\", \"S\"]\n",
        "        },\n",
        "        \"Istern_scale\": {\n",
        "            \"aaroh\": [\"B3\", \"C4\", \"E4\", \"F4\", \"G4\", \"B4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"B4\", \"G4\", \"F4\", \"E4\", \"D4\", \"C4\"]\n",
        "        },\n",
        "        \"vaadi\": \"P\",\n",
        "        \"samvaadi\": \"S\",\n",
        "        \"typical_phrases\": [\n",
        "            \"N2 S G2 M1 P N2 S'\",\n",
        "            \"S G2 M1 P\",\n",
        "            \"N2 S' N2 P\",\n",
        "            \"P M1 G2 R2 S\",\n",
        "            \"G2 M1 P N2 S'\",\n",
        "            \"N2 P M1 G2\",\n",
        "            \"P N2 S' R2 S\",\n",
        "            \"G2 R2 S\",\n",
        "            \"N2 S' N2\",\n",
        "            \"S G2 M1\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"N2 from P\",\n",
        "            \"G2 from S\"\n",
        "        ],\n",
        "        \"skipped_swars\": [\"D2\"],\n",
        "        \"microtonal_nuances\": \"Shuddha Ni, approach from loIr Pa. Subtle meends G2 <-> M1.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Darbari Kanada\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"S\", \"R2\", \"r1\", \"G2\", \"M1\", \"P\", \"D2\", \"n1\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"n1\", \"D2\", \"P\", \"M1\", \"G2\", \"r1\", \"S\"]\n",
        "        },\n",
        "        \"Istern_scale\": {\n",
        "            \"aaroh\": [\"C4\", \"D4\", \"Db4\", \"E4\", \"F4\", \"G4\", \"A4\", \"Ab4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"Ab4\", \"A4\", \"G4\", \"F4\", \"E4\", \"Db4\", \"C4\"]\n",
        "        },\n",
        "        \"vaadi\": \"R2\",\n",
        "        \"samvaadi\": \"P\",\n",
        "        \"typical_phrases\": [\n",
        "            \"S R2 r1 G2 M1 P D2 n1 S'\",\n",
        "            \"r1 G2 M1 P D2 n1 S'\",\n",
        "            \"S' n1 D2 P M1 G2 r1 S\",\n",
        "            \"G2 r1 S\",\n",
        "            \"r1 G2 M1\",\n",
        "            \"P D2 n1 S'\",\n",
        "            \"n1 S' r1 G2\",\n",
        "            \"M1 G2 r1 S\",\n",
        "            \"D2 P M1 G2\",\n",
        "            \"r1 S n1 D2 P\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"r1 from S with deep andolan\",\n",
        "            \"n1 from D2 with slow oscillation\"\n",
        "        ],\n",
        "        \"skipped_swars\": [],\n",
        "        \"microtonal_nuances\": \"Famous for andolan on Re and Ga, Komal Ni with heavy oscillation.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Shankara\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"S\", \"R2\", \"S\", \"R2\", \"M1\", \"P\", \"N2\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"N2\", \"P\", \"M1\", \"R2\", \"S\"]\n",
        "        },\n",
        "        \"Istern_scale\": {\n",
        "            \"aaroh\": [\"C4\", \"D4\", \"C4\", \"D4\", \"F4\", \"G4\", \"B4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"B4\", \"G4\", \"F4\", \"D4\", \"C4\"]\n",
        "        },\n",
        "        \"vaadi\": \"R2\",\n",
        "        \"samvaadi\": \"P\",\n",
        "        \"typical_phrases\": [\n",
        "            \"S R2 S R2 M1 P N2 S'\",\n",
        "            \"R2 S R2 M1 P\",\n",
        "            \"M1 P N2 S'\",\n",
        "            \"N2 S' R2 S\",\n",
        "            \"R2 M1 P N2 S'\",\n",
        "            \"S' N2 P M1 R2 S\",\n",
        "            \"P M1 R2 S R2\",\n",
        "            \"M1 R2 S\",\n",
        "            \"R2 S N2 P\",\n",
        "            \"M1 P N2\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"R2 from S repeated\",\n",
        "            \"N2 from P directly\"\n",
        "        ],\n",
        "        \"skipped_swars\": [\"G2\", \"D2\"],\n",
        "        \"microtonal_nuances\": \"Shuddha notes, repeated Re usage. Some meend from M1 to P.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Shree\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"S\", \"r1\", \"M1\", \"P\", \"N2\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"N2\", \"P\", \"M1\", \"r1\", \"S\"]\n",
        "        },\n",
        "        \"Istern_scale\": {\n",
        "            \"aaroh\": [\"C4\", \"Db4\", \"F4\", \"G4\", \"B4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"B4\", \"G4\", \"F4\", \"Db4\", \"C4\"]\n",
        "        },\n",
        "        \"vaadi\": \"P\",\n",
        "        \"samvaadi\": \"r1\",\n",
        "        \"typical_phrases\": [\n",
        "            \"S r1 M1 P N2 S'\",\n",
        "            \"r1 M1 P N2 S'\",\n",
        "            \"S' N2 P M1 r1 S\",\n",
        "            \"M1 r1 S\",\n",
        "            \"P N2 S'\",\n",
        "            \"r1 M1 P\",\n",
        "            \"N2 P r1 S\",\n",
        "            \"S' N2 P\",\n",
        "            \"r1 S r1 M1\",\n",
        "            \"M1 r1 S\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"r1 from S with andolan\",\n",
        "            \"N2 from P\"\n",
        "        ],\n",
        "        \"skipped_swars\": [\"G2\", \"D2\"],\n",
        "        \"microtonal_nuances\": \"Komal Re with slow oscillation, rest shuddha. P is strong note.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Marwa\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"S\", \"r1\", \"M2\", \"D2\", \"N2\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"N2\", \"D2\", \"M2\", \"r1\", \"S\"]\n",
        "        },\n",
        "        \"Istern_scale\": {\n",
        "            \"aaroh\": [\"C4\", \"Db4\", \"F#4\", \"A4\", \"B4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"B4\", \"A4\", \"F#4\", \"Db4\", \"C4\"]\n",
        "        },\n",
        "        \"vaadi\": \"r1\",\n",
        "        \"samvaadi\": \"D2\",\n",
        "        \"typical_phrases\": [\n",
        "            \"S r1 M2 D2 N2 S'\",\n",
        "            \"r1 M2 D2 N2 S'\",\n",
        "            \"S' N2 D2 M2 r1 S\",\n",
        "            \"D2 N2 S'\",\n",
        "            \"M2 D2 N2 S'\",\n",
        "            \"r1 S N2 D2\",\n",
        "            \"M2 r1 S\",\n",
        "            \"r1 M2 r1 S\",\n",
        "            \"N2 S' r1 M2\",\n",
        "            \"D2 N2 S'\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"r1 from S with tension\",\n",
        "            \"M2 from r1\"\n",
        "        ],\n",
        "        \"skipped_swars\": [\"G2\", \"P\"],\n",
        "        \"microtonal_nuances\": \"Komal Re with tension, Tivra Ma, no Pa. Tension Re-Ma hallmark.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Puriya\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"S\", \"r1\", \"G2\", \"M2\", \"D2\", \"N2\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"N2\", \"D2\", \"M2\", \"G2\", \"r1\", \"S\"]\n",
        "        },\n",
        "        \"Istern_scale\": {\n",
        "            \"aaroh\": [\"C4\", \"Db4\", \"E4\", \"F#4\", \"A4\", \"B4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"B4\", \"A4\", \"F#4\", \"E4\", \"Db4\", \"C4\"]\n",
        "        },\n",
        "        \"vaadi\": \"G2\",\n",
        "        \"samvaadi\": \"r1\",\n",
        "        \"typical_phrases\": [\n",
        "            \"S r1 G2 M2 D2 N2 S'\",\n",
        "            \"r1 G2 M2 D2 N2 S'\",\n",
        "            \"S' N2 D2 M2 G2 r1 S\",\n",
        "            \"r1 S r1 G2\",\n",
        "            \"G2 M2 D2 N2 S'\",\n",
        "            \"N2 S' r1 G2\",\n",
        "            \"M2 G2 r1 S\",\n",
        "            \"r1 G2 M2\",\n",
        "            \"D2 N2 S'\",\n",
        "            \"G2 r1 S\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"r1 from S with a meend\",\n",
        "            \"G2 from r1 quickly\"\n",
        "        ],\n",
        "        \"skipped_swars\": [\"P\"],\n",
        "        \"microtonal_nuances\": \"Similar tension as Marwa, skipping Pa, Komal Re, Tivra Ma.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Puriya Dhanashree\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"S\", \"r1\", \"G2\", \"M2\", \"P\", \"D2\", \"N2\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"N2\", \"D2\", \"P\", \"M2\", \"G2\", \"r1\", \"S\"]\n",
        "        },\n",
        "        \"Istern_scale\": {\n",
        "            \"aaroh\": [\"C4\", \"Db4\", \"E4\", \"F#4\", \"G4\", \"A4\", \"B4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"B4\", \"A4\", \"G4\", \"F#4\", \"E4\", \"Db4\", \"C4\"]\n",
        "        },\n",
        "        \"vaadi\": \"P\",\n",
        "        \"samvaadi\": \"r1\",\n",
        "        \"typical_phrases\": [\n",
        "            \"S r1 G2 M2 P D2 N2 S'\",\n",
        "            \"r1 G2 M2 P D2 N2 S'\",\n",
        "            \"S' N2 D2 P M2 G2 r1 S\",\n",
        "            \"r1 G2 M2 P\",\n",
        "            \"D2 N2 S'\",\n",
        "            \"P M2 G2 r1 S\",\n",
        "            \"N2 S' r1 G2\",\n",
        "            \"G2 M2 P D2 N2\",\n",
        "            \"r1 S r1 G2\",\n",
        "            \"P D2 N2 S'\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"r1 from S with andolan\",\n",
        "            \"D2 from P with short meend\"\n",
        "        ],\n",
        "        \"skipped_swars\": [],\n",
        "        \"microtonal_nuances\": \"Komal Re, Tivra Ma, strong Pa/Ni usage. Similar scale to Puriya but includes Pa.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Lalit\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"S\", \"r1\", \"G2\", \"M1\", \"M2\", \"D2\", \"N2\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"N2\", \"D2\", \"M2\", \"M1\", \"G2\", \"r1\", \"S\"]\n",
        "        },\n",
        "        \"Istern_scale\": {\n",
        "            \"aaroh\": [\"C4\", \"Db4\", \"E4\", \"F4\", \"F#4\", \"A4\", \"B4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"B4\", \"A4\", \"F#4\", \"F4\", \"E4\", \"Db4\", \"C4\"]\n",
        "        },\n",
        "        \"vaadi\": \"M1/M2\",\n",
        "        \"samvaadi\": \"S\",\n",
        "        \"typical_phrases\": [\n",
        "            \"S r1 G2 M1 M2 D2 N2 S'\",\n",
        "            \"M1 M2 D2 N2 S'\",\n",
        "            \"N2 D2 M2 M1 G2 r1 S\",\n",
        "            \"M2 M1 G2 r1 S\",\n",
        "            \"r1 S N2 D2\",\n",
        "            \"M1 M2 r1 S\",\n",
        "            \"r1 G2 M1 M2\",\n",
        "            \"D2 N2 S'\",\n",
        "            \"G2 r1 S\",\n",
        "            \"M1 M2 D2\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"M2 from M1 with direct slide\",\n",
        "            \"r1 from S with meend\"\n",
        "        ],\n",
        "        \"skipped_swars\": [],\n",
        "        \"microtonal_nuances\": \"Distinct usage of both Ma1 & Ma2 side by side, Komal Re, rest mostly shuddha.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Chandrakauns\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"S\", \"g1\", \"M1\", \"d1\", \"n1\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"n1\", \"d1\", \"M1\", \"g1\", \"S\"]\n",
        "        },\n",
        "        \"Istern_scale\": {\n",
        "            \"aaroh\": [\"C4\", \"Eb4\", \"F4\", \"Ab4\", \"Bb4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"Bb4\", \"Ab4\", \"F4\", \"Eb4\", \"C4\"]\n",
        "        },\n",
        "        \"vaadi\": \"M1\",\n",
        "        \"samvaadi\": \"S\",\n",
        "        \"typical_phrases\": [\n",
        "            \"S g1 M1 d1 n1 S'\",\n",
        "            \"g1 M1 d1 n1 S'\",\n",
        "            \"S' n1 d1 M1 g1 S\",\n",
        "            \"n1 S' g1 M1 S\",\n",
        "            \"d1 n1 S'\",\n",
        "            \"g1 M1 g1 S\",\n",
        "            \"M1 d1 n1 S'\",\n",
        "            \"n1 S' n1 d1\",\n",
        "            \"M1 d1 g1 S\",\n",
        "            \"g1 M1 d1 n1\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"g1 from S with slight andolan\",\n",
        "            \"d1 from M1 smoothly\"\n",
        "        ],\n",
        "        \"skipped_swars\": [\"R2\", \"P\", \"D2\", \"N2\"],\n",
        "        \"microtonal_nuances\": \"Similar to Malkauns but a variant approach. Komal Ga, Dha, Ni with oscillation.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Jog\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"S\", \"G2\", \"M1\", \"P\", \"n1\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"n1\", \"P\", \"M1\", \"G2\", \"S\"]\n",
        "        },\n",
        "        \"Istern_scale\": {\n",
        "            \"aaroh\": [\"C4\", \"E4\", \"F4\", \"G4\", \"Bb4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"Bb4\", \"G4\", \"F4\", \"E4\", \"C4\"]\n",
        "        },\n",
        "        \"vaadi\": \"G2\",\n",
        "        \"samvaadi\": \"S\",\n",
        "        \"typical_phrases\": [\n",
        "            \"S G2 M1 P n1 S'\",\n",
        "            \"G2 M1 P n1 S'\",\n",
        "            \"S' n1 P M1 G2 S\",\n",
        "            \"n1 S' G2 M1 S\",\n",
        "            \"P n1 S'\",\n",
        "            \"M1 G2 S\",\n",
        "            \"G2 M1 P\",\n",
        "            \"n1 S' n1\",\n",
        "            \"M1 P n1 S'\",\n",
        "            \"G2 M1 G2 S\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"G2 from S directly\",\n",
        "            \"n1 from P with a small meend\"\n",
        "        ],\n",
        "        \"skipped_swars\": [\"R2\", \"D2\", \"N2\"],\n",
        "        \"microtonal_nuances\": \"Hybrid scale, Komal Ni with rest mostly shuddha. Subtle meends from Pa to Ni.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Ahir Bhairav\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"S\", \"r1\", \"G2\", \"M1\", \"P\", \"N2\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"N2\", \"d1\", \"P\", \"M1\", \"G2\", \"r1\", \"S\"]\n",
        "        },\n",
        "        \"Istern_scale\": {\n",
        "            \"aaroh\": [\"C4\", \"Db4\", \"E4\", \"F4\", \"G4\", \"B4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"B4\", \"Ab4\", \"G4\", \"F4\", \"E4\", \"Db4\", \"C4\"]\n",
        "        },\n",
        "        \"vaadi\": \"M1\",\n",
        "        \"samvaadi\": \"S\",\n",
        "        \"typical_phrases\": [\n",
        "            \"S r1 G2 M1 P N2 S'\",\n",
        "            \"r1 G2 M1 P N2 S'\",\n",
        "            \"S' N2 d1 P M1 G2 r1 S\",\n",
        "            \"G2 r1 S\",\n",
        "            \"M1 G2 r1 S\",\n",
        "            \"P N2 S'\",\n",
        "            \"N2 S' r1 G2\",\n",
        "            \"d1 P M1 G2\",\n",
        "            \"r1 G2 M1 P\",\n",
        "            \"S' N2 d1 P\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"r1 from S with slow approach\",\n",
        "            \"N2 from P partial andolan\"\n",
        "        ],\n",
        "        \"skipped_swars\": [\"D2\"],\n",
        "        \"microtonal_nuances\": \"Mix of Bhairav & Kafi flavors. Komal Re, Shuddha Ga, Komal Dha in avroh.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Madhuvanti\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"S\", \"R2\", \"G2\", \"M2\", \"P\", \"N2\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"N2\", \"P\", \"M2\", \"G2\", \"R2\", \"S\"]\n",
        "        },\n",
        "        \"Istern_scale\": {\n",
        "            \"aaroh\": [\"C4\", \"D4\", \"E4\", \"F#4\", \"G4\", \"B4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"B4\", \"G4\", \"F#4\", \"E4\", \"D4\", \"C4\"]\n",
        "        },\n",
        "        \"vaadi\": \"R2\",\n",
        "        \"samvaadi\": \"P\",\n",
        "        \"typical_phrases\": [\n",
        "            \"S R2 G2 M2 P N2 S'\",\n",
        "            \"R2 G2 M2 P N2 S'\",\n",
        "            \"S' N2 P M2 G2 R2 S\",\n",
        "            \"N2 P M2 G2 R2 S\",\n",
        "            \"G2 M2 R2 S\",\n",
        "            \"R2 S N2 P\",\n",
        "            \"M2 G2 R2 S\",\n",
        "            \"P N2 S'\",\n",
        "            \"R2 G2 M2\",\n",
        "            \"N2 P R2 S\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"R2 from S with slight meend\",\n",
        "            \"M2 from G2 carefully\"\n",
        "        ],\n",
        "        \"skipped_swars\": [\"D2\"],\n",
        "        \"microtonal_nuances\": \"Tivra Ma, bright mood, subtle meends on Ga-Ma.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Saraswati\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"S\", \"R2\", \"M2\", \"P\", \"N2\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"N2\", \"P\", \"M2\", \"R2\", \"S\"]\n",
        "        },\n",
        "        \"Istern_scale\": {\n",
        "            \"aaroh\": [\"C4\", \"D4\", \"F#4\", \"G4\", \"B4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"B4\", \"G4\", \"F#4\", \"D4\", \"C4\"]\n",
        "        },\n",
        "        \"vaadi\": \"R2\",\n",
        "        \"samvaadi\": \"P\",\n",
        "        \"typical_phrases\": [\n",
        "            \"S R2 M2 P N2 S'\",\n",
        "            \"R2 M2 P N2 S'\",\n",
        "            \"S' N2 P M2 R2 S\",\n",
        "            \"M2 P N2 S'\",\n",
        "            \"R2 S N2 P\",\n",
        "            \"M2 R2 S\",\n",
        "            \"P N2 S'\",\n",
        "            \"N2 P M2 R2\",\n",
        "            \"R2 M2 P\",\n",
        "            \"S R2 M2\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"R2 from S quickly\",\n",
        "            \"M2 from R2\"\n",
        "        ],\n",
        "        \"skipped_swars\": [\"G2\", \"D2\"],\n",
        "        \"microtonal_nuances\": \"Tivra Ma, bright approach, minimal microtonal glides.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Basant\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"S\", \"r1\", \"M2\", \"P\", \"N2\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"N2\", \"P\", \"M2\", \"r1\", \"S\"]\n",
        "        },\n",
        "        \"Istern_scale\": {\n",
        "            \"aaroh\": [\"C4\", \"Db4\", \"F#4\", \"G4\", \"B4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"B4\", \"G4\", \"F#4\", \"Db4\", \"C4\"]\n",
        "        },\n",
        "        \"vaadi\": \"r1\",\n",
        "        \"samvaadi\": \"P\",\n",
        "        \"typical_phrases\": [\n",
        "            \"S r1 M2 P N2 S'\",\n",
        "            \"r1 M2 P N2 S'\",\n",
        "            \"S' N2 P M2 r1 S\",\n",
        "            \"M2 r1 S\",\n",
        "            \"r1 M2 P\",\n",
        "            \"N2 S' r1\",\n",
        "            \"M2 r1 M2 P\",\n",
        "            \"r1 S N2 P\",\n",
        "            \"r1 S r1 M2\",\n",
        "            \"N2 P r1 S\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"r1 from S with a meend\",\n",
        "            \"N2 from P directly\"\n",
        "        ],\n",
        "        \"skipped_swars\": [\"G2\", \"D2\"],\n",
        "        \"microtonal_nuances\": \"Komal Re, Tivra Ma. Associated with spring. Glides from Re to Ma.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Pilu\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"S\", \"R2\", \"G2\", \"M1\", \"P\", \"D2\", \"N2\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"N2\", \"D2\", \"P\", \"M1\", \"G2\", \"R2\", \"S\"]\n",
        "        },\n",
        "        \"Istern_scale\": {\n",
        "            \"aaroh\": [\"C4\", \"D4\", \"E4\", \"F4\", \"G4\", \"A4\", \"B4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"B4\", \"A4\", \"G4\", \"F4\", \"E4\", \"D4\", \"C4\"]\n",
        "        },\n",
        "        \"vaadi\": \"G2\",\n",
        "        \"samvaadi\": \"N2\",\n",
        "        \"typical_phrases\": [\n",
        "            \"S R2 G2 M1 P D2 N2 S'\",\n",
        "            \"R2 G2 M1 P D2 N2 S'\",\n",
        "            \"S' N2 D2 P M1 G2 R2 S\",\n",
        "            \"G2 M1 R2 S\",\n",
        "            \"R2 S N2 D2 P\",\n",
        "            \"P D2 N2 S'\",\n",
        "            \"S' N2 D2 P\",\n",
        "            \"G2 M1 G2 R2 S\",\n",
        "            \"D2 P M1 G2\",\n",
        "            \"R2 S D2 P\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"R2 from S\",\n",
        "            \"G2 from R2\"\n",
        "        ],\n",
        "        \"skipped_swars\": [],\n",
        "        \"microtonal_nuances\": \"Light classical usage, often allows thumri-style slides. Ga, Ni might vary slightly.\"\n",
        "    },\n",
        "    ################\n",
        "    #  RAGA #31\n",
        "    ################\n",
        "    {\n",
        "        \"name\": \"Amritvarshini\",\n",
        "        \"indian_scale\": {\n",
        "            \"aaroh\": [\"S\", \"G2\", \"M2\", \"P\", \"N3\", \"S'\"],\n",
        "            \"avroh\": [\"S'\", \"N3\", \"P\", \"M2\", \"G2\", \"S\"]\n",
        "        },\n",
        "        \"Istern_scale\": {\n",
        "            \"aaroh\": [\"C4\", \"E4\", \"F#4\", \"G4\", \"B4\", \"C5\"],\n",
        "            \"avroh\": [\"C5\", \"B4\", \"G4\", \"F#4\", \"E4\", \"C4\"]\n",
        "        },\n",
        "        \"vaadi\": \"M2\",\n",
        "        \"samvaadi\": \"S\",\n",
        "        \"typical_phrases\": [\n",
        "            \"S G2 M2 P N3 S'\",\n",
        "            \"G2 M2 P\",\n",
        "            \"M2 P N3 S'\",\n",
        "            \"N3 P M2 G2 S\",\n",
        "            \"G2 M2 G2 S\",\n",
        "            \"P N3 S'\",\n",
        "            \"S' N3 P M2\",\n",
        "            \"M2 G2 S\",\n",
        "            \"G2 M2 P N3\",\n",
        "            \"N3 P G2 S\"\n",
        "        ],\n",
        "        \"approach_notes\": [\n",
        "            \"G2 from S with a slight meend\",\n",
        "            \"M2 from G2 with a direct approach\"\n",
        "        ],\n",
        "        \"skipped_swars\": [\"R2\", \"D2\", \"r1\", \"d1\"],\n",
        "        \"microtonal_nuances\": \"Carnatic-origin raga. Typically bright scale with Tivra Ma; minimal loIr swaras.\"\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13e2539e",
      "metadata": {},
      "source": [
        "## Detect Raga from Notes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d7d5d06",
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def normalize_note(note):\n",
        "    \"\"\"\n",
        "    Convert a note into a canonical representation.\n",
        "\n",
        "    - If the note is already in Indian notation (e.g., S, G2, M2, etc.), return it unchanged.\n",
        "    - If it is in Istern notation (e.g., \"C4\", \"F#4\"), remove the octave and map it to an Indian equivalent.\n",
        "    \n",
        "    Adjust the mapping dictionary as needed.\n",
        "    \"\"\"\n",
        "    indian_notes = {\"S\", \"R1\", \"R2\", \"R3\", \"G1\", \"G2\", \"G3\", \"M1\", \"M2\", \"P\", \"D1\", \"D2\", \"D3\", \"N1\", \"N2\", \"N3\"}\n",
        "    if note in indian_notes or note == \"S'\":  # allow S' for the upper tonic\n",
        "        return note\n",
        "\n",
        "    match = re.match(r\"^([A-G][#b]?)(\\d+)$\", note)\n",
        "    if match:\n",
        "        note_letter = match.group(1)\n",
        "        # Example mapping; adjust according to your tonic and definitions.\n",
        "        Istern_to_indian = {\n",
        "            \"C\": \"S\",\n",
        "            \"C#\": \"R1\",\n",
        "            \"D\": \"R2\",\n",
        "            \"D#\": \"G1\",\n",
        "            \"E\": \"G2\",\n",
        "            \"F\": \"M1\",\n",
        "            \"F#\": \"M2\",\n",
        "            \"G\": \"P\",\n",
        "            \"G#\": \"D1\",\n",
        "            \"A\": \"D2\",\n",
        "            \"A#\": \"N1\",\n",
        "            \"B\": \"N3\"\n",
        "        }\n",
        "        return Istern_to_indian.get(note_letter, note)\n",
        "    \n",
        "    return note\n",
        "\n",
        "def detect_raga_from_notes(detected_notes, raga_db):\n",
        "    \"\"\"\n",
        "    Compare the singer's note usage (detected_notes) to each raga in raga_db.\n",
        "    Uses note normalization so that both Indian and Istern note names are handled.\n",
        "    Returns a dict with:\n",
        "       - 'best_raga': the highest-scoring raga,\n",
        "       - 'explanation': explanation for the best raga,\n",
        "       - 'top_ragas': a list of the top three scored ragas,\n",
        "       - 'all_scores': a list of all raga scores.\n",
        "    \"\"\"\n",
        "    # Normalize detected notes.\n",
        "    normalized_notes = [normalize_note(n) for n in detected_notes]\n",
        "    note_counter = Counter(normalized_notes)\n",
        "    total_notes = sum(note_counter.values()) if note_counter else 1\n",
        "    most_common_note, _ = note_counter.most_common(1)[0] if note_counter else (\"?\", 0)\n",
        "    \n",
        "    # Create a space-delimited string for contiguous phrase matching.\n",
        "    detected_sequence = \" \".join(normalized_notes)\n",
        "    \n",
        "    results = []\n",
        "    for raga in raga_db:\n",
        "        raga_name = raga[\"name\"]\n",
        "\n",
        "        # Normalize scale notes for the raga.\n",
        "        indian_aaroh = {normalize_note(n) for n in raga[\"indian_scale\"][\"aaroh\"]}\n",
        "        indian_avroh = {normalize_note(n) for n in raga[\"indian_scale\"][\"avroh\"]}\n",
        "        Istern_aaroh = {normalize_note(n) for n in raga[\"Istern_scale\"][\"aaroh\"]}\n",
        "        Istern_avroh = {normalize_note(n) for n in raga[\"Istern_scale\"][\"avroh\"]}\n",
        "        combined_scale = indian_aaroh.union(indian_avroh, Istern_aaroh, Istern_avroh)\n",
        "\n",
        "        # Normalize vaadi and samvaadi, if provided.\n",
        "        vaadi = normalize_note(raga.get(\"vaadi\", \"\"))\n",
        "        samvaadi = normalize_note(raga.get(\"samvaadi\", \"\"))\n",
        "        typical_phrases = raga.get(\"typical_phrases\", [])\n",
        "\n",
        "        # (A) Coverage: Fraction of detected notes that lie in the raga's scale.\n",
        "        in_scale_count = sum(count for note, count in note_counter.items() if note in combined_scale)\n",
        "        coverage = in_scale_count / total_notes\n",
        "\n",
        "        # (B) Vaadi/Samvaadi bonus.\n",
        "        vaadi_bonus = 0.0\n",
        "        reason_list = []\n",
        "        if most_common_note == vaadi:\n",
        "            vaadi_bonus += 0.2\n",
        "            reason_list.append(f\"Most used note matches vaadi={vaadi}.\")\n",
        "        if most_common_note == samvaadi:\n",
        "            vaadi_bonus += 0.1\n",
        "            reason_list.append(f\"Most used note matches samvaadi={samvaadi}.\")\n",
        "\n",
        "        # (C) Phrase bonus: Check for contiguous appearance of each normalized phrase.\n",
        "        phrase_bonus = 0.0\n",
        "        phrase_hits = 0\n",
        "        for phrase in typical_phrases:\n",
        "            # Normalize the phrase notes and join into a string.\n",
        "            phrase_norm = \" \".join(normalize_note(n) for n in phrase.split() if n)\n",
        "            # Check if the normalized phrase is a contiguous subsequence of the detected sequence.\n",
        "            if phrase_norm in detected_sequence:\n",
        "                phrase_bonus += 0.03  # Reduced bonus Iight; adjust as needed.\n",
        "                phrase_hits += 1\n",
        "\n",
        "        match_score = coverage + vaadi_bonus + phrase_bonus\n",
        "        explanation = (f\"Coverage={coverage:.2f}, Vaadi bonus={vaadi_bonus:.2f}, \"\n",
        "                       f\"Phrase bonus={phrase_bonus:.2f} (matched {phrase_hits} phrases).\")\n",
        "        if reason_list:\n",
        "            explanation += \" | \" + \" \".join(reason_list)\n",
        "\n",
        "        results.append((raga_name, match_score, explanation))\n",
        "\n",
        "    results_sorted = sorted(results, key=lambda x: x[1], reverse=True)\n",
        "    \n",
        "    if not results_sorted:\n",
        "        return {\n",
        "            \"best_raga\": \"Unknown\",\n",
        "            \"explanation\": \"No ragas in the database or no notes detected.\",\n",
        "            \"top_ragas\": [],\n",
        "            \"all_scores\": []\n",
        "        }\n",
        "    \n",
        "    # Prepare the top three ragas (if available)\n",
        "    top_three = results_sorted[:3]\n",
        "    best_raga, best_score, best_explanation = results_sorted[0]\n",
        "    \n",
        "    return {\n",
        "        \"best_raga\": best_raga,\n",
        "        \"explanation\": best_explanation,  # Explanation for the best raga.\n",
        "        \"top_ragas\": [\n",
        "            {\"raga\": r[0], \"score\": r[1], \"explanation\": r[2]} for r in top_three\n",
        "        ],\n",
        "        \"all_scores\": [\n",
        "            {\"raga\": r[0], \"score\": r[1], \"explanation\": r[2]} for r in results_sorted\n",
        "        ]\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mod11-plot",
      "metadata": {},
      "source": [
        "## 11. Plotting All Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "plot-all-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "#def plot_sruti_overlay(ax, times_small, pitch_small, sruti_labels):\n",
        "#    \"\"\"\n",
        "#    Overlays a scatter plot on 'ax' using times vs. pitch, colored by sruti index.\n",
        "#    I'll parse the 'sruti_XX' from each label, if possible, and color them with a\n",
        "#    discrete colormap of 22 steps.\n",
        "#    \"\"\"\n",
        "#    import re\n",
        "#    from matplotlib import colormaps\n",
        "\n",
        "    # 1) Grab the base 'hsv' colormap\n",
        "#    base_cmap = colormaps[\"hsv\"]\n",
        "    # 2) Resample to 22 discrete levels\n",
        "#    cmap = base_cmap.resampled(22)\n",
        "\n",
        "#    sruti_indices = []\n",
        "#    for lbl in sruti_labels:\n",
        "#        match = re.search(r\"sruti_(\\d+)\", lbl)\n",
        "#        if match:\n",
        "#            sruti_indices.append(int(match.group(1)))\n",
        "#        else:\n",
        "            # default to 0 if unknown\n",
        "#            sruti_indices.append(0)\n",
        "\n",
        "#    sc = ax.scatter(\n",
        "#        times_small, pitch_small,\n",
        "#        c=sruti_indices, cmap=cmap,\n",
        "#        alpha=0.8, s=10, edgecolors='none'\n",
        "#    )\n",
        "#    ax.figure.colorbar(sc, ax=ax, label='22 Śruti Index (0..21)')\n",
        "#    ax.set_title(\"Pitch with 22-Sruti Classification\")\n",
        "#    ax.set_xlabel(\"Time (s)\")\n",
        "#    ax.set_ylabel(\"Pitch (Hz)\")\n",
        "\n",
        "\n",
        "def plot_sruti_overlay(ax, times_small, pitch_small, sruti_labels):\n",
        "    import re\n",
        "    sruti_indices = []\n",
        "    for lbl in sruti_labels:\n",
        "        # If the label is None, replace it with a default value.\n",
        "        if lbl is None:\n",
        "            lbl = \"sruti_unknown\"\n",
        "        # Attempt to extract the numeric portion.\n",
        "        match = re.search(r\"sruti_(\\d+)\", lbl)\n",
        "        if match:\n",
        "            sruti_indices.append(int(match.group(1)))\n",
        "        else:\n",
        "            # If the label doesn't match the expected format, default to 0.\n",
        "            sruti_indices.append(0)\n",
        "    # Continue with your plotting code, using sruti_indices as needed.\n",
        "    # For example, you might overlay these indices on your pitch plot.\n",
        "    ax.plot(times_small, pitch_small, label=\"Pitch contour\")\n",
        "    ax.scatter(times_small, sruti_indices, color='red', label=\"Sruti index\")\n",
        "    ax.set_xlabel(\"Time (s)\")\n",
        "    ax.set_ylabel(\"Pitch / Sruti Index\")\n",
        "    ax.legend()\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Define a constant for the output directory (instead of hard-coding it inline).\n",
        "OUTPUT_DIR = \"data/analysisoutput\"\n",
        "\n",
        "def plot_all_metrics(\n",
        "    time_matrix_small,\n",
        "    praat_matrix,\n",
        "    audio_data,\n",
        "    sr,\n",
        "    time_matrix_tempo_medium,\n",
        "    time_matrix_tempo_large,\n",
        "    aggregated_items,\n",
        "    analysis_dict,\n",
        "    input_filename\n",
        "):\n",
        "    \"\"\"\n",
        "    Plot each metric in its own figure and overwrite existing image files if they exist.\n",
        "    The file outputs will start with the base name derived from `input_filename`.\n",
        "    For MFCC, I remove \"60\" from both the graph title and the file name.\n",
        "    \"\"\"\n",
        "\n",
        "    import os\n",
        "    import re\n",
        "    import librosa\n",
        "    import librosa.display\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    # Overwrite behavior (no timestamps)\n",
        "    OUTPUT_DIR = \"data/analysisoutput\"\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "    # Use the actual base name from input_filename\n",
        "    base_name = os.path.splitext(os.path.basename(input_filename))[0]\n",
        "\n",
        "    #------------------------------------------------------------\n",
        "    # Grab data from time_matrix_small\n",
        "    times_small = [row.get('time_s', 0.0) for row in time_matrix_small]\n",
        "    pitch_small = [row.get('pitch_hz', 0.0) for row in time_matrix_small]\n",
        "    note_freq_small = [row.get('note_freq_hz', 0.0) for row in time_matrix_small]\n",
        "\n",
        "    tone_vals = [row.get('tone_to_noise', 0.0) for row in time_matrix_small]\n",
        "    trans_vals = [row.get('transition_score', 0.0) for row in time_matrix_small]\n",
        "\n",
        "    # Praat\n",
        "    praat_times = [row.get('start_time_s', 0.0) for row in praat_matrix]\n",
        "    praat_hnr   = [row.get('praat_hnr', 0.0) for row in praat_matrix]\n",
        "\n",
        "    # Sruti\n",
        "    sruti_labels = [row.get('sruti_class', 'sruti_unknown') for row in time_matrix_small]\n",
        "    sruti_indices = []\n",
        "    for lbl in sruti_labels:\n",
        "        if lbl is None:\n",
        "            lbl = \"sruti_unknown\"\n",
        "        match = re.search(r\"sruti_(\\d+)\", lbl)\n",
        "        if match:\n",
        "            sruti_indices.append(int(match.group(1)))\n",
        "        else:\n",
        "            sruti_indices.append(0)\n",
        "\n",
        "    # chunk-based lines for advanced features from time_matrix_small\n",
        "    zcr_vals   = [row.get('zcr', 0.0) for row in time_matrix_small]\n",
        "    cent_vals  = [row.get('spec_centroid', 0.0) for row in time_matrix_small]\n",
        "    roll_vals  = [row.get('spec_rolloff', 0.0) for row in time_matrix_small]\n",
        "    bw_vals    = [row.get('spec_bandwidth', 0.0) for row in time_matrix_small]\n",
        "    rms_vals   = [row.get('rms_db', 0.0) for row in time_matrix_small]\n",
        "    lufs_vals  = [row.get('lufs', 0.0) for row in time_matrix_small]\n",
        "    tempo_vals_small = [row.get('tempo_bpm', 0.0) for row in time_matrix_small]\n",
        "\n",
        "    # For tempo medium/large\n",
        "    if time_matrix_tempo_medium is not None:\n",
        "        med_times = [row.get('start_time_s', 0.0) for row in time_matrix_tempo_medium]\n",
        "        med_bpm   = [row.get('tempo_bpm', 0.0) for row in time_matrix_tempo_medium]\n",
        "    else:\n",
        "        med_times, med_bpm = [], []\n",
        "\n",
        "    if time_matrix_tempo_large is not None:\n",
        "        large_times = [row.get('start_time_s', 0.0) for row in time_matrix_tempo_large]\n",
        "        large_bpm   = [row.get('tempo_bpm', 0.0) for row in time_matrix_tempo_large]\n",
        "    else:\n",
        "        large_times, large_bpm = [], []\n",
        "\n",
        "    # Advanced vocal features\n",
        "    jitters   = [row.get(\"jitter\", 0.0) for row in time_matrix_small]\n",
        "    shimmers  = [row.get(\"shimmer\", 0.0) for row in time_matrix_small]\n",
        "    formant_F1= [row.get(\"formants\", {}).get(\"F1\", 0.0) for row in time_matrix_small]\n",
        "    formant_F2= [row.get(\"formants\", {}).get(\"F2\", 0.0) for row in time_matrix_small]\n",
        "    formant_F3= [row.get(\"formants\", {}).get(\"F3\", 0.0) for row in time_matrix_small]\n",
        "    vib_extents= [row.get(\"vibrato_extent\", 0.0) for row in time_matrix_small]\n",
        "    vib_rates = [row.get(\"vibrato_rate\", 0.0) for row in time_matrix_small]\n",
        "\n",
        "    #------------------------------------------------------------\n",
        "    # 1) Pitch & Note Frequency\n",
        "    plt.figure()\n",
        "    plt.plot(times_small, pitch_small, label='Pitch (Hz)', color='b')\n",
        "    plt.plot(times_small, note_freq_small, label='NoteFreq (Hz)', color='orange', alpha=0.7)\n",
        "    plt.ylabel('Frequency (Hz)')\n",
        "    plt.title('Pitch & Note Frequency')\n",
        "    plt.legend()\n",
        "    pitch_filename = os.path.join(OUTPUT_DIR, f\"{base_name}_pitch_note_freq.png\")\n",
        "    plt.savefig(pitch_filename)\n",
        "    #plt.show()\n",
        "    plt.close()\n",
        "\n",
        "    #------------------------------------------------------------\n",
        "    # 2) Spectral Flatness & Transition Score\n",
        "    plt.figure()\n",
        "    plt.plot(times_small, tone_vals, label='Spectral Flatness', color='g')\n",
        "    plt.plot(times_small, trans_vals, label='Transition Score', color='r')\n",
        "    plt.ylabel('Arbitrary scale')\n",
        "    plt.title('Spectral Flatness & Transition Score')\n",
        "    plt.ylim([0,1])\n",
        "    plt.legend()\n",
        "    spectral_flat_filename = os.path.join(OUTPUT_DIR, f\"{base_name}_spectral_flatness_transition.png\")\n",
        "    plt.savefig(spectral_flat_filename)\n",
        "    #plt.show()\n",
        "    plt.close()\n",
        "\n",
        "    #------------------------------------------------------------\n",
        "    # 3) Praat HNR\n",
        "    plt.figure()\n",
        "    plt.plot(praat_times, praat_hnr, label='Praat HNR (dB)', color='purple')\n",
        "    plt.xlabel('Time (s)')\n",
        "    plt.ylabel('HNR (dB)')\n",
        "    plt.title('Praat HNR (2048-chunk)')\n",
        "    plt.ylim([0,30])\n",
        "    plt.legend()\n",
        "    praat_filename = os.path.join(OUTPUT_DIR, f\"{base_name}_praat_hnr.png\")\n",
        "    plt.savefig(praat_filename)\n",
        "    #plt.show()\n",
        "    plt.close()\n",
        "\n",
        "    #------------------------------------------------------------\n",
        "    # Sruti Overlay\n",
        "    plt.figure()\n",
        "    plt.plot(times_small, pitch_small, label=\"Pitch contour\")\n",
        "    plt.scatter(times_small, sruti_indices, color='red', label=\"Sruti index\")\n",
        "    plt.xlabel(\"Time (s)\")\n",
        "    plt.ylabel(\"Pitch / Sruti Index\")\n",
        "    plt.legend()\n",
        "    plt.title(\"22-Sruti Overlay\")\n",
        "    sruti_overlay_filename = os.path.join(OUTPUT_DIR, f\"{base_name}_sruti_overlay.png\")\n",
        "    plt.savefig(sruti_overlay_filename)\n",
        "    #plt.show()\n",
        "    plt.close()\n",
        "\n",
        "    #------------------------------------------------------------\n",
        "    # If I have audio_data & sr => advanced spectrogram features\n",
        "    if audio_data is not None and sr is not None:\n",
        "        from librosa import stft, amplitude_to_db\n",
        "        n_fft_local = 2048\n",
        "        D = stft(y=audio_data, n_fft=n_fft_local)\n",
        "        D_db = amplitude_to_db(np.abs(D), ref=np.max)\n",
        "\n",
        "        plt.figure()\n",
        "        librosa.display.specshow(D_db, sr=sr, hop_length=n_fft_local//4, x_axis='time', y_axis='log')\n",
        "        plt.title('Log-frequency Spectrogram (Advanced)')\n",
        "        plt.colorbar(format='%+2.0f dB')\n",
        "        spec_filename = os.path.join(OUTPUT_DIR, f\"{base_name}_log_spectrogram.png\")\n",
        "        plt.savefig(spec_filename)\n",
        "        #plt.show()\n",
        "        plt.close()\n",
        "\n",
        "        # MFCC (remove \"60\")\n",
        "        mfcc_data = librosa.feature.mfcc(y=audio_data, sr=sr, n_mfcc=60, n_fft=n_fft_local)\n",
        "        plt.figure()\n",
        "        librosa.display.specshow(mfcc_data, sr=sr, x_axis='time')\n",
        "        plt.title('MFCC')\n",
        "        plt.colorbar()\n",
        "        mfcc_filename = os.path.join(OUTPUT_DIR, f\"{base_name}_mfcc.png\")\n",
        "        plt.savefig(mfcc_filename)\n",
        "        #plt.show()\n",
        "        plt.close()\n",
        "\n",
        "        # Chromagram\n",
        "        chroma_data = librosa.feature.chroma_stft(y=audio_data, sr=sr, n_fft=n_fft_local)\n",
        "        plt.figure()\n",
        "        librosa.display.specshow(chroma_data, sr=sr, x_axis='time', y_axis='chroma')\n",
        "        plt.title('Chromagram')\n",
        "        plt.colorbar()\n",
        "        chroma_filename = os.path.join(OUTPUT_DIR, f\"{base_name}_chromagram.png\")\n",
        "        plt.savefig(chroma_filename)\n",
        "        #plt.show()\n",
        "        plt.close()\n",
        "\n",
        "        # chunk-based lines for advanced features\n",
        "\n",
        "        # ZCR\n",
        "        plt.figure()\n",
        "        plt.plot(times_small, zcr_vals, 'b', label='Zero Crossing Rate')\n",
        "        plt.title('Zero Crossing Rate')\n",
        "        plt.ylabel('ZCR')\n",
        "        plt.legend()\n",
        "        zcr_filename = os.path.join(OUTPUT_DIR, f\"{base_name}_zcr.png\")\n",
        "        plt.savefig(zcr_filename)\n",
        "        #plt.show()\n",
        "        plt.close()\n",
        "\n",
        "        # Spectral Centroid\n",
        "        plt.figure()\n",
        "        plt.plot(times_small, cent_vals, 'g', label='Spectral Centroid')\n",
        "        plt.title('Spectral Centroid')\n",
        "        plt.ylabel('Hz')\n",
        "        plt.legend()\n",
        "        centroid_filename = os.path.join(OUTPUT_DIR, f\"{base_name}_spectral_centroid.png\")\n",
        "        plt.savefig(centroid_filename)\n",
        "        #plt.show()\n",
        "        plt.close()\n",
        "\n",
        "        # Rolloff\n",
        "        plt.figure()\n",
        "        plt.plot(times_small, roll_vals, 'r', label='Spectral Rolloff')\n",
        "        plt.title('Spectral Rolloff')\n",
        "        plt.ylabel('Hz')\n",
        "        plt.legend()\n",
        "        rolloff_filename = os.path.join(OUTPUT_DIR, f\"{base_name}_spectral_rolloff.png\")\n",
        "        plt.savefig(rolloff_filename)\n",
        "        #plt.show()\n",
        "        plt.close()\n",
        "\n",
        "        # Bandwidth\n",
        "        plt.figure()\n",
        "        plt.plot(times_small, bw_vals, 'm', label='Spectral Bandwidth')\n",
        "        plt.title('Spectral Bandwidth')\n",
        "        plt.ylabel('Hz')\n",
        "        plt.legend()\n",
        "        bandwidth_filename = os.path.join(OUTPUT_DIR, f\"{base_name}_spectral_bandwidth.png\")\n",
        "        plt.savefig(bandwidth_filename)\n",
        "        #plt.show()\n",
        "        plt.close()\n",
        "\n",
        "        # RMS(dB)\n",
        "        plt.figure()\n",
        "        plt.plot(times_small, rms_vals, 'orange', label='RMS(dB)')\n",
        "        plt.title('RMS(dB) with LUF_CHUNK_SIZE')\n",
        "        plt.ylabel('dB')\n",
        "        plt.legend()\n",
        "        rms_filename = os.path.join(OUTPUT_DIR, f\"{base_name}_rms_db.png\")\n",
        "        plt.savefig(rms_filename)\n",
        "        #plt.show()\n",
        "        plt.close()\n",
        "\n",
        "        # LUFS\n",
        "        plt.figure()\n",
        "        plt.plot(times_small, lufs_vals, 'c', label='LUFS')\n",
        "        plt.title('LUFS with LUF_CHUNK_SIZE')\n",
        "        plt.ylabel('LUFS')\n",
        "        plt.legend()\n",
        "        lufs_filename = os.path.join(OUTPUT_DIR, f\"{base_name}_lufs.png\")\n",
        "        plt.savefig(lufs_filename)\n",
        "        #plt.show()\n",
        "        plt.close()\n",
        "\n",
        "        # Tempo (512)\n",
        "        plt.figure()\n",
        "        plt.plot(times_small, tempo_vals_small, 'k', label='Small-chunk Tempo (512)')\n",
        "        plt.title('Small-chunk Tempo (512)')\n",
        "        plt.ylabel('BPM')\n",
        "        plt.legend()\n",
        "        tempo_small_filename = os.path.join(OUTPUT_DIR, f\"{base_name}_tempo_small.png\")\n",
        "        plt.savefig(tempo_small_filename)\n",
        "        #plt.show()\n",
        "        plt.close()\n",
        "\n",
        "        # Medium & Large Tempo\n",
        "        plt.figure()\n",
        "        if med_times and med_bpm:\n",
        "            plt.plot(med_times, med_bpm, 'y', label='Medium-chunk Tempo (4096)')\n",
        "        if large_times and large_bpm:\n",
        "            plt.plot(large_times, large_bpm, 'r', label='Large-chunk Tempo (16384)')\n",
        "        plt.title('Tempo (4096 & 16384)')\n",
        "        plt.ylabel('BPM')\n",
        "        plt.xlabel('Time (s)')\n",
        "        plt.legend()\n",
        "        tempo_big_filename = os.path.join(OUTPUT_DIR, f\"{base_name}_tempo_big.png\")\n",
        "        plt.savefig(tempo_big_filename)\n",
        "        #plt.show()\n",
        "        plt.close()\n",
        "\n",
        "    #------------------------------------------------------------\n",
        "    # 4) Advanced vocal features figure (split subplots as Ill)\n",
        "\n",
        "    # Jitter\n",
        "    plt.figure()\n",
        "    plt.plot(times_small, jitters, marker='o', linestyle='-', color='blue', label='Jitter')\n",
        "    plt.title(\"Jitter (local) over Time\")\n",
        "    plt.xlabel(\"Time (s)\")\n",
        "    plt.ylabel(\"Jitter\")\n",
        "    plt.legend()\n",
        "    jitter_filename = os.path.join(OUTPUT_DIR, f\"{base_name}_jitter.png\")\n",
        "    plt.savefig(jitter_filename)\n",
        "    #plt.show()\n",
        "    plt.close()\n",
        "\n",
        "    # Shimmer\n",
        "    plt.figure()\n",
        "    plt.plot(times_small, shimmers, marker='o', linestyle='-', color='orange', label='Shimmer')\n",
        "    plt.title(\"Shimmer (local) over Time\")\n",
        "    plt.xlabel(\"Time (s)\")\n",
        "    plt.ylabel(\"Shimmer\")\n",
        "    plt.legend()\n",
        "    shimmer_filename = os.path.join(OUTPUT_DIR, f\"{base_name}_shimmer.png\")\n",
        "    plt.savefig(shimmer_filename)\n",
        "    #plt.show()\n",
        "    plt.close()\n",
        "\n",
        "    # Formants\n",
        "    plt.figure()\n",
        "    plt.plot(times_small, formant_F1, marker='o', linestyle='-', color='green',  label='F1')\n",
        "    plt.plot(times_small, formant_F2, marker='o', linestyle='-', color='red',    label='F2')\n",
        "    plt.plot(times_small, formant_F3, marker='o', linestyle='-', color='purple', label='F3')\n",
        "    plt.title(\"Formant Frequencies over Time\")\n",
        "    plt.xlabel(\"Time (s)\")\n",
        "    plt.ylabel(\"Frequency (Hz)\")\n",
        "    plt.legend()\n",
        "    formant_filename = os.path.join(OUTPUT_DIR, f\"{base_name}_formants.png\")\n",
        "    plt.savefig(formant_filename)\n",
        "    #plt.show()\n",
        "    plt.close()\n",
        "\n",
        "    # Vibrato Extent\n",
        "    plt.figure()\n",
        "    plt.plot(times_small, vib_extents, marker='o', linestyle='-', color='magenta', label='Vibrato Extent')\n",
        "    plt.title(\"Vibrato Extent over Time\")\n",
        "    plt.xlabel(\"Time (s)\")\n",
        "    plt.ylabel(\"Vibrato Extent (Hz)\")\n",
        "    plt.legend()\n",
        "    vib_ext_filename = os.path.join(OUTPUT_DIR, f\"{base_name}_vibrato_extent.png\")\n",
        "    plt.savefig(vib_ext_filename)\n",
        "    #plt.show()\n",
        "    plt.close()\n",
        "\n",
        "    # Vibrato Rate\n",
        "    plt.figure()\n",
        "    plt.plot(times_small, vib_rates, marker='o', linestyle='-', color='brown', label='Vibrato Rate')\n",
        "    plt.title(\"Vibrato Rate over Time\")\n",
        "    plt.xlabel(\"Time (s)\")\n",
        "    plt.ylabel(\"Vibrato Rate (Hz)\")\n",
        "    plt.legend()\n",
        "    vib_rate_filename = os.path.join(OUTPUT_DIR, f\"{base_name}_vibrato_rate.png\")\n",
        "    plt.savefig(vib_rate_filename)\n",
        "    #plt.show()\n",
        "    plt.close()\n",
        "\n",
        "    # If I have note attack data:\n",
        "    if analysis_dict and \"advanced_note_features\" in analysis_dict:\n",
        "        note_feats = analysis_dict[\"advanced_note_features\"]\n",
        "        note_start_times = []\n",
        "        note_attack_times = []\n",
        "        for note in note_feats:\n",
        "            note_time = note.get(\"time_s\", None)\n",
        "            for detail in note.get(\"asd_details\", []):\n",
        "                attack = detail.get(\"attack\", None)\n",
        "                if note_time is not None and attack is not None:\n",
        "                    note_start_times.append(note_time)\n",
        "                    note_attack_times.append(attack)\n",
        "\n",
        "        plt.figure()\n",
        "        if note_attack_times:\n",
        "            plt.plot(\n",
        "                note_start_times,\n",
        "                note_attack_times,\n",
        "                marker='o',\n",
        "                linestyle='-',\n",
        "                color='cyan',\n",
        "                label=\"Attack Time\"\n",
        "            )\n",
        "            plt.title(\"Note Attack Time over Time\")\n",
        "            plt.xlabel(\"Time (s)\")\n",
        "            plt.ylabel(\"Attack Time (s)\")\n",
        "            plt.legend()\n",
        "        else:\n",
        "            plt.text(0.5, 0.5, \"No Attack Data\", ha='center', va='center')\n",
        "        attack_filename = os.path.join(OUTPUT_DIR, f\"{base_name}_note_attack_time.png\")\n",
        "        plt.savefig(attack_filename)\n",
        "        #plt.show()\n",
        "        plt.close()\n",
        "    else:\n",
        "        plt.figure()\n",
        "        plt.text(0.5, 0.5, \"No Attack Data\", ha='center', va='center')\n",
        "        plt.title(\"Note Attack Time over Time\")\n",
        "        attack_filename = os.path.join(OUTPUT_DIR, f\"{base_name}_note_attack_time.png\")\n",
        "        plt.savefig(attack_filename)\n",
        "        #plt.show()\n",
        "        plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78c862fd",
      "metadata": {},
      "source": [
        "## 11.5: Classification Summaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96385aed",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 14. Classification Summaries (Global/Track-Level Distribution Counts)\n",
        "from collections import Counter\n",
        "\n",
        "def summarize_classifications(\n",
        "    time_matrix_small,\n",
        "    time_matrix_praat,\n",
        "    time_matrix_tempo_medium,\n",
        "    time_matrix_tempo_large\n",
        "):\n",
        "    \"\"\"\n",
        "    Gathers global classification counts across all relevant 'time_matrix' data.\n",
        "    Returns a dictionary, e.g.:\n",
        "\n",
        "    {\n",
        "      \"pitch_accuracy_category\": {\"perfect\": 12, \"good\": 25, ...},\n",
        "      \"tone_to_noise_category\": {...},\n",
        "      \"transition_category\": {...},\n",
        "      \"rms_db_category\": {...},\n",
        "      \"lufs_category\": {...},\n",
        "      \"tempo_bpm_category_small\": {...},\n",
        "      \"tempo_bpm_category_medium\": {...},\n",
        "      \"tempo_bpm_category_large\": {...},\n",
        "      \"praat_hnr_category\": {...}\n",
        "    }\n",
        "\n",
        "    You can adjust or expand these counts as needed.\n",
        "    \"\"\"\n",
        "\n",
        "    # Helper function to safely gather a given field from a list of dictionaries\n",
        "    def gather_categories(matrix, field_name):\n",
        "        # returns e.g. [\"perfect\", \"good\", \"fair\", ...]\n",
        "        cat_list = []\n",
        "        for row in matrix:\n",
        "            val = row.get(field_name, None)\n",
        "            if val is not None:\n",
        "                cat_list.append(val)\n",
        "        return cat_list\n",
        "\n",
        "    # 1) pitch_accuracy_category from time_matrix_small\n",
        "    pitch_categories = gather_categories(time_matrix_small, \"pitch_accuracy_category\")\n",
        "\n",
        "    # 2) tone_to_noise_category from time_matrix_small\n",
        "    tone_categories = gather_categories(time_matrix_small, \"tone_to_noise_category\")\n",
        "\n",
        "    # 3) transition_category from time_matrix_small\n",
        "    transition_categories = gather_categories(time_matrix_small, \"transition_category\")\n",
        "\n",
        "    # 4) RMS & LUFS from time_matrix_small\n",
        "    rms_categories = gather_categories(time_matrix_small, \"rms_db_category\")\n",
        "    lufs_categories = gather_categories(time_matrix_small, \"lufs_category\")\n",
        "\n",
        "    # 5) tempo BPM categories (512) from time_matrix_small\n",
        "    tempo_small_categories = gather_categories(time_matrix_small, \"tempo_bpm_category\")\n",
        "\n",
        "    # 6) medium-tempo matrix\n",
        "    tempo_medium_categories = gather_categories(time_matrix_tempo_medium, \"tempo_bpm_category\")\n",
        "\n",
        "    # 7) large-tempo matrix\n",
        "    tempo_large_categories = gather_categories(time_matrix_tempo_large, \"tempo_bpm_category\")\n",
        "\n",
        "    # 8) Praat HNR\n",
        "    praat_categories = gather_categories(time_matrix_praat, \"praat_hnr_category\")\n",
        "\n",
        "    # Build a dictionary of counts for each classification dimension\n",
        "    classification_summary = {\n",
        "        \"pitch_accuracy_category\": dict(Counter(pitch_categories)),\n",
        "        \"tone_to_noise_category\": dict(Counter(tone_categories)),\n",
        "        \"transition_category\": dict(Counter(transition_categories)),\n",
        "        \"rms_db_category\": dict(Counter(rms_categories)),\n",
        "        \"lufs_category\": dict(Counter(lufs_categories)),\n",
        "        \"tempo_bpm_category_small\": dict(Counter(tempo_small_categories)),\n",
        "        \"tempo_bpm_category_medium\": dict(Counter(tempo_medium_categories)),\n",
        "        \"tempo_bpm_category_large\": dict(Counter(tempo_large_categories)),\n",
        "        \"praat_hnr_category\": dict(Counter(praat_categories))\n",
        "    }\n",
        "\n",
        "    return classification_summary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b99d9b0",
      "metadata": {},
      "source": [
        "## 15. Generate Actionable Feedback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a8a6812",
      "metadata": {},
      "outputs": [],
      "source": [
        "###\n",
        "# 15. Actionable Feedback Generation ( Module)\n",
        "###\n",
        "\n",
        "def generate_detailed_feedback(analysis_dict):\n",
        "    \"\"\"\n",
        "    Loop over 'time_matrix_small' and produce chunk-level feedback items in a structured format.\n",
        "    E.g. a lis of:\n",
        "      {\n",
        "        \"start_time\": float,\n",
        "        \"end_time\": float,\n",
        "        \"issue_type\": \"pitch\" or \"tempo\" or \"volume\" etc.,\n",
        "        \"issue_level\": \"very poor\" or \"too fast\", etc.,\n",
        "        \"message\": \"Time ~2.54s: pitch accuracy was very poor...\"\n",
        "      }\n",
        "    I'll later use this for more advanced aggregation.\n",
        "    \"\"\"\n",
        "    time_matrix_small = analysis_dict.get(\"time_matrix_small\", [])\n",
        "    sr = analysis_dict[\"sample_rate\"]  # or from track-level\n",
        "    hop_duration = float(512) / sr     # each chunk is ~0.0116s at 44.1kHz, but you might have a bigger chunk\n",
        "\n",
        "    feedback_items = []\n",
        "\n",
        "    for chunk in time_matrix_small:\n",
        "        start_time = chunk.get(\"time_s\", 0.0)\n",
        "        end_time = start_time + hop_duration\n",
        "\n",
        "        pitch_cat = chunk.get(\"pitch_accuracy_category\", \"unknown\")\n",
        "        rms_cat = chunk.get(\"rms_db_category\", \"unknown\")\n",
        "        tempo_cat = chunk.get(\"tempo_bpm_category\", \"unknown\")\n",
        "        transition_cat = chunk.get(\"transition_category\", \"unknown\")\n",
        "        tone_cat = chunk.get(\"tone_to_noise_category\", \"unknown\")\n",
        "\n",
        "        # 1) Pitch Issues\n",
        "        if pitch_cat in [\"poor\", \"very poor\"]:\n",
        "            feedback_items.append({\n",
        "                \"start_time\": start_time,\n",
        "                \"end_time\": end_time,\n",
        "                \"issue_type\": \"pitch\",\n",
        "                \"issue_level\": pitch_cat,\n",
        "                \"message\": f\"Time ~{start_time:.2f}s: pitch accuracy was {pitch_cat}.\"\n",
        "            })\n",
        "\n",
        "        # 2) Volume Issues (RMS)\n",
        "        if rms_cat in [\"very loud\", \"very soft\"]:\n",
        "            feedback_items.append({\n",
        "                \"start_time\": start_time,\n",
        "                \"end_time\": end_time,\n",
        "                \"issue_type\": \"volume\",\n",
        "                \"issue_level\": rms_cat,\n",
        "                \"message\": f\"Time ~{start_time:.2f}s: volume is {rms_cat}.\"\n",
        "            })\n",
        "\n",
        "        # 3) Tempo Issues\n",
        "        if tempo_cat in [\"fast\", \"very fast\", \"slow\", \"very slow\"]:\n",
        "            feedback_items.append({\n",
        "                \"start_time\": start_time,\n",
        "                \"end_time\": end_time,\n",
        "                \"issue_type\": \"tempo\",\n",
        "                \"issue_level\": tempo_cat,\n",
        "                \"message\": f\"Time ~{start_time:.2f}s: tempo is {tempo_cat}.\"\n",
        "            })\n",
        "\n",
        "        # 4) Transition Issues\n",
        "        if transition_cat == \"abrupt\":\n",
        "            feedback_items.append({\n",
        "                \"start_time\": start_time,\n",
        "                \"end_time\": end_time,\n",
        "                \"issue_type\": \"transition\",\n",
        "                \"issue_level\": \"abrupt\",\n",
        "                \"message\": f\"Time ~{start_time:.2f}s: pitch transition was abrupt.\"\n",
        "            })\n",
        "\n",
        "        # 5) Tone-Noisy\n",
        "        if tone_cat in [\"noisy\", \"very noisy\"]:\n",
        "            feedback_items.append({\n",
        "                \"start_time\": start_time,\n",
        "                \"end_time\": end_time,\n",
        "                \"issue_type\": \"tone\",\n",
        "                \"issue_level\": tone_cat,\n",
        "                \"message\": f\"Time ~{start_time:.2f}s: tone is {tone_cat}.\"\n",
        "            })\n",
        "\n",
        "    return feedback_items\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def aggregate_feedback(feedback_items, max_gap=0.5):\n",
        "    \"\"\"\n",
        "    Combine consecutive issues of the same type & level into a single aggregated message.\n",
        "    :param feedback_items: list of dicts from generate_detailed_feedback.\n",
        "    :param max_gap: max alloId gap (in seconds) to still merge consecutive issues.\n",
        "    :return: a list of aggregated feedback dicts.\n",
        "    \"\"\"\n",
        "    if not feedback_items:\n",
        "        return []\n",
        "\n",
        "    # Sort by start time\n",
        "    feedback_sorted = sorted(feedback_items, key=lambda x: x[\"start_time\"])\n",
        "    aggregated = []\n",
        "\n",
        "    # I'll accumulate consecutive items in a \"current group\" if they match.\n",
        "    current_group = None\n",
        "\n",
        "    for item in feedback_sorted:\n",
        "        if current_group is None:\n",
        "            # start a  group\n",
        "            current_group = {\n",
        "                \"issue_type\": item[\"issue_type\"],\n",
        "                \"issue_level\": item[\"issue_level\"],\n",
        "                \"start_time\": item[\"start_time\"],\n",
        "                \"end_time\": item[\"end_time\"],\n",
        "            }\n",
        "        else:\n",
        "            # check if I can merge with the current group\n",
        "            same_issue = (\n",
        "                (item[\"issue_type\"] == current_group[\"issue_type\"]) and\n",
        "                (item[\"issue_level\"] == current_group[\"issue_level\"])\n",
        "            )\n",
        "            small_gap = (item[\"start_time\"] - current_group[\"end_time\"]) <= max_gap\n",
        "\n",
        "            if same_issue and small_gap:\n",
        "                # expand current group end_time\n",
        "                current_group[\"end_time\"] = item[\"end_time\"]\n",
        "            else:\n",
        "                # finalize the current group\n",
        "                aggregated.append(current_group)\n",
        "                # start a  group\n",
        "                current_group = {\n",
        "                    \"issue_type\": item[\"issue_type\"],\n",
        "                    \"issue_level\": item[\"issue_level\"],\n",
        "                    \"start_time\": item[\"start_time\"],\n",
        "                    \"end_time\": item[\"end_time\"],\n",
        "                }\n",
        "\n",
        "    # finalize the last group if it exists\n",
        "    if current_group:\n",
        "        aggregated.append(current_group)\n",
        "\n",
        "    return aggregated\n",
        "\n",
        "\n",
        "\n",
        "def generate_aggregate_messages(aggregated_items):\n",
        "    \"\"\"\n",
        "    Convert the aggregated dictionary items into text messages for the user.\n",
        "    \"\"\"\n",
        "    messages = []\n",
        "    for group in aggregated_items:\n",
        "        issue_type = group[\"issue_type\"]\n",
        "        issue_level = group[\"issue_level\"]\n",
        "        start_t = group[\"start_time\"]\n",
        "        end_t = group[\"end_time\"]\n",
        "\n",
        "        # You can tailor the text per issue_type\n",
        "        if issue_type == \"pitch\":\n",
        "            msg = (\n",
        "                f\"From {start_t:.2f}s to {end_t:.2f}s, your pitch accuracy was consistently {issue_level}. \"\n",
        "                \"Focus on matching the target pitch in this section.\"\n",
        "            )\n",
        "        elif issue_type == \"tempo\":\n",
        "            msg = (\n",
        "                f\"From {start_t:.2f}s to {end_t:.2f}s, your tempo was {issue_level}. \"\n",
        "                \"Try adjusting your speed in that phrase.\"\n",
        "            )\n",
        "        elif issue_type == \"volume\":\n",
        "            msg = (\n",
        "                f\"From {start_t:.2f}s to {end_t:.2f}s, your volume was {issue_level}. \"\n",
        "                \"Consider moderating your dynamics during that passage.\"\n",
        "            )\n",
        "        elif issue_type == \"transition\":\n",
        "            msg = (\n",
        "                f\"From {start_t:.2f}s to {end_t:.2f}s, transitions Ire {issue_level}. \"\n",
        "                \"Practice smoother note changes.\"\n",
        "            )\n",
        "        elif issue_type == \"tone\":\n",
        "            msg = (\n",
        "                f\"From {start_t:.2f}s to {end_t:.2f}s, tone was {issue_level}. \"\n",
        "                \"Work on clarity and resonance for a more refined sound.\"\n",
        "            )\n",
        "        else:\n",
        "            msg = (\n",
        "                f\"From {start_t:.2f}s to {end_t:.2f}s, there was an issue with {issue_type} ({issue_level}).\"\n",
        "            )\n",
        "\n",
        "        messages.append(msg)\n",
        "\n",
        "    return messages\n",
        "\n",
        "\n",
        "\n",
        "###\n",
        "# High-Level Aggregation ( Module)\n",
        "###\n",
        "\n",
        "def aggregate_feedback_high_level(feedback_items, max_gap=5.0):\n",
        "    \"\"\"\n",
        "    Further aggregate detailed feedback items over larger time spans.\n",
        "    \n",
        "    Groups feedback items by the same issue_type and issue_level if the gap betIen them is less than max_gap seconds.\n",
        "    Returns a list of aggregated items where each includes:\n",
        "      - issue_type, issue_level, start_time, end_time, and a count of events in that interval.\n",
        "    \"\"\"\n",
        "    if not feedback_items:\n",
        "        return []\n",
        "\n",
        "    # Sort the detailed feedback by start time.\n",
        "    sorted_items = sorted(feedback_items, key=lambda x: x[\"start_time\"])\n",
        "    high_level = []\n",
        "    current_group = None\n",
        "\n",
        "    for item in sorted_items:\n",
        "        if current_group is None:\n",
        "            current_group = {\n",
        "                \"issue_type\": item[\"issue_type\"],\n",
        "                \"issue_level\": item[\"issue_level\"],\n",
        "                \"start_time\": item[\"start_time\"],\n",
        "                \"end_time\": item[\"end_time\"],\n",
        "                \"count\": 1,\n",
        "            }\n",
        "        else:\n",
        "            # Merge if same issue and gap is less than max_gap.\n",
        "            if (item[\"issue_type\"] == current_group[\"issue_type\"] and \n",
        "                item[\"issue_level\"] == current_group[\"issue_level\"] and \n",
        "                (item[\"start_time\"] - current_group[\"end_time\"]) <= max_gap):\n",
        "                current_group[\"end_time\"] = item[\"end_time\"]\n",
        "                current_group[\"count\"] += 1\n",
        "            else:\n",
        "                high_level.append(current_group)\n",
        "                current_group = {\n",
        "                    \"issue_type\": item[\"issue_type\"],\n",
        "                    \"issue_level\": item[\"issue_level\"],\n",
        "                    \"start_time\": item[\"start_time\"],\n",
        "                    \"end_time\": item[\"end_time\"],\n",
        "                    \"count\": 1,\n",
        "                }\n",
        "    if current_group:\n",
        "        high_level.append(current_group)\n",
        "\n",
        "    return high_level\n",
        "\n",
        "\n",
        "def aggregate_feedback_actionable(feedback_items, max_gap=5.0, min_duration=3.0, min_count=3):\n",
        "    \"\"\"\n",
        "    Further aggregate detailed feedback items over longer time spans and filter out groups that are too short or isolated.\n",
        "    \n",
        "    - max_gap: gap in seconds to merge consecutive feedback items.\n",
        "    - min_duration: minimum duration (in seconds) for the group to be considered actionable.\n",
        "    - min_count: minimum number of occurrences within the group.\n",
        "    \n",
        "    Returns only those groups that meet the thresholds.\n",
        "    \"\"\"\n",
        "    high_level = aggregate_feedback_high_level(feedback_items, max_gap=max_gap)\n",
        "    actionable = []\n",
        "    for group in high_level:\n",
        "        duration = group[\"end_time\"] - group[\"start_time\"]\n",
        "        if duration >= min_duration and group[\"count\"] >= min_count:\n",
        "            actionable.append(group)\n",
        "    return actionable\n",
        "\n",
        "\n",
        "def generate_high_level_messages(aggregated_high_items):\n",
        "    \"\"\"\n",
        "    Convert the high-level aggregated feedback items into human-friendly summary messages.\n",
        "    \"\"\"\n",
        "    messages = []\n",
        "    for group in aggregated_high_items:\n",
        "        issue_type = group[\"issue_type\"]\n",
        "        issue_level = group[\"issue_level\"]\n",
        "        start_t = group[\"start_time\"]\n",
        "        end_t = group[\"end_time\"]\n",
        "        count = group[\"count\"]\n",
        "\n",
        "        if issue_type == \"pitch\":\n",
        "            msg = (f\"BetIen {start_t:.2f}s and {end_t:.2f}s, you had pitch issues \"\n",
        "                   f\"({issue_level}) on {count} separate occasions.\")\n",
        "        elif issue_type == \"tempo\":\n",
        "            msg = (f\"BetIen {start_t:.2f}s and {end_t:.2f}s, your tempo was {issue_level} \"\n",
        "                   f\"on {count} separate occasions.\")\n",
        "        elif issue_type == \"volume\":\n",
        "            msg = (f\"BetIen {start_t:.2f}s and {end_t:.2f}s, your volume was {issue_level} \"\n",
        "                   f\"on {count} separate occasions.\")\n",
        "        elif issue_type == \"transition\":\n",
        "            msg = (f\"BetIen {start_t:.2f}s and {end_t:.2f}s, transitions Ire {issue_level} \"\n",
        "                   f\"on {count} occasions.\")\n",
        "        elif issue_type == \"tone\":\n",
        "            msg = (f\"BetIen {start_t:.2f}s and {end_t:.2f}s, your tone was {issue_level} \"\n",
        "                   f\"on {count} separate occasions.\")\n",
        "        else:\n",
        "            msg = (f\"BetIen {start_t:.2f}s and {end_t:.2f}s, there Ire {count} instances of {issue_type} issues ({issue_level}).\")\n",
        "        messages.append(msg)\n",
        "    return messages\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e78cdc2",
      "metadata": {},
      "source": [
        "## 19.5: Some helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3c142c6",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_advanced_stats(time_matrix):\n",
        "    \"\"\"Computes averages and standard deviations for advanced vocal features, replacing NaNs.\"\"\"\n",
        "    def safe_stat(values):\n",
        "        \"\"\"Returns mean and std deviation, replacing NaNs with 0.0.\"\"\"\n",
        "        values = np.array(values, dtype=np.float32)\n",
        "        values = values[~np.isnan(values)]  # Remove NaNs before computation\n",
        "        if len(values) == 0:\n",
        "            return 0.0, 0.0  # Avoid NaNs in stats\n",
        "        return float(np.mean(values)), float(np.std(values))\n",
        "\n",
        "    jitter_vals = [row.get(\"jitter\", 0) for row in time_matrix]\n",
        "    shimmer_vals = [row.get(\"shimmer\", 0) for row in time_matrix]\n",
        "    F1_vals = [row.get(\"formants\", {}).get(\"F1\", 0) for row in time_matrix]\n",
        "    F2_vals = [row.get(\"formants\", {}).get(\"F2\", 0) for row in time_matrix]\n",
        "    F3_vals = [row.get(\"formants\", {}).get(\"F3\", 0) for row in time_matrix]\n",
        "    vib_extents = [row.get(\"vibrato_extent\", 0) for row in time_matrix]\n",
        "    vib_rates = [row.get(\"vibrato_rate\", 0) for row in time_matrix]\n",
        "\n",
        "    return {\n",
        "        \"avg_jitter\": safe_stat(jitter_vals)[0],\n",
        "        \"std_jitter\": safe_stat(jitter_vals)[1],\n",
        "        \"avg_shimmer\": safe_stat(shimmer_vals)[0],\n",
        "        \"std_shimmer\": safe_stat(shimmer_vals)[1],\n",
        "        \"avg_F1\": safe_stat(F1_vals)[0],\n",
        "        \"std_F1\": safe_stat(F1_vals)[1],\n",
        "        \"avg_F2\": safe_stat(F2_vals)[0],\n",
        "        \"std_F2\": safe_stat(F2_vals)[1],\n",
        "        \"avg_F3\": safe_stat(F3_vals)[0],\n",
        "        \"std_F3\": safe_stat(F3_vals)[1],\n",
        "        \"avg_vibrato_extent\": safe_stat(vib_extents)[0],\n",
        "        \"std_vibrato_extent\": safe_stat(vib_extents)[1],\n",
        "        \"avg_vibrato_rate\": safe_stat(vib_rates)[0],\n",
        "        \"std_vibrato_rate\": safe_stat(vib_rates)[1],\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8ca8cdf",
      "metadata": {},
      "source": [
        "## 20. Quantum Computing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11f3b26e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure logging to show only INFO-level messages.\n",
        "logging.basicConfig(level=logging.INFO,\n",
        "                    format='[%(levelname)s] %(asctime)s - %(message)s',\n",
        "                    datefmt='%H:%M:%S')\n",
        "\n",
        "\n",
        "\n",
        "# --- Helper function to build a variational quantum circuit ---\n",
        "# Here I build a circuit of size num_qubits, for num_layers layers. \n",
        "# Each layer applies Rx, Ry, and Rz (in that sequence) to each qubit using parameters from a ParameterVector. \n",
        "# Then, it applies a ring of entangling gates (by default, CX). I measure all qubits at the end, but I only bind parameters right before execution.\n",
        "def build_variational_circuit(num_qubits, num_layers):\n",
        "    \"\"\"\n",
        "    Creates a parameterized QuantumCircuit with:\n",
        "      - 3 * num_qubits * num_layers free parameters (Rx, Ry, Rz per qubit per layer).\n",
        "      - A ring entangling layer after each set of single-qubit rotations.\n",
        "    Returns (qc, param_vector).\n",
        "    \"\"\"\n",
        "    # I need 3 angles per qubit per layer => total_params\n",
        "    total_params = 3 * num_qubits * num_layers\n",
        "    param_vector = ParameterVector('θ', length=total_params)\n",
        "\n",
        "    qc = QuantumCircuit(num_qubits)\n",
        "\n",
        "    param_idx = 0\n",
        "    for _layer in range(num_layers):\n",
        "        # Apply Rx, Ry, Rz to each qubit\n",
        "        for q in range(num_qubits):\n",
        "            qc.rx(param_vector[param_idx], q)\n",
        "            param_idx += 1\n",
        "            qc.ry(param_vector[param_idx], q)\n",
        "            param_idx += 1\n",
        "            qc.rz(param_vector[param_idx], q)\n",
        "            param_idx += 1\n",
        "\n",
        "        # Entangling: ring of CX\n",
        "        for q in range(num_qubits):\n",
        "            qc.cx(q, (q + 1) % num_qubits)\n",
        "\n",
        "    # I'll add measurement outside, right before execution, to keep circuit parametric\n",
        "    return qc, param_vector\n",
        "\n",
        "\n",
        "# --- Helper function to initialize parameters from features ---\n",
        "# helper that merges your scaled feature vector with the circuit’s free parameters\n",
        "# \tIf param_vector is longer than feature_vector, I tile the features (or just treat them as offsets)\n",
        "#   Or I can keep them separate.\n",
        "def initialize_params_from_features(feature_vector, param_vector):\n",
        "    \"\"\"\n",
        "    Example approach:\n",
        "    - If param_vector is longer than feature_vector, tile feature_vector until I fill param_vector.\n",
        "    - This is just one possible strategy.\n",
        "    Returns a numpy array of the same length as param_vector.\n",
        "    \"\"\"\n",
        "    p_len = len(param_vector)\n",
        "    f_len = len(feature_vector)\n",
        "    if f_len == 0:\n",
        "        # fallback if no features\n",
        "        return np.random.uniform(-math.pi, math.pi, p_len)\n",
        "\n",
        "    tiled_params = []\n",
        "    idx = 0\n",
        "    while len(tiled_params) < p_len:\n",
        "        tiled_params.append(feature_vector[idx % f_len])\n",
        "        idx += 1\n",
        "    return np.array(tiled_params[:p_len], dtype=float)\n",
        "\n",
        "\n",
        "def run_circuit_with_params(qc_template, param_vector, param_values, shots=1024):\n",
        "    \"\"\"\n",
        "    Binds param_values to param_vector in the circuit template.\n",
        "    Measures all qubits.\n",
        "    Returns measurement counts from AerSimulator.\n",
        "    \"\"\"\n",
        "    # 1) Copy the template circuit\n",
        "    qc_bound = qc_template.copy()\n",
        "\n",
        "    # 2) Bind parameters\n",
        "    bind_dict = {p: v for p, v in zip(param_vector, param_values)}\n",
        "    qc_bound = qc_bound.assign_parameters(bind_dict,  inplace=False)\n",
        "\n",
        "    # 3) Add final measurement (all qubits)\n",
        "    num_qubits = qc_bound.num_qubits\n",
        "    qc_bound.measure_all()\n",
        "\n",
        "    # 4) Execute\n",
        "    backend = AerSimulator()\n",
        "    tqc = transpile(qc_bound, backend)\n",
        "    job = backend.run(tqc, shots=shots)\n",
        "    result = job.result()\n",
        "    return result.get_counts()\n",
        "\n",
        "\n",
        "\n",
        "def measurement_distribution_entropy(counts):\n",
        "    total = sum(counts.values())\n",
        "    entropy = 0.0\n",
        "    for _, c in counts.items():\n",
        "        p = c / total\n",
        "        if p > 0:\n",
        "            entropy -= p * math.log2(p)\n",
        "    return entropy\n",
        "\n",
        "\n",
        "\n",
        "# Negative Entropy + Regularization\n",
        "def cost_function(param_values, qc_template, param_vector, shots, regularization_strength=0.0):\n",
        "    \"\"\"\n",
        "    cost = -entropy + reg\n",
        "    Minimizing cost => maximizing Shannon entropy, with L2 penalty on param_values.\n",
        "    \"\"\"\n",
        "    counts = run_circuit_with_params(qc_template, param_vector, param_values, shots=shots)\n",
        "    entropy = measurement_distribution_entropy(counts)\n",
        "    reg = regularization_strength * np.sum(param_values**2)\n",
        "    return -entropy + reg, counts\n",
        "\n",
        "\n",
        "\n",
        "def param_shift_gradient(\n",
        "    current_params,\n",
        "    qc_template,\n",
        "    param_vector,\n",
        "    shots,\n",
        "    regularization_strength=0.0,\n",
        "    shift=math.pi/2\n",
        "):\n",
        "    \"\"\"\n",
        "    Computes gradient of cost_function using the parameter-shift rule.\n",
        "    Returns a numpy array 'grad' same shape as current_params.\n",
        "    \"\"\"\n",
        "    grad = np.zeros_like(current_params, dtype=float)\n",
        "    base_cost, _ = cost_function(current_params, qc_template, param_vector, shots, regularization_strength)\n",
        "\n",
        "    for i in range(len(current_params)):\n",
        "        # + shift\n",
        "        params_plus = np.copy(current_params)\n",
        "        params_plus[i] += shift\n",
        "        cost_plus, _ = cost_function(params_plus, qc_template, param_vector, shots, regularization_strength)\n",
        "        \n",
        "        # - shift\n",
        "        params_minus = np.copy(current_params)\n",
        "        params_minus[i] -= shift\n",
        "        cost_minus, _ = cost_function(params_minus, qc_template, param_vector, shots, regularization_strength)\n",
        "\n",
        "        # Param-shift formula\n",
        "        grad[i] = (cost_plus - cost_minus) / 2.0  # since shift=pi/2 => sin(shift)=1\n",
        "\n",
        "    return grad\n",
        "\n",
        "\n",
        "def gradient_descent_optimization(\n",
        "    qc_template,\n",
        "    param_vector,\n",
        "    init_params,\n",
        "    shots,\n",
        "    regularization_strength,\n",
        "    learning_rate=0.05,\n",
        "    max_iter=10\n",
        "):\n",
        "    \"\"\"\n",
        "    Performs vanilla gradient descent on cost_function.\n",
        "    Returns (best_params, best_cost, final_counts).\n",
        "    \"\"\"\n",
        "    params = np.copy(init_params)\n",
        "    best_cost, best_counts = cost_function(params, qc_template, param_vector, shots, regularization_strength)\n",
        "\n",
        "    for _ in range(max_iter):\n",
        "        # 1) Compute gradient\n",
        "        grad = param_shift_gradient(\n",
        "            params, qc_template, param_vector,\n",
        "            shots=shots,\n",
        "            regularization_strength=regularization_strength\n",
        "        )\n",
        "        # 2) Update\n",
        "        params -= learning_rate * grad\n",
        "\n",
        "        # 3) Evaluate  cost\n",
        "        _cost, _counts = cost_function(params, qc_template, param_vector, shots, regularization_strength)\n",
        "        if _cost < best_cost:\n",
        "            best_cost = _cost\n",
        "            best_counts = _counts\n",
        "\n",
        "    return params, best_cost, best_counts\n",
        "\n",
        "\n",
        "def objective(trial, analysis_dict):\n",
        "    \"\"\"\n",
        "    Optuna objective: for each hyperparam set, build a param circuit, \n",
        "    initialize parameters from features, do gradient descent, and return cost.\n",
        "    \"\"\"\n",
        "    # Pull the user’s data and scale features\n",
        "    feature_vector = _compute_quantum_features_from_analysis(analysis_dict)\n",
        "    scaled_features = nonlinear_scale_features(feature_vector)\n",
        "\n",
        "    # Hyperparams\n",
        "    num_qubits = trial.suggest_int(\"num_qubits\", 2, 6)\n",
        "    num_layers = trial.suggest_int(\"num_layers\", 1, 4)\n",
        "    shots = trial.suggest_categorical(\"shots\", [512, 1024, 2048])\n",
        "    regularization_strength = trial.suggest_float(\"regularization_strength\", 0.0, 0.1, step=0.01)\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 0.001, 0.1, log=True)\n",
        "    max_iter = trial.suggest_int(\"max_iter\", 5, 20)\n",
        "\n",
        "    # Build param circuit\n",
        "    qc_template, param_vec = build_variational_circuit(num_qubits, num_layers)\n",
        "\n",
        "    # If the user picks more qubits than features, I can pad; if feIr, I can truncate\n",
        "    if len(scaled_features) < num_qubits:\n",
        "        # pad with zeros\n",
        "        scaled_features = list(scaled_features) + [0.0]*(num_qubits - len(scaled_features))\n",
        "    else:\n",
        "        # maybe just keep the first 'num_qubits' for demonstration\n",
        "        scaled_features = scaled_features[:num_qubits]\n",
        "\n",
        "    # I need to create initial param values that fill the entire param_vec\n",
        "    init_params_full = initialize_params_from_features(scaled_features, param_vec)\n",
        "\n",
        "    # Run gradient descent\n",
        "    best_params, best_cost, _counts = gradient_descent_optimization(\n",
        "        qc_template,\n",
        "        param_vec,\n",
        "        init_params_full,\n",
        "        shots=shots,\n",
        "        regularization_strength=regularization_strength,\n",
        "        learning_rate=learning_rate,\n",
        "        max_iter=max_iter\n",
        "    )\n",
        "\n",
        "    return best_cost\n",
        "\n",
        "\n",
        "def hyperparameter_search_optuna(analysis_dict, n_trials=10):\n",
        "    \"\"\"\n",
        "    Optuna search that tries different circuit sizes (#qubits, #layers),\n",
        "    shots, reg strength, learning rates, etc. for the \n",
        "    negative entropy + L2 cost.\n",
        "    \"\"\"\n",
        "    def optuna_objective(tr):\n",
        "        return objective(tr, analysis_dict)\n",
        "\n",
        "    study = optuna.create_study(direction=\"minimize\")\n",
        "    study.optimize(optuna_objective, n_trials=n_trials)\n",
        "\n",
        "    best_trial = study.best_trial\n",
        "    best_params = best_trial.params\n",
        "    best_cost = best_trial.value\n",
        "\n",
        "    # Re-run with best hyperparams to get final distribution\n",
        "    # 1) Extract needed params\n",
        "    feature_vector = _compute_quantum_features_from_analysis(analysis_dict)\n",
        "    scaled_features = nonlinear_scale_features(feature_vector)\n",
        "\n",
        "    num_qubits = best_params[\"num_qubits\"]\n",
        "    num_layers = best_params[\"num_layers\"]\n",
        "    shots = best_params[\"shots\"]\n",
        "    regularization_strength = best_params[\"regularization_strength\"]\n",
        "    learning_rate = best_params[\"learning_rate\"]\n",
        "    max_iter = best_params[\"max_iter\"]\n",
        "\n",
        "    # 2) Build final param circuit\n",
        "    qc_template, param_vec = build_variational_circuit(num_qubits, num_layers)\n",
        "\n",
        "    # Match or pad features\n",
        "    if len(scaled_features) < num_qubits:\n",
        "        scaled_features = list(scaled_features) + [0.0]*(num_qubits - len(scaled_features))\n",
        "    else:\n",
        "        scaled_features = scaled_features[:num_qubits]\n",
        "\n",
        "    init_params_full = initialize_params_from_features(scaled_features, param_vec)\n",
        "\n",
        "    # 3) Final gradient-descent optimization\n",
        "    final_params, final_cost, final_counts = gradient_descent_optimization(\n",
        "        qc_template,\n",
        "        param_vec,\n",
        "        init_params_full,\n",
        "        shots=shots,\n",
        "        regularization_strength=regularization_strength,\n",
        "        learning_rate=learning_rate,\n",
        "        max_iter=max_iter\n",
        "    )\n",
        "\n",
        "    # 4) Compute final entropy\n",
        "    final_entropy = measurement_distribution_entropy(final_counts)\n",
        "\n",
        "    # 5) Optionally plot\n",
        "    plot_quantum_counts(final_counts)\n",
        "\n",
        "    # 6) Store\n",
        "    analysis_dict[\"quantum_analysis_variational\"] = {\n",
        "        \"best_params\": best_params,\n",
        "        \"best_optuna_cost\": best_cost,\n",
        "        \"final_params\": final_params.tolist(),\n",
        "        \"final_cost\": final_cost,\n",
        "        \"final_entropy\": final_entropy,\n",
        "        \"counts\": dict(final_counts),\n",
        "    }\n",
        "\n",
        "    print(\"\\n===== BEST HYPERPARAMS FROM OPTUNA =====\")\n",
        "    for k, v in best_params.items():\n",
        "        print(f\"  {k} = {v}\")\n",
        "    print(f\"Best cost (Optuna): {best_cost:.4f}\")\n",
        "    print(f\"Final cost (re-run): {final_cost:.4f}\")\n",
        "    print(f\"Final entropy      : {final_entropy:.4f}\")\n",
        "\n",
        "    return analysis_dict\n",
        "\n",
        "def run_and_store_variational_analysis(analysis_dict):\n",
        "    \"\"\"\n",
        "    Run an Optuna-driven hyperparameter search for a real\n",
        "    gradient-based variational circuit, then store results\n",
        "    in analysis_dict.\n",
        "    \"\"\"\n",
        "    analysis_dict = hyperparameter_search_optuna(analysis_dict, n_trials=1)\n",
        "    return analysis_dict\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# --- Helper function to encode a detected raga ---\n",
        "def encode_raga(raga):\n",
        "    \"\"\"\n",
        "    Encodes a raga string into a numerical value.\n",
        "    A simple encoding: if raga is a string, compute the sum of the Unicode code points \n",
        "    modulo 100 and normalize to [0, 1]. Replace this with a domain-specific encoding as needed.\n",
        "    \"\"\"\n",
        "    if raga is None:\n",
        "        return 0.0\n",
        "    if isinstance(raga, str):\n",
        "        return (sum(ord(c) for c in raga) % 100) / 100.0\n",
        "    try:\n",
        "        return float(raga)\n",
        "    except Exception:\n",
        "        return 0.0\n",
        "\n",
        "\n",
        "def scale_features_individually(features, loIr_bounds, upper_bounds):\n",
        "    \"\"\"\n",
        "    Scales each feature individually from its own [loIr, upper] range to [0, 2*pi].\n",
        "    - features: list of raw feature values.\n",
        "    - loIr_bounds: list of loIr bounds for each feature.\n",
        "    - upper_bounds: list of upper bounds for each feature.\n",
        "    Returns a list of scaled angles.\n",
        "    \"\"\"\n",
        "    scaled = []\n",
        "    for f, l, u in zip(features, loIr_bounds, upper_bounds):\n",
        "        spread = u - l if u != l else 1\n",
        "        scaled_angle = ((f - l) / spread) * 2 * np.pi\n",
        "        scaled.append(scaled_angle)\n",
        "    return scaled\n",
        "\n",
        "\n",
        "def nonlinear_scale_features(features):\n",
        "    \"\"\"\n",
        "    Scales a list of features into rotation angles in [0, 2π] using an arctan transformation.\n",
        "    The transformation centers the features around their median and compresses them based on\n",
        "    the median absolute deviation (MAD).\n",
        "    \n",
        "    The formula for each feature f is:\n",
        "       angle = [arctan(scaling_factor * (f - median)) + π/2] * 2\n",
        "    where scaling_factor = 1 / MAD (with a fallback if MAD is zero).\n",
        "    \"\"\"\n",
        "    features = np.asarray(features, dtype=float)\n",
        "    median_val = np.median(features)\n",
        "    mad = np.median(np.abs(features - median_val))\n",
        "    if mad == 0:\n",
        "        mad = np.std(features) if np.std(features) > 0 else 1.0\n",
        "    scaling_factor = 1.0 / mad\n",
        "    \n",
        "    scaled = []\n",
        "    for f in features:\n",
        "        angle = (np.arctan(scaling_factor * (f - median_val)) + (np.pi / 2)) * 2\n",
        "        scaled.append(angle)\n",
        "    return scaled\n",
        "\n",
        "\n",
        "# --- Function to compute quantum features from analysis_dict ---\n",
        "def _compute_quantum_features_from_analysis(analysis_dict):\n",
        "    \"\"\"\n",
        "    Extracts metrics from analysis_dict and returns a feature vector.\n",
        "    \n",
        "    The full feature set normally includes (1 to 25):\n",
        "      1)  avg_dev_cents,\n",
        "      2)  std_dev_cents,\n",
        "      3)  avg_tnr,\n",
        "      4)  avg_praat_hnr,\n",
        "      5)  rms_db_mean,\n",
        "      6)  lufs_mean,\n",
        "      7)  mean_pitch,\n",
        "      8)  mean_praat_hnr_tmatrix,\n",
        "      9)  detected_raga,\n",
        "      10) off_pitch_count,\n",
        "      11) avg_tempo,\n",
        "      12) avg_jitter,\n",
        "      13) std_jitter,\n",
        "      14) avg_shimmer,\n",
        "      15) std_shimmer,\n",
        "      16) avg_F1,\n",
        "      17) std_F1,\n",
        "      18) avg_F2,\n",
        "      19) std_F2,\n",
        "      20) avg_F3,\n",
        "      21) std_F3,\n",
        "      22) avg_vibrato_extent,\n",
        "      23) std_vibrato_extent,\n",
        "      24) avg_vibrato_rate,\n",
        "      25) std_vibrato_rate.\n",
        "\n",
        "    I're commenting out certain features per your request:\n",
        "      - avg_dev_cents (1)\n",
        "      - rms_db_mean (5)\n",
        "      - lufs_mean (6)\n",
        "      - mean_pitch (7)\n",
        "      - avg_tempo (11)\n",
        "      - avg_jitter (12)\n",
        "      - avg_shimmer (14)\n",
        "      - avg_F1 (16)\n",
        "      - avg_F2 (18)\n",
        "      - avg_F3 (20)\n",
        "      - avg_vibrato_extent (22)\n",
        "      - avg_vibrato_rate (24)\n",
        "    \"\"\"\n",
        "    results = analysis_dict.get(\"results\", {})\n",
        "    avg_dev_cents = results.get(\"average_dev_cents\", 0.0)\n",
        "    std_dev_cents = results.get(\"std_dev_cents\", 0.0)\n",
        "    avg_tnr = results.get(\"avg_tnr\", 0.0)\n",
        "    avg_hnr = results.get(\"avg_praat_hnr\", 0.0)\n",
        "\n",
        "    dyn_sum = analysis_dict.get(\"dynamics_summary\", {})\n",
        "    rms_db_mean = dyn_sum.get(\"rms_db\", {}).get(\"mean\", -40.0)\n",
        "    lufs_mean = dyn_sum.get(\"lufs\", {}).get(\"mean\", -35.0)\n",
        "\n",
        "    time_matrix_small = analysis_dict.get(\"time_matrix_small\", [])\n",
        "    time_matrix_praat = analysis_dict.get(\"time_matrix_praat\", [])\n",
        "    pitch_vals = [row.get(\"pitch_hz\", 0) for row in time_matrix_small if row.get(\"pitch_hz\", 0) > 0]\n",
        "    mean_pitch = float(np.mean(pitch_vals)) if pitch_vals else 0.0\n",
        "    hnr_vals = [row.get(\"praat_hnr\", 0) for row in time_matrix_praat if row.get(\"praat_hnr\", 0) > 0]\n",
        "    mean_praat_hnr_tmatrix = float(np.mean(hnr_vals)) if hnr_vals else 0.0\n",
        "\n",
        "    detected_raga_raw = analysis_dict.get(\"detected_raga\", None)\n",
        "    detected_raga = encode_raga(detected_raga_raw)\n",
        "    \n",
        "    off_pitch_count = analysis_dict.get(\"off_pitch_count\", 0)\n",
        "    avg_tempo = analysis_dict.get(\"avg_tempo\", 0.0)\n",
        "    \n",
        "    # Advanced vocal stats\n",
        "    adv_stats = analysis_dict.get(\"advanced_vocal_stats\", {})\n",
        "    # I'll explicitly log the advanced features:\n",
        "    #logging.info(f\"Advanced features: {adv_stats}\")\n",
        "\n",
        "    avg_jitter = adv_stats.get(\"avg_jitter\", 0.0)\n",
        "    std_jitter = adv_stats.get(\"std_jitter\", 0.0)\n",
        "    avg_shimmer = adv_stats.get(\"avg_shimmer\", 0.0)\n",
        "    std_shimmer = adv_stats.get(\"std_shimmer\", 0.0)\n",
        "    avg_F1 = adv_stats.get(\"avg_F1\", 0.0)\n",
        "    std_F1 = adv_stats.get(\"std_F1\", 0.0)\n",
        "    avg_F2 = adv_stats.get(\"avg_F2\", 0.0)\n",
        "    std_F2 = adv_stats.get(\"std_F2\", 0.0)\n",
        "    avg_F3 = adv_stats.get(\"avg_F3\", 0.0)\n",
        "    std_F3 = adv_stats.get(\"std_F3\", 0.0)\n",
        "    avg_vib_extent = adv_stats.get(\"avg_vibrato_extent\", 0.0)\n",
        "    std_vib_extent = adv_stats.get(\"std_vibrato_extent\", 0.0)\n",
        "    avg_vib_rate = adv_stats.get(\"avg_vibrato_rate\", 0.0)\n",
        "    std_vib_rate = adv_stats.get(\"std_vibrato_rate\", 0.0)\n",
        "    \n",
        "    # Build the feature vector, with certain features commented out\n",
        "    feature_vector = [\n",
        "        avg_dev_cents,  # (1)\n",
        "        std_dev_cents,    # (2)\n",
        "        avg_tnr,          # (3)\n",
        "        avg_hnr,          # (4)\n",
        "        rms_db_mean,    # (5)\n",
        "        # lufs_mean,      # (6)\n",
        "        # mean_pitch,     # (7)\n",
        "        mean_praat_hnr_tmatrix,  # (8)\n",
        "        detected_raga,    # (9)\n",
        "        off_pitch_count,  # (10)\n",
        "        # avg_tempo,      # (11)\n",
        "        avg_jitter,     # (12)\n",
        "        std_jitter,       # (13)\n",
        "        # avg_shimmer,    # (14)\n",
        "        std_shimmer,      # (15)\n",
        "        # avg_F1,         # (16)\n",
        "        # std_F1,           # (17)\n",
        "        # avg_F2,         # (18)\n",
        "        # std_F2,           # (19)\n",
        "        # avg_F3,         # (20)\n",
        "        # std_F3,           # (21)\n",
        "        # avg_vib_extent, # (22)\n",
        "        std_vib_extent,   # (23)\n",
        "        # avg_vib_rate,   # (24)\n",
        "        std_vib_rate      # (25)\n",
        "    ]\n",
        "    \n",
        "    # Only print the final feature vector\n",
        "    #logging.info(f\"Final feature vector: {feature_vector}\")\n",
        "    return feature_vector\n",
        "\n",
        "\n",
        "def quantum_pipeline_complex(feature_vector):\n",
        "    \"\"\"\n",
        "    Quantum circuit pipeline for musical feature analysis.\n",
        "    Input: feature_vector - PRE-SCALED angles in the correct order:\n",
        "      [average_pitch_deviation, average_jitter, std_tempo_deviation,\n",
        "       average_shimmer, average_lufs_energy, std_lufs_energy,\n",
        "       std_mfcc, zero_crossing_rate, average_tone_to_noise_ratio]\n",
        "\n",
        "    Returns: scaled_features, counts\n",
        "    \"\"\"\n",
        "\n",
        "    from qiskit import QuantumCircuit, transpile\n",
        "    from qiskit_aer import AerSimulator\n",
        "    from qiskit.visualization import plot_histogram\n",
        "\n",
        "    num_qubits = 9\n",
        "    qc = QuantumCircuit(num_qubits, num_qubits)\n",
        "\n",
        "    # Step 1: Hadamard layer for initial superposition\n",
        "    qc.h(range(num_qubits))\n",
        "\n",
        "    # Step 2: Feature-based rotations\n",
        "    # Rx (Pitch/Tempo): qubits 0,1,2\n",
        "    qc.rx(feature_vector[0], 0)  # avg pitch deviation\n",
        "    qc.rx(feature_vector[1], 1)  # avg jitter\n",
        "    qc.rx(feature_vector[2], 2)  # std tempo deviation\n",
        "\n",
        "    # Ry (Dynamics/Energy): qubits 3,4,5\n",
        "    qc.ry(feature_vector[3], 3)  # avg shimmer\n",
        "    qc.ry(feature_vector[4], 4)  # avg LUFS energy\n",
        "    qc.ry(feature_vector[5], 5)  # std LUFS energy (dynamic motion)\n",
        "\n",
        "    # Rz (Timbre/ZCR/Tonal Clarity): qubits 6,7,8\n",
        "    qc.rz(feature_vector[6], 6)  # std MFCC (timbre consistency)\n",
        "    qc.rz(feature_vector[7], 7)  # zero-crossing rate\n",
        "    qc.rz(feature_vector[8], 8)  # avg tone-to-noise ratio\n",
        "\n",
        "    # Step 3: Intra-group entanglement\n",
        "    # Rx group (pitch/tempo stability)\n",
        "    qc.cx(0, 1)\n",
        "    qc.cx(1, 2)\n",
        "\n",
        "    # Ry group (energy/dynamics)\n",
        "    qc.cx(3, 4)\n",
        "    qc.cx(4, 5)\n",
        "\n",
        "    # Rz group (timbre/clarity)\n",
        "    qc.cx(6, 7)\n",
        "    qc.cx(7, 8)\n",
        "\n",
        "    # Step 4: Cross-group entanglement (Inter-group Coupling)\n",
        "    qc.cx(2, 3)  # tempo stability ↔ shimmer (expressivity)\n",
        "    qc.cx(1, 4)  # jitter ↔ avg LUFS energy\n",
        "    qc.cx(0, 6)  # pitch stability ↔ timbre consistency (stylistic nuance)\n",
        "    qc.cx(5, 7)  # dynamic motion ↔ zero-crossing (rhythmic clarity)\n",
        "\n",
        "    # Step 5: Measurement\n",
        "    qc.measure(range(num_qubits), range(num_qubits))\n",
        "\n",
        "    # Optional (recommended): circuit visualization for debugging\n",
        "    print(qc.draw(output='text'))\n",
        "\n",
        "    # Step 6: Execute on Aer simulator\n",
        "    simulator = AerSimulator()\n",
        "    compiled_qc = transpile(qc, simulator)\n",
        "    result = simulator.run(compiled_qc, shots=1024).result()\n",
        "    counts = result.get_counts()\n",
        "\n",
        "    # Your existing function returns scaled features AND counts:\n",
        "    return feature_vector, counts\n",
        "\n",
        "\n",
        "def plot_quantum_counts(counts):\n",
        "    \"\"\"\n",
        "    Plots the measurement distribution.\n",
        "    \"\"\"\n",
        "    #outcomes = list(counts.keys())\n",
        "    #frequencies = list(counts.values())\n",
        "    #fig, ax = plt.subplots(figsize=(7, 4))\n",
        "    #ax.bar(outcomes, frequencies, color='skyblue')\n",
        "    #ax.set_xlabel(\"Measurement Outcome\")\n",
        "    #ax.set_ylabel(\"Counts\")\n",
        "    #ax.set_title(\"Quantum Measurement Distribution\")\n",
        "    #plt.xticks(rotation=45)\n",
        "    #plt.tight_layout()\n",
        "    #plt.show()\n",
        "\n",
        "\n",
        "def run_and_store_quantum_analysis(analysis_dict):\n",
        "    \"\"\"\n",
        "    1) Compute the feature vector (with some features commented out).\n",
        "    2) Run the complex quantum circuit.\n",
        "    3) Plot the measurement distribution.\n",
        "    4) Store results in analysis_dict.\n",
        "    \"\"\"\n",
        "\n",
        "    # Example of retrieving advanced stats, if you want them in your quantum section:\n",
        "    advanced_stats = analysis_dict.get(\"advanced_vocal_stats\", {})\n",
        "\n",
        "    feature_vector = _compute_quantum_features_from_analysis(analysis_dict)\n",
        "    scaled_features, measurement_counts = quantum_pipeline_complex(feature_vector)\n",
        "    plot_quantum_counts(measurement_counts)\n",
        "    \n",
        "    analysis_dict[\"quantum_analysis\"] = {\n",
        "        \"feature_vector\": feature_vector,\n",
        "        \"scaled_angles\": scaled_features,\n",
        "        \"advanced_stats\": advanced_stats,\n",
        "        \"measurement_counts\": dict(measurement_counts)\n",
        "    }\n",
        "    \n",
        "    # Comment out or remove other debug messages; keep the essential final line if you want:\n",
        "    # logging.info(\"Quantum analysis complete (complex circuit).\")\n",
        "\n",
        "    return analysis_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mod12-classical",
      "metadata": {},
      "source": [
        "## 12. Pipeline - both classical AND quantum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "classical-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def _replace_nan_with_none(value):\n",
        "    if isinstance(value, float) and np.isnan(value):\n",
        "        return None\n",
        "    elif isinstance(value, list):\n",
        "        return [_replace_nan_with_none(v) for v in value]\n",
        "    elif isinstance(value, tuple):\n",
        "        return tuple(_replace_nan_with_none(v) for v in value)\n",
        "    elif isinstance(value, dict):\n",
        "        return {k: _replace_nan_with_none(v) for k, v in value.items()}\n",
        "    else:\n",
        "        return value\n",
        "\n",
        "def grade_single_file(file_name):\n",
        "    \"\"\"\n",
        "    1) Preprocess => audio_data\n",
        "    2) Noise reduction => clean_audio\n",
        "    3) pitch => times, pitches\n",
        "    4) map pitch => note_names/freqs/deviations\n",
        "    5) stats => dev cents\n",
        "    6) build time_matrix_small (512)\n",
        "    7) Module 7 (Revised): analyze_dynamics_module7 => uses LUF_CHUNK_SIZE for RMS & LUFS.\n",
        "    8) analyze => spectral flatness => tone_to_noise, transition_score\n",
        "    9) advanced features => zcr, centroid, rolloff...\n",
        "    10) small-chunk tempo => 512\n",
        "    11) build time_matrix_praat => 2048 => hnr\n",
        "    12) build time_matrix_tempo_medium => 4096, time_matrix_tempo_large => 16384\n",
        "    13) store in DB => analysis_dict, then plot (including RMS & LUFS)\n",
        "    \"\"\"\n",
        "    input_path = os.path.join(INPUT_DIR, file_name)\n",
        "    \n",
        "\n",
        "    # 1. Preprocessing\n",
        "    audio_data, sr = preprocess_audio(input_path)\n",
        "\n",
        "    # 2. Noise reduction\n",
        "    clean_audio = simple_noise_reduction(audio_data, sr)\n",
        "\n",
        "    ###\n",
        "    # PATCH 2: Remove Percussion from Clean Audio\n",
        "    ###\n",
        "    harmonic_audio = remove_percussive_components(clean_audio, sr, margin=(1.0, 1.0))\n",
        "    # now to pass this everywhere \n",
        "    \n",
        "    # 3. pitch\n",
        "    times, pitches, voiced_flags, confidences = extract_pitch_contour(harmonic_audio, sr)\n",
        "\n",
        "    # Estimate the base pitch (drone)\n",
        "    base_pitch = estimate_base_pitch_from_contour(pitches, voiced_flags)\n",
        "\n",
        "    # Map to standard notes\n",
        "    Istern_notes, hindustani_notes = map_pitches_with_librosa_functions(\n",
        "        pitches, base_pitch=base_pitch\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    # 4. map pitch => note\n",
        "    # note_names, note_freqs, deviations = map_pitches_to_notes(pitches)\n",
        "    # note_names, note_freqs, deviations, sruti_labels = map_pitches_to_notes_22sruti(pitches)\n",
        "    # Map to 22-sruti classification (the user's microtonal logic)\n",
        "    note_names, note_freqs, deviations, sruti_labels = map_pitches_to_notes_22sruti(pitches, tonic=base_pitch if base_pitch else 240.0)\n",
        "\n",
        "\n",
        "\n",
        "    # 5. stats\n",
        "    valid_devs = [d for d in deviations if d is not None]\n",
        "    if len(valid_devs) > 0:\n",
        "        avg_deviation_cents = float(np.mean(np.abs(valid_devs)))\n",
        "        std_deviation_cents = float(np.std(valid_devs))\n",
        "    else:\n",
        "        avg_deviation_cents = 0.0\n",
        "        std_deviation_cents = 0.0\n",
        "\n",
        "    # 6. build time_matrix_small\n",
        "    time_matrix_small = []\n",
        "    for t, p, nf, d, nn, sruti in zip(times, pitches, note_freqs, deviations, note_names, sruti_labels):\n",
        "        dev_flag_int = 1 if d is not None and abs(d) > DEVIATION_THRESHOLD else 0\n",
        "        time_matrix_small.append({\n",
        "            \"time_s\": float(t),\n",
        "            \"pitch_hz\": p,\n",
        "            \"note_name\": nn,\n",
        "            \"note_freq_hz\": nf,\n",
        "            \"deviation_cents\": d,\n",
        "            \"dev_flag\": dev_flag_int,\n",
        "            \"sruti_class\": sruti,  # store which śruti class it belongs to\n",
        "            \"pitch_accuracy_category\": classify_pitch_deviation(d)\n",
        "        })\n",
        "\n",
        "    # 7. DYNAMICS => RMS & LUFS (with bigger chunk => LUF_CHUNK_SIZE)\n",
        "    time_matrix_small, dyn_summary = analyze_dynamics_module7(\n",
        "        harmonic_audio, sr, time_matrix_small\n",
        "    )\n",
        "\n",
        "    # 8. analyze => tone_to_noise = spectral flatness, transition_score\n",
        "    time_matrix_small = analyze_note_transitions(harmonic_audio, sr, time_matrix_small)\n",
        "\n",
        "    # 9. advanced features => skip RMS here, I do zcr, centroid, rolloff...\n",
        "    time_matrix_small = update_time_matrix_small_with_advanced_features(harmonic_audio, sr, time_matrix_small)\n",
        "\n",
        "    # 10. small-chunk tempo => 512\n",
        "    time_matrix_small = analyze_tempo_adherence(harmonic_audio, sr, time_matrix_small)\n",
        "\n",
        "    # 11. build time_matrix_praat => 2048-chunk\n",
        "    time_matrix_praat = build_praat_time_matrix(harmonic_audio, sr)\n",
        "    for row in time_matrix_praat:\n",
        "        row[\"praat_hnr_category\"] = classify_praat_hnr(row[\"praat_hnr\"])\n",
        "\n",
        "    # 12. build time_matrix_tempo_medium => 4096, time_matrix_tempo_large => 16384\n",
        "    time_matrix_tempo_medium = build_larger_tempo_matrix(\n",
        "        harmonic_audio, sr, chunk_size=TEMPO_CHUNK_SIZE_MEDIUM, overlap=0.5\n",
        "    )\n",
        "    time_matrix_tempo_large = build_larger_tempo_matrix(\n",
        "        harmonic_audio, sr, chunk_size=TEMPO_CHUNK_SIZE_LARGE, overlap=0.5\n",
        "    )\n",
        "\n",
        "    # gather final stats => from time_matrix_small\n",
        "    transition_scores = [row.get('transition_score', 0.0) for row in time_matrix_small]\n",
        "    tone_to_noise_vals = [row.get('tone_to_noise', 0.0) for row in time_matrix_small]\n",
        "\n",
        "    if tone_to_noise_vals:\n",
        "        avg_tnr = float(np.mean(tone_to_noise_vals))\n",
        "        std_tnr = float(np.std(tone_to_noise_vals))\n",
        "    else:\n",
        "        avg_tnr = 0.0\n",
        "        std_tnr = 0.0\n",
        "\n",
        "    if transition_scores:\n",
        "        avg_transition_score = float(np.mean(transition_scores))\n",
        "        std_transition_score = float(np.std(transition_scores))\n",
        "    else:\n",
        "        avg_transition_score = 0.0\n",
        "        std_transition_score = 0.0\n",
        "\n",
        "    # gather praat HNR stats\n",
        "    praat_hnr_vals = [row['praat_hnr'] for row in time_matrix_praat]\n",
        "    if praat_hnr_vals:\n",
        "        avg_praat_hnr = float(np.mean(praat_hnr_vals))\n",
        "        std_praat_hnr = float(np.std(praat_hnr_vals))\n",
        "    else:\n",
        "        avg_praat_hnr = 0.0\n",
        "        std_praat_hnr = 0.0\n",
        "\n",
        "    # Compute advanced note features over the whole file (or using your existing note segmentation)\n",
        "    note_segments = detect_notes(clean_audio, sr)  # Or use your preferred segmentation method\n",
        "    advanced_note_features = update_note_transition_analysis_with_advanced_features(clean_audio, sr, note_segments)\n",
        "\n",
        "    advanced_vocal_stats = compute_advanced_stats(time_matrix_small)\n",
        "\n",
        "\n",
        "    analysis_dict = {\n",
        "        \"file_name\": file_name,\n",
        "        \"sample_rate\": sr,\n",
        "        \"results\": {\n",
        "            \"average_dev_cents\": avg_deviation_cents,\n",
        "            \"std_dev_cents\": std_deviation_cents,\n",
        "            \"deviation_threshold\": DEVIATION_THRESHOLD,\n",
        "            \"avg_transition_score\": avg_transition_score,\n",
        "            \"std_transition_score\": std_transition_score,\n",
        "            \"avg_tnr\": avg_tnr,\n",
        "            \"std_tnr\": std_tnr,\n",
        "            \"avg_praat_hnr\": avg_praat_hnr,\n",
        "            \"std_praat_hnr\": std_praat_hnr\n",
        "        },\n",
        "        \"time_matrix_small\": time_matrix_small,\n",
        "        \"time_matrix_praat\": time_matrix_praat,\n",
        "        \"dynamics_summary\": dyn_summary,\n",
        "        \"time_matrix_tempo_medium\": time_matrix_tempo_medium,\n",
        "        \"time_matrix_tempo_large\": time_matrix_tempo_large,\n",
        "        \"advanced_note_features\": advanced_note_features,\n",
        "        \"advanced_vocal_stats\": advanced_vocal_stats  #  key with aggregated advanced features\n",
        "    }\n",
        "\n",
        "    analysis_dict[\"pyin_pitch_contour\"] = {\n",
        "        \"times\": times.tolist(),\n",
        "        \"pitches\": pitches.tolist(),\n",
        "        \"voiced_flags\": voiced_flags.tolist(),\n",
        "        \"confidences\": confidences.tolist(),\n",
        "        \"base_pitch\": float(base_pitch) if base_pitch else None,\n",
        "        \"Istern_notes\": Istern_notes,\n",
        "        \"hindustani_notes_standard\": hindustani_notes,\n",
        "        \"hindustani_notes_22sruti\": sruti_labels\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # 7) Align with existing time_matrix_small, assuming it uses the same hop_length\n",
        "    #    and has len(time_matrix_small) == number of frames for the same analysis window.\n",
        "    time_matrix_small = analysis_dict.get(\"time_matrix_small\", [])\n",
        "    if len(time_matrix_small) == len(times):\n",
        "        # Direct index-based alignment\n",
        "        for i, row in enumerate(time_matrix_small):\n",
        "            row[\"pyin_pitch\"] = float(pitches[i]) if not np.isnan(pitches[i]) else None\n",
        "            row[\"pyin_Istern_note\"] = Istern_notes[i]\n",
        "            row[\"pyin_hindustani_note_standard\"] = hindustani_notes[i]\n",
        "            row[\"pyin_hindustani_note_22sruti\"] = sruti_labels[i]\n",
        "    else:\n",
        "        # If the lengths don't match, do nearest-time alignment\n",
        "        for row in time_matrix_small:\n",
        "            t_s = row.get(\"time_s\", 0.0)\n",
        "            idx = np.argmin(np.abs(times - t_s))\n",
        "            row[\"pyin_pitch\"] = float(pitches[idx]) if not np.isnan(pitches[idx]) else None\n",
        "            row[\"pyin_Istern_note\"] = Istern_notes[idx]\n",
        "            row[\"pyin_hindustani_note_standard\"] = hindustani_notes[idx]\n",
        "            row[\"pyin_hindustani_note_22sruti\"] = sruti_labels[idx]\n",
        "\n",
        "    analysis_dict[\"time_matrix_small\"] = time_matrix_small\n",
        "\n",
        "\n",
        "\n",
        "    ################\n",
        "    # : Summarize classification counts\n",
        "    ################\n",
        "    classification_summary = summarize_classifications(\n",
        "        time_matrix_small=time_matrix_small,\n",
        "        time_matrix_praat=time_matrix_praat,\n",
        "        time_matrix_tempo_medium=time_matrix_tempo_medium,\n",
        "        time_matrix_tempo_large=time_matrix_tempo_large\n",
        "    )\n",
        "    analysis_dict[\"classification_summary\"] = classification_summary\n",
        "\n",
        "    # ======  QUANTUM STEP ======\n",
        "    analysis_dict = run_and_store_quantum_analysis(analysis_dict)\n",
        "    #analysis_dict = run_and_store_variational_analysis(analysis_dict)\n",
        "\n",
        "    # ====== END  QUANTUM STEP ======\n",
        "\n",
        "    # Generate feedback\n",
        "    analysis_dict[\"detailed_feedback_items\"] = generate_detailed_feedback(analysis_dict)\n",
        "\n",
        "    # Aggregate (basic level)\n",
        "    aggregated_items = aggregate_feedback(analysis_dict[\"detailed_feedback_items\"], max_gap=2.0)\n",
        "    # Then generate textual messages\n",
        "    aggregated_messages = generate_aggregate_messages(aggregated_items)\n",
        "    analysis_dict[\"aggregate_feedback\"] = aggregated_messages\n",
        "\n",
        "    # High-level actionable aggregation:\n",
        "    # Only groups with a duration of at least 3 seconds and at least 3 occurrences are retained.\n",
        "    high_level_feedback = aggregate_feedback_actionable(\n",
        "        analysis_dict[\"detailed_feedback_items\"],\n",
        "        max_gap=5.0,\n",
        "        min_duration=3.0,\n",
        "        min_count=3\n",
        "    )\n",
        "    high_level_messages = generate_high_level_messages(high_level_feedback)\n",
        "    analysis_dict[\"high_level_feedback\"] = high_level_messages\n",
        "    \n",
        "\n",
        "    # Raga detection\n",
        "    # Build a list of notes:\n",
        "    detected_notes = [row[\"note_name\"] for row in time_matrix_small if row.get(\"note_name\")]\n",
        "    raga_info = detect_raga_from_notes(detected_notes, raga_db)\n",
        "    analysis_dict[\"raga_detection\"] = raga_info\n",
        "    # raga_info has keys: best_raga, score, explanation, all_scores\n",
        "\n",
        "    # --- Display Indian Note Transitions ---\n",
        "    # --- Display Indian Note Transitions with Proper Names ---\n",
        "    # print(\"\\nIndian Note Transitions:\")\n",
        "\n",
        "    # Define the note cycle: 8 items (7 distinct notes plus the higher Sa)\n",
        "    notes = [\"Sa\", \"Re\", \"Ga\", \"Ma\", \"Pa\", \"Dha\", \"Ni\", \"Sa\"]\n",
        "\n",
        "    prev_indian_note = None\n",
        "    for entry in time_matrix_small:\n",
        "        # Get the raw sruti value (e.g., \"sruti_10\")\n",
        "        current_sruti = entry.get(\"sruti_class\")\n",
        "        # Convert the raw sruti to a proper note using our mapping logic.\n",
        "        if current_sruti is not None and \"_\" in current_sruti:\n",
        "            try:\n",
        "                # Extract the numeric part.\n",
        "                num = int(current_sruti.split('_')[1])\n",
        "                # Map to the index in our notes list.\n",
        "                # Here I assume sruti_10 should map to \"Sa\", so subtract 10.\n",
        "                note_index = (num - 10) % 8\n",
        "                current_indian_note = notes[note_index]\n",
        "            except Exception as e:\n",
        "                # In case of any error, fall back to the raw sruti.\n",
        "                current_indian_note = current_sruti\n",
        "        else:\n",
        "            current_indian_note = current_sruti\n",
        "\n",
        "        time_s = entry.get(\"time_s\")\n",
        "        if current_indian_note != prev_indian_note:\n",
        "        #    if prev_indian_note is None:\n",
        "        #        print(f\"Start at {current_indian_note} at {time_s:.2f} s\")\n",
        "        #    else:\n",
        "        #        print(f\"Transition: {prev_indian_note} -> {current_indian_note} at {time_s:.2f} s\")\n",
        "            prev_indian_note = current_indian_note\n",
        "\n",
        "\n",
        "\n",
        "    sanitized_analysis_dict = _replace_nan_with_none(analysis_dict)\n",
        "\n",
        "\n",
        "    if SAVE_TO_DB:\n",
        "        db = QuantumMusicDB()\n",
        "        db.create_tables()\n",
        "        rec_id = db.insert_analysis(\n",
        "            file_name=file_name,\n",
        "            sample_rate=sr,\n",
        "            analysis_data=sanitized_analysis_dict\n",
        "        )\n",
        "        #print(f\"Inserted analysis record with ID: {rec_id}\")\n",
        "        db.close()\n",
        "\n",
        "    #print(\"\\nNow playing the processed audio (bandpass, normalized, trimmed):\")\n",
        "    #display(Audio(data=clean_audio, rate=sr))\n",
        "\n",
        "    #print(\"\\\\nNow playing the processed audio with percussion removed:\")\n",
        "    #display(Audio(data=harmonic_audio, rate=sr))\n",
        "\n",
        "    #print(\"\\\\nNow playing the processed audio with tanpura removed:\")\n",
        "    #tanpuraremoved_audio = remove_drone(harmonic_audio, sr)\n",
        "    #display(Audio(data=tanpuraremoved_audio, rate=sr))\n",
        "\n",
        "\n",
        "    # Print the best raga (which may be 'Kedar' if that has the highest score).\n",
        "    #print(f\"Detected Raga: {raga_info['best_raga']}, reason: {raga_info['explanation']}\")\n",
        "\n",
        "    # Print the top 3 detected ragas.\n",
        "    #print(\"\\nTop 3 detected ragas:\")\n",
        "    #for index, raga in enumerate(raga_info['top_ragas'], start=1):\n",
        "    #    print(f\"{index}. {raga['raga']} (Score: {raga['score']:.2f}) - {raga['explanation']}\")\n",
        "\n",
        "    plot_all_metrics(\n",
        "        time_matrix_small,\n",
        "        time_matrix_praat,\n",
        "        audio_data=harmonic_audio,  # Using harmonic audio\n",
        "        sr=sr,\n",
        "        time_matrix_tempo_medium=time_matrix_tempo_medium,\n",
        "        time_matrix_tempo_large=time_matrix_tempo_large,\n",
        "        aggregated_items=aggregated_items,\n",
        "        analysis_dict=analysis_dict,\n",
        "        input_filename=file_name\n",
        "    )\n",
        "\n",
        "    # Print the basic aggregated feedback.\n",
        "    #print(\"\\nAGGREGATED FEEDBACK\")\n",
        "    #for msg in analysis_dict[\"aggregate_feedback\"]:\n",
        "    #    print(msg)\n",
        "\n",
        "    # Print the high-level (actionable) feedback summary.\n",
        "    #print(\"\\nHigh-Level Feedback Summary:\")\n",
        "    #for msg in analysis_dict[\"high_level_feedback\"]:\n",
        "    #    print(msg)\n",
        "\n",
        "\n",
        "    return sanitized_analysis_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mod13-quantum",
      "metadata": {},
      "source": [
        "## 13. Quantum Pipeline (Placeholder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "quantum-placeholder-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 13. QUANTUM PIPELINE (PLACEHOLDER)\n",
        "def quantum_pipeline_placeholder(feature_matrix):\n",
        "    #print(\"[Quantum Pipeline Placeholder] Feature matrix received.\")\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb0edca1",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import librosa\n",
        "from scipy.signal import iirnotch, filtfilt\n",
        "\n",
        "def remove_drone(audio_data, sr, search_duration=2.0, q_factor=30.0):\n",
        "    \"\"\"\n",
        "    Identify and remove a strong background drone at a specific pitch.\n",
        "    \n",
        "    :param audio_data: 1D numpy array of the audio samples\n",
        "    :param sr: sampling rate\n",
        "    :param search_duration: length (in seconds) of audio to analyze for the drone freq\n",
        "    :param q_factor: notch filter Q-factor (sharpness). Higher => narroIr notch.\n",
        "    :return: drone_removed_audio (numpy array)\n",
        "    \"\"\"\n",
        "    # 1) Analyze a short portion of the audio (up to search_duration seconds) \n",
        "    #    to estimate the main drone frequency.\n",
        "    max_samples = min(len(audio_data), int(search_duration * sr))\n",
        "    snippet = audio_data[:max_samples]\n",
        "\n",
        "    # 2) Compute an FFT-based poIr spectrum.\n",
        "    #    I'll find the frequency bin with the highest magnitude (excluding DC).\n",
        "    fft_spectrum = np.fft.rfft(snippet)\n",
        "    freqs = np.fft.rfftfreq(len(snippet), d=1.0/sr)\n",
        "    \n",
        "    # Exclude DC (freq=0) to avoid picking silence or offset as the drone\n",
        "    fft_spectrum[0] = 0.0  # zero out DC component\n",
        "    poIr_spectrum = np.abs(fft_spectrum)\n",
        "    \n",
        "    # 3) Identify the peak frequency\n",
        "    peak_index = np.argmax(poIr_spectrum)\n",
        "    drone_freq = freqs[peak_index]\n",
        "    print(f\"Identified drone frequency ~ {drone_freq:.2f} Hz\")\n",
        "    \n",
        "    # 4) Design a narrow notch filter around that frequency\n",
        "    # w0 = (target freq) / (nyquist freq)\n",
        "    nyquist = 0.5 * sr\n",
        "    w0 = drone_freq / nyquist\n",
        "    b, a = iirnotch(w0, Q=q_factor)  # scipy.signal.iirnotch\n",
        "    \n",
        "    # 5) Apply the notch filter to the entire audio using filtfilt\n",
        "    drone_removed = filtfilt(b, a, audio_data)\n",
        "    \n",
        "    return drone_removed"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7789edb0",
      "metadata": {},
      "source": [
        "# Execution Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "c79e6e0e",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/98/ykbpmmjd0pdf8zgd8bc1hhmr0000gn/T/ipykernel_10432/3236860307.py:40: UserWarning: kernel_size exceeds volume extent: the volume will be zero-padded.\n",
            "  smoothed = scipy.signal.medfilt(f0, kernel_size=MEDIAN_FILTER_KERNEL_SIZE)\n",
            "/opt/miniconda3/envs/quantumvenv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=512\n",
            "  warnings.warn(\n",
            "[INFO] 15:30:46 - Pass: ContainsInstruction - 0.00906 (ms)\n",
            "[INFO] 15:30:46 - Pass: UnitarySynthesis - 0.00811 (ms)\n",
            "[INFO] 15:30:46 - Pass: HighLevelSynthesis - 0.13113 (ms)\n",
            "[INFO] 15:30:46 - Pass: BasisTranslator - 0.07486 (ms)\n",
            "[INFO] 15:30:46 - Pass: ElidePermutations - 0.02098 (ms)\n",
            "[INFO] 15:30:46 - Pass: RemoveDiagonalGatesBeforeMeasure - 0.07105 (ms)\n",
            "[INFO] 15:30:46 - Pass: RemoveIdentityEquivalent - 0.00811 (ms)\n",
            "[INFO] 15:30:46 - Pass: InverseCancellation - 0.11992 (ms)\n",
            "[INFO] 15:30:46 - Pass: CommutativeCancellation - 0.08297 (ms)\n",
            "[INFO] 15:30:46 - Pass: ConsolidateBlocks - 0.20480 (ms)\n",
            "[INFO] 15:30:46 - Pass: Split2QUnitaries - 0.01669 (ms)\n",
            "[INFO] 15:30:46 - Pass: UnitarySynthesis - 0.00501 (ms)\n",
            "[INFO] 15:30:46 - Pass: HighLevelSynthesis - 0.10014 (ms)\n",
            "[INFO] 15:30:46 - Pass: BasisTranslator - 0.07296 (ms)\n",
            "[INFO] 15:30:46 - Pass: ConsolidateBlocks - 0.11182 (ms)\n",
            "[INFO] 15:30:46 - Pass: UnitarySynthesis - 0.00691 (ms)\n",
            "[INFO] 15:30:46 - Pass: Depth - 0.03123 (ms)\n",
            "[INFO] 15:30:46 - Pass: FixedPoint - 0.00906 (ms)\n",
            "[INFO] 15:30:46 - Pass: Size - 0.00405 (ms)\n",
            "[INFO] 15:30:46 - Pass: FixedPoint - 0.00978 (ms)\n",
            "[INFO] 15:30:46 - Pass: RemoveIdentityEquivalent - 0.01097 (ms)\n",
            "[INFO] 15:30:46 - Pass: Optimize1qGatesDecomposition - 0.15116 (ms)\n",
            "[INFO] 15:30:46 - Pass: CommutativeCancellation - 0.06080 (ms)\n",
            "[INFO] 15:30:46 - Pass: GatesInBasis - 0.01597 (ms)\n",
            "[INFO] 15:30:46 - Pass: Depth - 0.02527 (ms)\n",
            "[INFO] 15:30:46 - Pass: FixedPoint - 0.00691 (ms)\n",
            "[INFO] 15:30:46 - Pass: Size - 0.00381 (ms)\n",
            "[INFO] 15:30:46 - Pass: FixedPoint - 0.01001 (ms)\n",
            "[INFO] 15:30:46 - Pass: RemoveIdentityEquivalent - 0.00596 (ms)\n",
            "[INFO] 15:30:46 - Pass: Optimize1qGatesDecomposition - 0.09799 (ms)\n",
            "[INFO] 15:30:46 - Pass: CommutativeCancellation - 0.05412 (ms)\n",
            "[INFO] 15:30:46 - Pass: GatesInBasis - 0.00715 (ms)\n",
            "[INFO] 15:30:46 - Pass: Depth - 0.01287 (ms)\n",
            "[INFO] 15:30:46 - Pass: FixedPoint - 0.00501 (ms)\n",
            "[INFO] 15:30:46 - Pass: Size - 0.00405 (ms)\n",
            "[INFO] 15:30:46 - Pass: FixedPoint - 0.01001 (ms)\n",
            "[INFO] 15:30:46 - Pass: ContainsInstruction - 0.00882 (ms)\n",
            "[INFO] 15:30:46 - Total Transpile Time - 140.13076 (ms)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     ┌───┐ ┌────────────┐                          ┌─┐                  \n",
            "q_0: ┤ H ├─┤ Rx(13.527) ├───■─────────■────────────┤M├──────────────────\n",
            "     ├───┤ ├────────────┤ ┌─┴─┐       │            └╥┘┌─┐               \n",
            "q_1: ┤ H ├─┤ Rx(15.752) ├─┤ X ├──■────┼─────────■───╫─┤M├───────────────\n",
            "     ├───┤┌┴────────────┴┐└───┘┌─┴─┐  │         │   ║ └╥┘┌─┐            \n",
            "q_2: ┤ H ├┤ Rx(0.012811) ├─────┤ X ├──┼────■────┼───╫──╫─┤M├────────────\n",
            "     ├───┤└┬────────────┬┘     └───┘  │  ┌─┴─┐  │   ║  ║ └╥┘┌─┐         \n",
            "q_3: ┤ H ├─┤ Ry(20.482) ├───■─────────┼──┤ X ├──┼───╫──╫──╫─┤M├─────────\n",
            "     ├───┤┌┴────────────┤ ┌─┴─┐       │  └───┘┌─┴─┐ ║  ║  ║ └╥┘┌─┐      \n",
            "q_4: ┤ H ├┤ Ry(-23.585) ├─┤ X ├──■────┼───────┤ X ├─╫──╫──╫──╫─┤M├──────\n",
            "     ├───┤└┬────────────┤ └───┘┌─┴─┐  │       └───┘ ║  ║  ║  ║ └╥┘┌─┐   \n",
            "q_5: ┤ H ├─┤ Ry(20.606) ├──────┤ X ├──┼────■────────╫──╫──╫──╫──╫─┤M├───\n",
            "     ├───┤ └─┬───────┬──┘      └───┘┌─┴─┐  │   ┌─┐  ║  ║  ║  ║  ║ └╥┘   \n",
            "q_6: ┤ H ├───┤ Rz(0) ├──────■───────┤ X ├──┼───┤M├──╫──╫──╫──╫──╫──╫────\n",
            "     ├───┤   ├───────┤    ┌─┴─┐     └───┘┌─┴─┐ └╥┘  ║  ║  ║  ║  ║  ║ ┌─┐\n",
            "q_7: ┤ H ├───┤ Rz(0) ├────┤ X ├──■───────┤ X ├──╫───╫──╫──╫──╫──╫──╫─┤M├\n",
            "     ├───┤┌──┴───────┴───┐└───┘┌─┴─┐ ┌─┐ └───┘  ║   ║  ║  ║  ║  ║  ║ └╥┘\n",
            "q_8: ┤ H ├┤ Rz(0.006075) ├─────┤ X ├─┤M├────────╫───╫──╫──╫──╫──╫──╫──╫─\n",
            "     └───┘└──────────────┘     └───┘ └╥┘        ║   ║  ║  ║  ║  ║  ║  ║ \n",
            "c: 9/═════════════════════════════════╩═════════╩═══╩══╩══╩══╩══╩══╩══╩═\n",
            "                                      8         6   0  1  2  3  4  5  7 \n",
            "Connected to database quantummusic successfully.\n",
            "Tables ensured.\n",
            "Database connection closed.\n"
          ]
        }
      ],
      "source": [
        "result = grade_single_file(\"Bhairav3Rohan.wav\")\n",
        "#result  # Uncomment to see full analysis dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61a5a1e4",
      "metadata": {},
      "source": [
        "# Execute ALL files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40d9fd70",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import concurrent.futures\n",
        "\n",
        "def process_file(file, training_dir, output_dir, grade_single_file):\n",
        "    \"\"\"\n",
        "    Helper function to process a single file: runs grade_single_file and then moves the file.\n",
        "    \"\"\"\n",
        "    file_path = os.path.join(training_dir, file)\n",
        "    print(f\"Processing {file}...\")\n",
        "    try:\n",
        "        # This call is assumed to do the DB insertion internally:\n",
        "        result = grade_single_file(file)\n",
        "        # Move the file to output_dir after processing\n",
        "        shutil.move(file_path, os.path.join(output_dir, file))\n",
        "        print(f\"Processed & moved {file} -> {output_dir}\")\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file}: {e}\")\n",
        "        return None\n",
        "\n",
        "def process_all_files(\n",
        "    grade_single_file,\n",
        "    training_dir=\"data/trainingdata\",\n",
        "    output_dir=\"data/trainingdataoutput\",\n",
        "    num_workers=8\n",
        "):\n",
        "    \"\"\"\n",
        "    Processes all .wav files in `training_dir` using the provided\n",
        "    `grade_single_file` function (which already inserts its results into the database),\n",
        "    then moves the processed files to `output_dir` using parallel execution.\n",
        "    \"\"\"\n",
        "    if not callable(grade_single_file):\n",
        "        raise ValueError(\"grade_single_file must be a callable that accepts a filename.\")\n",
        "\n",
        "    # Ensure the output directory exists\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    # List only .wav files\n",
        "    files = [f for f in os.listdir(training_dir) if f.endswith(\".wav\")]\n",
        "    if not files:\n",
        "        print(f\"No  .wav files found in {training_dir}\")\n",
        "        return\n",
        "\n",
        "    # Use ProcessPoolExecutor to process files in parallel\n",
        "    with concurrent.futures.ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
        "        # Submit each file as a separate job\n",
        "        futures = {\n",
        "            executor.submit(process_file, file, training_dir, output_dir, grade_single_file): file\n",
        "            for file in files\n",
        "        }\n",
        "        # Wait for each future to complete and handle exceptions if any\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            file = futures[future]\n",
        "            try:\n",
        "                _ = future.result()\n",
        "            except Exception as exc:\n",
        "                print(f\"Error processing {file}: {exc}\")\n",
        "\n",
        "    print(\"All .wav files processed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "308656de",
      "metadata": {},
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser(description=\"Process audio files in parallel.\")\n",
        "    parser.add_argument(\n",
        "        \"--workers\",\n",
        "        type=int,\n",
        "        default=8,\n",
        "        help=\"Number of parallel worker processes (e.g., 8 to 16)\"\n",
        "    )\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    process_all_files(\n",
        "        grade_single_file,\n",
        "        training_dir=\"data/trainingdata\",\n",
        "        output_dir=\"data/trainingdataoutput\",\n",
        "        num_workers=args.workers\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "authors": [
      {
        "name": "Rohan Agarwal"
      }
    ],
    "kernelspec": {
      "display_name": "quantumvenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "name": "QuantumMusic_Unified_LargerLUFS",
    "summary": "A multi-module system for analyzing and grading singing performances with LUFS_CHUNK_SIZE for RMS & LUFS, multi-chunk tempo, and short-chunk n_fft fixes."
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
