{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS device for GPU acceleration on Apple Silicon.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/98/ykbpmmjd0pdf8zgd8bc1hhmr0000gn/T/ipykernel_82714/1918989237.py:636: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location=HYPERPARAMS[\"DEVICE\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to database quantummusic_csef. (fetch-only)\n",
      "Database connection closed.\n",
      "Inference for Record ID=3797:\n",
      "  True raga=Bhairav, True quality=2\n",
      "  Predicted raga=Bhairav, predicted quality=2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchaudio.transforms import MelSpectrogram\n",
    "import librosa\n",
    "import librosa.display\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from transformers import ASTModel, ASTConfig\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# MPS DEVICE CHECK\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "    print(\"Using MPS device for GPU acceleration on Apple Silicon.\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    print(\"MPS not available. Falling back to CPU.\")\n",
    "\n",
    "\n",
    "# HYPERPARAMETERS / GLOBAL SETTINGS\n",
    "HYPERPARAMS = {\n",
    "    \"OUTPUT_DIR\": \"data/trainingdataoutput\",\n",
    "    \"DB_LIMIT\": 1000,               # We'll read up to 1000 records from DB\n",
    "    \"BATCH_SIZE\": 8,\n",
    "    \"EPOCHS\": 10,                   # up to 10 or 20 epochs\n",
    "    \"LEARNING_RATE_MAIN\": 5e-5,     # for AST layers\n",
    "    \"LEARNING_RATE_HEAD\": 1e-4,     # for scalar/quantum/final layers\n",
    "    \"WEIGHT_DECAY\": 1e-4,           # L2 regularization\n",
    "    \"DEVICE\": DEVICE,\n",
    "    \"NUM_AST_LAYERS_UNFROZEN\": 4,   # partial unfreezing\n",
    "    \"PATIENCE\": 5,                  # early stopping patience\n",
    "    \"STOP_LIMIT\": 5,                # must train at least this many epochs\n",
    "\n",
    "    # AST model input shape for audio: (freq=128, time=1024) by default\n",
    "    \"AST_FREQ\": 128,\n",
    "    \"AST_TIME\": 1024\n",
    "}\n",
    "\n",
    "\n",
    "# Minimal DB\n",
    "class QuantumMusicDBFetchOnly:\n",
    "    def __init__(self, db_name=\"quantummusic_csef\", host=\"localhost\", user=\"postgres\", password=\"postgres\"):\n",
    "        import psycopg2\n",
    "        self.psycopg2 = psycopg2\n",
    "        self.db_name = db_name\n",
    "        self.host = host\n",
    "        self.user = user\n",
    "        self.password = password\n",
    "        self.conn = None\n",
    "        self.connect()\n",
    "\n",
    "    def connect(self):\n",
    "        try:\n",
    "            import psycopg2\n",
    "            self.conn = psycopg2.connect(\n",
    "                dbname=self.db_name,\n",
    "                host=self.host,\n",
    "                user=self.user,\n",
    "                password=self.password\n",
    "            )\n",
    "            print(f\"Connected to database {self.db_name}. (fetch-only)\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error connecting to database: {e}\")\n",
    "\n",
    "    def close(self):\n",
    "        if self.conn:\n",
    "            self.conn.close()\n",
    "            print(\"Database connection closed.\")\n",
    "\n",
    "    def fetch_limited_analysis_data(self, limit=1000):\n",
    "        \"\"\"\n",
    "        You can customize this query as needed.\n",
    "        \"\"\"\n",
    "        with self.conn.cursor() as cur:\n",
    "            query = \"\"\"\n",
    "                WITH cte AS (\n",
    "                    SELECT\n",
    "                        id,\n",
    "                        analysis_data,\n",
    "                        substring(file_name FROM '\\\\d+')::int AS file_num,\n",
    "                        ROW_NUMBER() OVER (\n",
    "                            PARTITION BY substring(file_name FROM '\\\\d+')::int\n",
    "                            ORDER BY random()\n",
    "                        ) AS rn\n",
    "                    FROM audio_analysis\n",
    "                    WHERE substring(file_name FROM '\\\\d+')::int IN (2, 3, 4, 5)\n",
    "                )\n",
    "                SELECT id, analysis_data\n",
    "                FROM cte\n",
    "                WHERE rn <= 66;\n",
    "            \"\"\"\n",
    "            cur.execute(query)\n",
    "            rows = cur.fetchall()\n",
    "        return rows\n",
    "\n",
    "    def fetch_single_record(self, record_id):\n",
    "        with self.conn.cursor() as cur:\n",
    "            query = \"SELECT id, analysis_data FROM audio_analysis WHERE id = %s\"\n",
    "            cur.execute(query, (record_id,))\n",
    "            row = cur.fetchone()\n",
    "        return row\n",
    "\n",
    "    def fetch_leftover_records(self, exclude_ids):\n",
    "        \"\"\"\n",
    "        Fetch records not in exclude_ids, with rating >=4 based on parse_raga_and_quality.\n",
    "        We'll fetch from the entire table or some subset.\n",
    "        \"\"\"\n",
    "        with self.conn.cursor() as cur:\n",
    "            q = \"SELECT id, analysis_data FROM audio_analysis\"\n",
    "            cur.execute(q)\n",
    "            rows = cur.fetchall()\n",
    "\n",
    "        leftover = []\n",
    "        for r_id, analysis_data in rows:\n",
    "            if r_id in exclude_ids:\n",
    "                continue\n",
    "            fname = analysis_data[\"file_name\"]\n",
    "            base = fname.replace(\".wav\", \"\")\n",
    "            m = re.match(r\"^([A-Za-z]+)([1-5])(.*)\", base)\n",
    "            if m:\n",
    "                rating = int(m.group(2))\n",
    "                if rating >= 4:\n",
    "                    leftover.append((r_id, analysis_data))\n",
    "        return leftover\n",
    "\n",
    "\n",
    "# Convert quantum measurement counts -> probability vector\n",
    "def convert_counts_to_probs_feature(counts_dict, max_bits=10):\n",
    "    total_counts = sum(counts_dict.values())\n",
    "    if total_counts == 0:\n",
    "        return np.zeros(2**max_bits, dtype=np.float32)\n",
    "\n",
    "    feature_vec = np.zeros(2**max_bits, dtype=np.float32)\n",
    "    for bitstring, c in counts_dict.items():\n",
    "        if len(bitstring) > max_bits:\n",
    "            truncated = bitstring[-max_bits:]\n",
    "        else:\n",
    "            truncated = bitstring.rjust(max_bits, '0')\n",
    "        idx = int(truncated, 2)\n",
    "        feature_vec[idx] += c / total_counts\n",
    "    return feature_vec\n",
    "\n",
    "\n",
    "def parse_raga_and_quality(fname: str):\n",
    "    \"\"\"\n",
    "    Using filename format: <RagaName><rating>[optional extras].wav\n",
    "    e.g. Bhairavi4_run2.wav => raga=Bhairavi, rating=4\n",
    "    \"\"\"\n",
    "    base = fname.replace(\".wav\", \"\")\n",
    "    m = re.match(r\"^([A-Za-z]+)([1-5])(.*)\", base)\n",
    "    if not m:\n",
    "        return None, None\n",
    "    raga = m.group(1)\n",
    "    rating = int(m.group(2))\n",
    "    return raga, rating\n",
    "\n",
    "\n",
    "# We include a Resize so all images have shape (1,128,128):\n",
    "transform_img = T.Compose([\n",
    "    T.Resize((128, 128)),\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "def load_image_as_tensor(image_path: str):\n",
    "    if not os.path.exists(image_path):\n",
    "        return None\n",
    "    with Image.open(image_path) as img:\n",
    "        # Convert to mono, then resize & convert to Tensor.\n",
    "        img = img.convert(\"L\")\n",
    "        return transform_img(img)  # => shape [1,128,128]\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# fetch_training_data_v2\n",
    "###############################################################################\n",
    "def fetch_training_data_v2(limit=None):\n",
    "    \"\"\"\n",
    "    This function fetches data from the DB, parses the new JSON structure,\n",
    "    and extracts scalar + quantum features plus a single Mel-spectrogram image.\n",
    "    The rest of the pipeline remains mostly unchanged.\n",
    "    \"\"\"\n",
    "    db = QuantumMusicDBFetchOnly()\n",
    "    rows = db.fetch_limited_analysis_data(limit=limit or HYPERPARAMS[\"DB_LIMIT\"])\n",
    "    db.close()\n",
    "\n",
    "    if not rows:\n",
    "        print(\"No data found in DB.\")\n",
    "        return None, []\n",
    "\n",
    "    # We'll store:\n",
    "    #   1) images_mel   : shape (1, 128, 128)\n",
    "    #   2) scalar_feats : from summary/time_matrices\n",
    "    #   3) quantum_feats: angles + measurement_counts\n",
    "    #   4) labels_raga, labels_quality\n",
    "    #   5) used_ids     : track DB row IDs used\n",
    "\n",
    "    images_mel = []\n",
    "    scalar_feats = []\n",
    "    quantum_feats = []\n",
    "    labels_raga = []\n",
    "    labels_quality = []\n",
    "    used_ids = []\n",
    "\n",
    "    for (record_id, analysis_data) in rows:\n",
    "        used_ids.append(record_id)\n",
    "\n",
    "        # Identify raga, quality from the filename\n",
    "        fname = analysis_data[\"file_name\"]\n",
    "        raga, quality = parse_raga_and_quality(fname)\n",
    "        if raga is None or quality is None:\n",
    "            continue\n",
    "\n",
    "        base_no_ext = fname.replace(\".wav\", \"\")\n",
    "        # Single image: Mel-spectrogram => <basename>_mel.png\n",
    "        mel_path = os.path.join(\"data\", \"analysisoutput\", f\"{base_no_ext}_mel.png\")\n",
    "        mel_img = load_image_as_tensor(mel_path)\n",
    "\n",
    "        # Provide a fallback if not found => shape [1,128,128]\n",
    "        if mel_img is None:\n",
    "            mel_img = torch.zeros((1, 128, 128))\n",
    "\n",
    "        images_mel.append(mel_img)\n",
    "\n",
    "        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        # Pull out features from the new JSON structure\n",
    "        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        summary_dict = analysis_data.get(\"summary\", {})\n",
    "        pitch_dev    = summary_dict.get(\"pitch_deviation\", {})\n",
    "        tnr_dict     = summary_dict.get(\"tone_to_noise_ratio\", {})\n",
    "        praat_dict   = summary_dict.get(\"praat\", {})\n",
    "        dynamics     = summary_dict.get(\"dynamics\", {})\n",
    "\n",
    "        # Pitch deviation => mean, std\n",
    "        avg_dev = pitch_dev.get(\"mean\", 0.0)\n",
    "        std_dev = pitch_dev.get(\"std\", 0.0)\n",
    "\n",
    "        # Tone-to-noise ratio => mean\n",
    "        avg_tnr = tnr_dict.get(\"mean\", 0.0)\n",
    "\n",
    "        # Praat => hnr_mean\n",
    "        avg_hnr = praat_dict.get(\"hnr_mean\", 0.0)\n",
    "\n",
    "        # RMS & LUFS\n",
    "        rms_db_stats = dynamics.get(\"rms_db\", {})\n",
    "        mean_rms_db  = rms_db_stats.get(\"mean\", 0.0)\n",
    "        lufs_stats   = dynamics.get(\"lufs\", {})\n",
    "        mean_lufs    = lufs_stats.get(\"mean\", 0.0)\n",
    "\n",
    "        # time_matrix_small => jitter, shimmer, vibrato, F1\n",
    "        time_matrices = analysis_data.get(\"time_matrices\", {})\n",
    "        time_matrix_small = time_matrices.get(\"time_matrix_small\", [])\n",
    "\n",
    "        jitter_vals = [entry.get(\"jitter\", 0.0) for entry in time_matrix_small]\n",
    "        avg_jitter  = float(np.mean(jitter_vals)) if len(jitter_vals) > 0 else 0.0\n",
    "\n",
    "        shimmer_vals = [entry.get(\"shimmer\", 0.0) for entry in time_matrix_small]\n",
    "        avg_shimmer  = float(np.mean(shimmer_vals)) if len(shimmer_vals) > 0 else 0.0\n",
    "\n",
    "        vib_vals = [entry.get(\"vibrato_rate\", 0.0) for entry in time_matrix_small]\n",
    "        avg_vibrato = float(np.mean(vib_vals)) if len(vib_vals) > 0 else 0.0\n",
    "\n",
    "        f1_vals = [entry.get(\"formants\", {}).get(\"F1\", 0.0) for entry in time_matrix_small]\n",
    "        avg_formant = float(np.mean(f1_vals)) if len(f1_vals) > 0 else 0.0\n",
    "\n",
    "        # Combine scalar features\n",
    "        scalars = [\n",
    "            avg_dev, std_dev,       # pitch_deviation\n",
    "            avg_hnr, avg_tnr,       # HNR, TNR\n",
    "            mean_rms_db, mean_lufs, # overall loudness/dynamics\n",
    "            avg_jitter, avg_shimmer, avg_vibrato, avg_formant\n",
    "        ]\n",
    "\n",
    "        # Quantum features\n",
    "        quantum_dict = analysis_data.get(\"quantum_analysis\", {})\n",
    "        angles = quantum_dict.get(\"scaled_angles\", [])\n",
    "        max_len = 10\n",
    "        angle_arr = np.zeros(max_len, dtype=np.float32)\n",
    "        for i in range(min(max_len, len(angles))):\n",
    "            angle_arr[i] = angles[i]\n",
    "\n",
    "        counts_d = quantum_dict.get(\"measurement_counts\", {})\n",
    "        dist_vec = convert_counts_to_probs_feature(counts_d, max_bits=10)\n",
    "        combined_q = np.concatenate([angle_arr, dist_vec], axis=0)\n",
    "\n",
    "        scalar_feats.append(scalars)\n",
    "        quantum_feats.append(combined_q)\n",
    "        labels_raga.append(raga)\n",
    "        labels_quality.append(quality)\n",
    "\n",
    "    # Stack everything\n",
    "    images_mel    = torch.stack(images_mel)  # shape => (N, 1, 128, 128)\n",
    "    scalar_feats  = np.array(scalar_feats, dtype=np.float32)\n",
    "    quantum_feats = np.array(quantum_feats, dtype=np.float32)\n",
    "\n",
    "    unique_ragas  = sorted(list(set(labels_raga)))\n",
    "    raga_to_idx   = {r: i for i, r in enumerate(unique_ragas)}\n",
    "    label_raga_idx = [raga_to_idx[r] for r in labels_raga]\n",
    "    label_quality_idx = np.array([q - 1 for q in labels_quality], dtype=np.int64)\n",
    "\n",
    "    data_dict = {\n",
    "        \"images_mel\": images_mel,\n",
    "        \"scalar_feats\": scalar_feats,\n",
    "        \"quantum_feats\": quantum_feats,\n",
    "        \"label_raga_idx\": np.array(label_raga_idx, dtype=np.int64),\n",
    "        \"label_quality\": label_quality_idx,\n",
    "        \"raga_to_idx\": raga_to_idx,\n",
    "        \"unique_ragas\": unique_ragas\n",
    "    }\n",
    "\n",
    "    return data_dict, used_ids\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# DATASET & MODEL (USING THE AUDIO SPECTROGRAM TRANSFORMER)\n",
    "###############################################################################\n",
    "\n",
    "class MultiLabelASTDataset(Dataset):\n",
    "    \"\"\"\n",
    "    We store:\n",
    "       1) mel_imgs\n",
    "       2) scalar, quantum arrays\n",
    "       3) raga, quality labels\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dict):\n",
    "        self.mel_imgs   = data_dict[\"images_mel\"]\n",
    "        self.scalars    = data_dict[\"scalar_feats\"]\n",
    "        self.quants     = data_dict[\"quantum_feats\"]\n",
    "        self.raga_lbl   = data_dict[\"label_raga_idx\"]\n",
    "        self.qual_lbl   = data_dict[\"label_quality\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raga_lbl)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        scal  = torch.tensor(self.scalars[idx], dtype=torch.float32)\n",
    "        quan  = torch.tensor(self.quants[idx], dtype=torch.float32)\n",
    "        rag   = torch.tensor(self.raga_lbl[idx], dtype=torch.long)\n",
    "        qual  = torch.tensor(self.qual_lbl[idx], dtype=torch.long)\n",
    "        return (\n",
    "            self.mel_imgs[idx],\n",
    "            scal,\n",
    "            quan,\n",
    "            rag,\n",
    "            qual\n",
    "        )\n",
    "\n",
    "\n",
    "class HybridASTModelV2(nn.Module):\n",
    "    \"\"\"\n",
    "    Updated to take a single 'mel_img' rather than separate mfcc/log.\n",
    "    We resize mel_img to (AST_FREQ x AST_TIME) and feed it to the AST.\n",
    "    Then we fuse scalar + quantum MLP embeddings, produce raga & quality outputs.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_ragas, scalar_dim=11, quantum_dim=(10 + 2**10), num_quality=5, num_unfrozen_layers=0):\n",
    "        super().__init__()\n",
    "        # Load AST config + model\n",
    "        self.config = ASTConfig.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "        self.ast_model = ASTModel.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\", config=self.config)\n",
    "\n",
    "        # Partial unfreeze: freeze everything, unfreeze last N encoder layers\n",
    "        for param in self.ast_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        if num_unfrozen_layers > 0:\n",
    "            total_layers = 12\n",
    "            start_layer = max(0, total_layers - num_unfrozen_layers)\n",
    "            for layer_idx in range(start_layer, total_layers):\n",
    "                for param in self.ast_model.encoder.layer[layer_idx].parameters():\n",
    "                    param.requires_grad = True\n",
    "\n",
    "        # MLP for scalar features\n",
    "        self.scalar_fc = nn.Sequential(\n",
    "            nn.Linear(scalar_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "\n",
    "        # MLP for quantum features\n",
    "        self.quantum_fc = nn.Sequential(\n",
    "            nn.Linear(quantum_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "\n",
    "        # The AST base output is 768 dims\n",
    "        combined_dim = 768 + 64 + 64\n",
    "\n",
    "        self.raga_head = nn.Sequential(\n",
    "            nn.Linear(combined_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, num_ragas)\n",
    "        )\n",
    "        self.quality_head = nn.Sequential(\n",
    "            nn.Linear(combined_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, num_quality)\n",
    "        )\n",
    "\n",
    "    def forward(self, mel_img, scal_in, quan_in):\n",
    "        # mel_img => (B, 1, 128, 128)\n",
    "        B, C, H, W = mel_img.shape\n",
    "        freq = HYPERPARAMS[\"AST_FREQ\"]\n",
    "        time = HYPERPARAMS[\"AST_TIME\"]\n",
    "\n",
    "        # Resize to the shape AST expects => (B, 1, 128, 1024)\n",
    "        mel_resized = F.interpolate(\n",
    "            mel_img, size=(freq, time),\n",
    "            mode='bilinear', align_corners=False\n",
    "        )\n",
    "        # The AST expects (B, freq, time) with no channel dim => remove channel\n",
    "        mel_for_ast = mel_resized.squeeze(1)  # (B, 128, 1024)\n",
    "\n",
    "        outputs = self.ast_model(input_values=mel_for_ast, output_hidden_states=True)\n",
    "        hidden = outputs.last_hidden_state  # (B, seq_len, 768)\n",
    "        ast_embedding = hidden[:, 0, :]     # [CLS] => (B, 768)\n",
    "\n",
    "        emb_scal = self.scalar_fc(scal_in)   # (B, 64)\n",
    "        emb_quan = self.quantum_fc(quan_in)  # (B, 64)\n",
    "\n",
    "        fused = torch.cat([ast_embedding, emb_scal, emb_quan], dim=1)  # (B, 768+64+64=896)\n",
    "\n",
    "        logits_raga    = self.raga_head(fused)    # (B, num_ragas)\n",
    "        logits_quality = self.quality_head(fused) # (B, 5)\n",
    "\n",
    "        return logits_raga, logits_quality\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# TRAIN MODEL\n",
    "###############################################################################\n",
    "def train_model_v2():\n",
    "    data_dict, used_ids = fetch_training_data_v2(limit=HYPERPARAMS[\"DB_LIMIT\"])\n",
    "    if data_dict is None:\n",
    "        print(\"No data to train.\")\n",
    "        return None, None\n",
    "\n",
    "    scalar_feats = data_dict[\"scalar_feats\"]\n",
    "    quantum_feats = data_dict[\"quantum_feats\"]\n",
    "    raga_to_idx = data_dict[\"raga_to_idx\"]\n",
    "    num_ragas = len(raga_to_idx)\n",
    "\n",
    "    # Build dataset\n",
    "    dataset_full = MultiLabelASTDataset(data_dict)\n",
    "    n_samples = len(dataset_full)\n",
    "    test_size = int(0.2 * n_samples)\n",
    "    train_size = n_samples - test_size\n",
    "    train_ds, test_ds = torch.utils.data.random_split(dataset_full, [train_size, test_size])\n",
    "\n",
    "    train_dl = DataLoader(train_ds, batch_size=HYPERPARAMS[\"BATCH_SIZE\"], shuffle=True)\n",
    "    test_dl  = DataLoader(test_ds, batch_size=HYPERPARAMS[\"BATCH_SIZE\"], shuffle=False)\n",
    "\n",
    "    # Build model\n",
    "    model = HybridASTModelV2(\n",
    "        num_ragas=num_ragas,\n",
    "        scalar_dim=scalar_feats.shape[1],\n",
    "        quantum_dim=quantum_feats.shape[1],\n",
    "        num_quality=5,\n",
    "        num_unfrozen_layers=HYPERPARAMS[\"NUM_AST_LAYERS_UNFROZEN\"]\n",
    "    ).to(HYPERPARAMS[\"DEVICE\"])\n",
    "\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=HYPERPARAMS[\"LEARNING_RATE_HEAD\"],\n",
    "        weight_decay=HYPERPARAMS[\"WEIGHT_DECAY\"]\n",
    "    )\n",
    "    crit_ce = nn.CrossEntropyLoss()\n",
    "\n",
    "    EPOCHS = HYPERPARAMS[\"EPOCHS\"]\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    best_test_loss = float(\"inf\")\n",
    "    best_model_state = None\n",
    "    patience = HYPERPARAMS[\"PATIENCE\"]\n",
    "    epochs_no_improve = 0\n",
    "    stop_limit = HYPERPARAMS[\"STOP_LIMIT\"]\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        # TRAIN\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "\n",
    "        for mel_img, scal, quan, rag_lbl, qual_lbl in train_dl:\n",
    "            mel_img = mel_img.to(HYPERPARAMS[\"DEVICE\"])\n",
    "            scal    = scal.to(HYPERPARAMS[\"DEVICE\"])\n",
    "            quan    = quan.to(HYPERPARAMS[\"DEVICE\"])\n",
    "            rag_lbl = rag_lbl.to(HYPERPARAMS[\"DEVICE\"])\n",
    "            qual_lbl= qual_lbl.to(HYPERPARAMS[\"DEVICE\"])\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits_raga, logits_quality = model(mel_img, scal, quan)\n",
    "            loss_raga = crit_ce(logits_raga, rag_lbl)\n",
    "            loss_qual = crit_ce(logits_quality, qual_lbl)\n",
    "            loss = loss_raga + loss_qual\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_dl)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # EVAL\n",
    "        model.eval()\n",
    "        total_test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for mel_img, scal, quan, rag_lbl, qual_lbl in test_dl:\n",
    "                mel_img = mel_img.to(HYPERPARAMS[\"DEVICE\"])\n",
    "                scal    = scal.to(HYPERPARAMS[\"DEVICE\"])\n",
    "                quan    = quan.to(HYPERPARAMS[\"DEVICE\"])\n",
    "                rag_lbl = rag_lbl.to(HYPERPARAMS[\"DEVICE\"])\n",
    "                qual_lbl= qual_lbl.to(HYPERPARAMS[\"DEVICE\"])\n",
    "\n",
    "                logits_raga, logits_quality = model(mel_img, scal, quan)\n",
    "                loss_raga = crit_ce(logits_raga, rag_lbl)\n",
    "                loss_qual = crit_ce(logits_quality, qual_lbl)\n",
    "                total_test_loss += (loss_raga + loss_qual).item()\n",
    "\n",
    "        avg_test_loss = total_test_loss / len(test_dl)\n",
    "        test_losses.append(avg_test_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}: train_loss={avg_train_loss:.4f}, test_loss={avg_test_loss:.4f}\")\n",
    "\n",
    "        # Early Stopping\n",
    "        if avg_test_loss < best_test_loss:\n",
    "            best_test_loss = avg_test_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if (epoch + 1) >= stop_limit and epochs_no_improve >= patience:\n",
    "                print(f\"No improvement for {patience} epochs after epoch {stop_limit}. Stopping at epoch {epoch+1}.\")\n",
    "                break\n",
    "\n",
    "    print(\"Training complete or early stopped.\")\n",
    "\n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        print(\"Loading best model state (lowest test loss).\")\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    # Evaluate final => confusion matrix\n",
    "    all_raga_preds, all_raga_truth = [], []\n",
    "    all_qual_preds, all_qual_truth = [], []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for mel_img, scal, quan, rag_lbl, qual_lbl in test_dl:\n",
    "            mel_img = mel_img.to(HYPERPARAMS[\"DEVICE\"])\n",
    "            scal    = scal.to(HYPERPARAMS[\"DEVICE\"])\n",
    "            quan    = quan.to(HYPERPARAMS[\"DEVICE\"])\n",
    "            rag_lbl = rag_lbl.to(HYPERPARAMS[\"DEVICE\"])\n",
    "            qual_lbl= qual_lbl.to(HYPERPARAMS[\"DEVICE\"])\n",
    "\n",
    "            logits_raga, logits_quality = model(mel_img, scal, quan)\n",
    "            pred_raga = logits_raga.argmax(dim=1).cpu().numpy()\n",
    "            pred_qual = logits_quality.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "            all_raga_preds.extend(pred_raga)\n",
    "            all_raga_truth.extend(rag_lbl.cpu().numpy())\n",
    "            all_qual_preds.extend(pred_qual)\n",
    "            all_qual_truth.extend(qual_lbl.cpu().numpy())\n",
    "\n",
    "    cm_raga = confusion_matrix(all_raga_truth, all_raga_preds)\n",
    "    used_raga_indices = sorted(set(all_raga_truth) | set(all_raga_preds))\n",
    "    idx_to_raga = {v: k for k, v in raga_to_idx.items()}\n",
    "    used_raga_labels = [idx_to_raga[i] for i in used_raga_indices]\n",
    "\n",
    "    disp_raga = ConfusionMatrixDisplay(cm_raga, display_labels=used_raga_labels)\n",
    "    disp_raga.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(\"Raga Confusion Matrix (Best Model)\")\n",
    "    plt.show()\n",
    "\n",
    "    cm_qual = confusion_matrix(all_qual_truth, all_qual_preds)\n",
    "    used_qual_indices = sorted(set(all_qual_truth) | set(all_qual_preds))\n",
    "    used_qual_labels  = [q+1 for q in used_qual_indices]\n",
    "    disp_qual = ConfusionMatrixDisplay(cm_qual, display_labels=used_qual_labels)\n",
    "    disp_qual.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(\"Quality Confusion Matrix (Best Model)\")\n",
    "    plt.show()\n",
    "\n",
    "    # Training curves\n",
    "    plt.figure()\n",
    "    plt.plot(train_losses, label=\"Train Loss\")\n",
    "    plt.plot(test_losses, label=\"Test Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Curves\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Save model + used_ids\n",
    "    os.makedirs(\"data/modeloutput\", exist_ok=True)\n",
    "    checkpoint_path = os.path.join(\"data\", \"modeloutput\", \"trained_model.pt\")\n",
    "    checkpoint_dict = {\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"raga_to_idx\": data_dict[\"raga_to_idx\"],\n",
    "        \"scalar_dim\": scalar_feats.shape[1],\n",
    "        \"quantum_dim\": quantum_feats.shape[1],\n",
    "        \"num_ragas\": len(data_dict[\"raga_to_idx\"]),\n",
    "        \"used_ids\": used_ids\n",
    "    }\n",
    "    torch.save(checkpoint_dict, checkpoint_path)\n",
    "    print(f\"Best model + metadata + used_ids saved to: {checkpoint_path}\")\n",
    "\n",
    "    return model, data_dict[\"raga_to_idx\"]\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# SINGLE-RECORD INFERENCE\n",
    "###############################################################################\n",
    "def run_inference_on_persisted_model(record_id, model_path=\"data/modeloutput/trained_model.pt\"):\n",
    "    \"\"\"\n",
    "    Single-record inference for quick demos, etc.\n",
    "    Uses the new JSON structure to parse features. \n",
    "    \"\"\"\n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(model_path, map_location=HYPERPARAMS[\"DEVICE\"])\n",
    "    model_state = checkpoint[\"model_state\"]\n",
    "    raga_to_idx = checkpoint[\"raga_to_idx\"]\n",
    "    scalar_dim  = checkpoint[\"scalar_dim\"]\n",
    "    quantum_dim = checkpoint[\"quantum_dim\"]\n",
    "    num_ragas   = checkpoint[\"num_ragas\"]\n",
    "\n",
    "    # Build the same model\n",
    "    model = HybridASTModelV2(\n",
    "        num_ragas=num_ragas,\n",
    "        scalar_dim=scalar_dim,\n",
    "        quantum_dim=quantum_dim,\n",
    "        num_quality=5,\n",
    "        num_unfrozen_layers=HYPERPARAMS[\"NUM_AST_LAYERS_UNFROZEN\"]\n",
    "    )\n",
    "    model.load_state_dict(model_state)\n",
    "    model.to(HYPERPARAMS[\"DEVICE\"])\n",
    "    model.eval()\n",
    "\n",
    "    # Fetch record\n",
    "    db = QuantumMusicDBFetchOnly()\n",
    "    row = db.fetch_single_record(record_id)\n",
    "    db.close()\n",
    "\n",
    "    if not row:\n",
    "        print(f\"No record found with ID={record_id}\")\n",
    "        return None, None\n",
    "\n",
    "    _, analysis_data = row\n",
    "    fname = analysis_data[\"file_name\"]\n",
    "    raga_true, quality_true = parse_raga_and_quality(fname)\n",
    "    base_no_ext = fname.replace(\".wav\", \"\")\n",
    "\n",
    "    # Load image => mel\n",
    "    mel_path = os.path.join(\"data\", \"analysisoutput\", f\"{base_no_ext}_mel.png\")\n",
    "    mel_img = load_image_as_tensor(mel_path)\n",
    "    if mel_img is None:\n",
    "        mel_img = torch.zeros((1, 128, 128))\n",
    "\n",
    "    # Extract scalar feats from new JSON\n",
    "    summary_dict = analysis_data.get(\"summary\", {})\n",
    "    pitch_dev    = summary_dict.get(\"pitch_deviation\", {})\n",
    "    tnr_dict     = summary_dict.get(\"tone_to_noise_ratio\", {})\n",
    "    praat_dict   = summary_dict.get(\"praat\", {})\n",
    "    dynamics     = summary_dict.get(\"dynamics\", {})\n",
    "\n",
    "    avg_dev = pitch_dev.get(\"mean\", 0.0)\n",
    "    std_dev = pitch_dev.get(\"std\", 0.0)\n",
    "    avg_tnr = tnr_dict.get(\"mean\", 0.0)\n",
    "    avg_hnr = praat_dict.get(\"hnr_mean\", 0.0)\n",
    "\n",
    "    rms_db_stats = dynamics.get(\"rms_db\", {})\n",
    "    mean_rms_db  = rms_db_stats.get(\"mean\", 0.0)\n",
    "    lufs_stats   = dynamics.get(\"lufs\", {})\n",
    "    mean_lufs    = lufs_stats.get(\"mean\", 0.0)\n",
    "\n",
    "    time_matrices = analysis_data.get(\"time_matrices\", {})\n",
    "    time_matrix_small = time_matrices.get(\"time_matrix_small\", [])\n",
    "\n",
    "    jitter_vals = [\n",
    "        x[\"jitter\"] if (x.get(\"jitter\") is not None) else 0.0\n",
    "        for x in time_matrix_small\n",
    "    ]\n",
    "    shimmer_vals = [\n",
    "        x[\"shimmer\"] if (x.get(\"shimmer\") is not None) else 0.0\n",
    "        for x in time_matrix_small\n",
    "    ]\n",
    "    vib_vals = [\n",
    "        x[\"vibrato_rate\"] if (x.get(\"vibrato_rate\") is not None) else 0.0\n",
    "        for x in time_matrix_small\n",
    "    ]\n",
    "    # formants might be nested\n",
    "    f1_vals = [\n",
    "        x[\"formants\"][\"F1\"] if (x.get(\"formants\") and x[\"formants\"][\"F1\"] is not None) else 0.0\n",
    "        for x in time_matrix_small\n",
    "    ]\n",
    "\n",
    "    avg_jitter  = float(np.mean(jitter_vals))  if jitter_vals else 0.0\n",
    "    avg_shimmer = float(np.mean(shimmer_vals)) if shimmer_vals else 0.0\n",
    "    avg_vibrato = float(np.mean(vib_vals))      if vib_vals else 0.0\n",
    "    avg_formant = float(np.mean(f1_vals))      if f1_vals else 0.0\n",
    "\n",
    "    scalars = [\n",
    "        avg_dev, std_dev, avg_hnr, avg_tnr,\n",
    "        mean_rms_db, mean_lufs,\n",
    "        avg_jitter, avg_shimmer, avg_vibrato, avg_formant\n",
    "    ]\n",
    "\n",
    "    # Quantum\n",
    "    quantum_dict = analysis_data.get(\"quantum_analysis\", {})\n",
    "    angles = quantum_dict.get(\"scaled_angles\", [])\n",
    "    max_len = 10\n",
    "    angle_arr = np.zeros(max_len, dtype=np.float32)\n",
    "    for i in range(min(max_len, len(angles))):\n",
    "        angle_arr[i] = angles[i]\n",
    "\n",
    "    counts_d = quantum_dict.get(\"measurement_counts\", {})\n",
    "    dist_vec = convert_counts_to_probs_feature(counts_d, max_bits=10)\n",
    "    combined_q = np.concatenate([angle_arr, dist_vec], axis=0)\n",
    "\n",
    "    # Prepare for model forward\n",
    "    device = HYPERPARAMS[\"DEVICE\"]\n",
    "    mel_img  = mel_img.unsqueeze(0).to(device)  # (1,1,128,128)\n",
    "    scal_ten = torch.tensor(scalars, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    quan_ten = torch.tensor(combined_q, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits_raga, logits_quality = model(mel_img, scal_ten, quan_ten)\n",
    "        pred_raga_idx = torch.argmax(logits_raga, dim=1).item()\n",
    "        pred_qual_idx = torch.argmax(logits_quality, dim=1).item()\n",
    "\n",
    "    inv_map = {v: k for k, v in raga_to_idx.items()}\n",
    "    pred_raga_str = inv_map.get(pred_raga_idx, \"UnknownRaga\")\n",
    "    pred_quality = pred_qual_idx + 1\n",
    "\n",
    "    print(f\"Inference for Record ID={record_id}:\")\n",
    "    print(f\"  True raga={raga_true}, True quality={quality_true}\")\n",
    "    print(f\"  Predicted raga={pred_raga_str}, predicted quality={pred_quality}\")\n",
    "    return pred_raga_str, pred_quality\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# EXAMPLE main section\n",
    "###############################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage:\n",
    "    # model, raga_map = train_model_v2()\n",
    "    run_inference_on_persisted_model(3797, \"data/modeloutput/trained_model.pt\")\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantumvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
