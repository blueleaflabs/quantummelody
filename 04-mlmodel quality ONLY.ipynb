{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS device for GPU acceleration on Apple Silicon.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/98/ykbpmmjd0pdf8zgd8bc1hhmr0000gn/T/ipykernel_83311/3782387064.py:854: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location=HYPERPARAMS[\"DEVICE\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to database quantummusic_csef. (fetch-only)\n",
      "Database connection closed.\n",
      "Inference for Record ID=2661:\n",
      "  True Quality Rating: 2\n",
      "  Predicted Quality Rating: 2\n",
      "Predicted Quality Rating: 2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import ASTModel, ASTConfig\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "\n",
    "###############################################################################\n",
    "# MPS DEVICE CHECK\n",
    "###############################################################################\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "    print(\"Using MPS device for GPU acceleration on Apple Silicon.\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    print(\"MPS not available. Falling back to CPU.\")\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# HYPERPARAMETERS / GLOBAL SETTINGS\n",
    "###############################################################################\n",
    "HYPERPARAMS = {\n",
    "    \"OUTPUT_DIR\": \"data/trainingdataoutput\",\n",
    "    \"DB_LIMIT\": 1000,               # We'll read up to this many records\n",
    "    \"BATCH_SIZE\": 8,\n",
    "    \"EPOCHS\": 10,                   # up to 10 or 20 epochs\n",
    "    \"LEARNING_RATE_MAIN\": 5e-5,     # for AST layers (if we unfreeze them)\n",
    "    \"LEARNING_RATE_HEAD\": 1e-4,     # for scalar/quantum/final layers\n",
    "    \"WEIGHT_DECAY\": 1e-4,           # L2 regularization\n",
    "    \"DEVICE\": DEVICE,\n",
    "    \"NUM_AST_LAYERS_UNFROZEN\": 4,   # partial unfreezing\n",
    "    \"PATIENCE\": 5,                  # early stopping patience\n",
    "    \"STOP_LIMIT\": 5,                # must train at least this many epochs\n",
    "\n",
    "    # AST model input shape for audio: (freq=128, time=1024) by default\n",
    "    \"AST_FREQ\": 128,\n",
    "    \"AST_TIME\": 1024\n",
    "}\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# DATABASE FETCH LOGIC (FETCH ONLY)\n",
    "###############################################################################\n",
    "class QuantumMusicDBFetchOnly:\n",
    "    \"\"\"\n",
    "    A minimal database utility class which fetches data from a Postgres table\n",
    "    named 'audio_analysis'. This specialized version only supports reading.\n",
    "    \"\"\"\n",
    "    def __init__(self, db_name=\"quantummusic_csef\", host=\"localhost\", user=\"postgres\", password=\"postgres\"):\n",
    "        import psycopg2\n",
    "        self.psycopg2 = psycopg2\n",
    "        self.db_name = db_name\n",
    "        self.host = host\n",
    "        self.user = user\n",
    "        self.password = password\n",
    "        self.conn = None\n",
    "        self.connect()\n",
    "\n",
    "    def connect(self):\n",
    "        try:\n",
    "            import psycopg2\n",
    "            self.conn = psycopg2.connect(\n",
    "                dbname=self.db_name,\n",
    "                host=self.host,\n",
    "                user=self.user,\n",
    "                password=self.password\n",
    "            )\n",
    "            print(f\"Connected to database {self.db_name}. (fetch-only)\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error connecting to database: {e}\")\n",
    "\n",
    "    def close(self):\n",
    "        if self.conn:\n",
    "            self.conn.close()\n",
    "            print(\"Database connection closed.\")\n",
    "\n",
    "    def fetch_limited_analysis_data(self, limit=70):\n",
    "        \"\"\"\n",
    "        Pull a balanced subset:\n",
    "        - Labels 2, 3, 4: all records for valid ragas\n",
    "        - Label 5: limit by singer for each raga, then overall limit\n",
    "        Returns only (id, analysis_data).\n",
    "        \"\"\"\n",
    "        with self.conn.cursor() as cur:\n",
    "            query = \"\"\"\n",
    "                WITH parsed AS (\n",
    "                    -------------------------------------------------------------------\n",
    "                    -- 1) Parse label, raga, and singer from file_name\n",
    "                    -------------------------------------------------------------------\n",
    "                    SELECT \n",
    "                        id,\n",
    "                        analysis_data,\n",
    "                        file_name,\n",
    "                        \n",
    "                        -- label = first run of digits\n",
    "                        CAST(substring(file_name FROM '\\\\d+') AS INT) AS label,\n",
    "                        \n",
    "                        -- raga = everything before first digit\n",
    "                        substring(file_name FROM '^[^0-9]+') AS raga,\n",
    "                        \n",
    "                        -- singer = substring after those digits, up to next digit or end\n",
    "                        substring(file_name FROM '^[^0-9]+\\\\d+([^0-9]+)') AS singer\n",
    "                    FROM audio_analysis\n",
    "                ),\n",
    "                valid_ragas AS (\n",
    "                    -------------------------------------------------------------------\n",
    "                    -- 2) Identify all ragas used by labels 2, 3, or 4\n",
    "                    -------------------------------------------------------------------\n",
    "                    SELECT DISTINCT raga\n",
    "                    FROM parsed\n",
    "                    WHERE label IN (2, 3, 4)\n",
    "                ),\n",
    "                small_labels AS (\n",
    "                    -------------------------------------------------------------------\n",
    "                    -- 3) For labels 2, 3, 4, include ALL rows for valid ragas\n",
    "                    -------------------------------------------------------------------\n",
    "                    SELECT \n",
    "                        id,\n",
    "                        analysis_data,\n",
    "                        label,\n",
    "                        raga,\n",
    "                        singer\n",
    "                    FROM parsed\n",
    "                    WHERE label IN (2, 3, 4)\n",
    "                      AND raga IN (SELECT raga FROM valid_ragas)\n",
    "                ),\n",
    "                label_5_all AS (\n",
    "                    -------------------------------------------------------------------\n",
    "                    -- 4) For label=5, only rows with ragas in valid_ragas\n",
    "                    -------------------------------------------------------------------\n",
    "                    SELECT \n",
    "                        id,\n",
    "                        analysis_data,\n",
    "                        label,\n",
    "                        raga,\n",
    "                        singer\n",
    "                    FROM parsed\n",
    "                    WHERE label = 5\n",
    "                      AND raga IN (SELECT raga FROM valid_ragas)\n",
    "                ),\n",
    "                label_5_singer_limited AS (\n",
    "                    -------------------------------------------------------------------\n",
    "                    -- 5) Limit how many recordings we take per (raga, singer)\n",
    "                    --    e.g. keep up to 5 per (raga, singer)\n",
    "                    -------------------------------------------------------------------\n",
    "                    SELECT\n",
    "                        id,\n",
    "                        analysis_data,\n",
    "                        label,\n",
    "                        raga,\n",
    "                        singer,\n",
    "                        ROW_NUMBER() OVER (\n",
    "                            PARTITION BY raga, singer\n",
    "                            ORDER BY RANDOM()\n",
    "                        ) AS rn_singer\n",
    "                    FROM label_5_all\n",
    "                ),\n",
    "                label_5_filtered AS (\n",
    "                    SELECT \n",
    "                        id,\n",
    "                        analysis_data,\n",
    "                        label,\n",
    "                        raga,\n",
    "                        singer\n",
    "                    FROM label_5_singer_limited\n",
    "                    WHERE rn_singer <= 5  -- e.g., keep up to 5 per (raga, singer)\n",
    "                ),\n",
    "                label_5_final AS (\n",
    "                    -------------------------------------------------------------------\n",
    "                    -- 6) Of the remaining label=5 rows, pick up to 'limit' total\n",
    "                    -------------------------------------------------------------------\n",
    "                    SELECT\n",
    "                        id,\n",
    "                        analysis_data,\n",
    "                        label,\n",
    "                        raga,\n",
    "                        singer,\n",
    "                        ROW_NUMBER() OVER (ORDER BY RANDOM()) AS rn\n",
    "                    FROM label_5_filtered\n",
    "                )\n",
    "                -----------------------------------------------------------------------\n",
    "                -- 7) Combine:\n",
    "                --    - all from small_labels (labels 2, 3, 4)\n",
    "                --    - up to 'limit' from label_5_final\n",
    "                -----------------------------------------------------------------------\n",
    "                SELECT id, analysis_data\n",
    "                FROM small_labels\n",
    "\n",
    "                UNION ALL\n",
    "\n",
    "                SELECT id, analysis_data\n",
    "                FROM label_5_final\n",
    "                WHERE rn <= %(limit_param)s\n",
    "            \"\"\"\n",
    "            \n",
    "            # Execute the query with the provided limit\n",
    "            cur.execute(query, {\"limit_param\": limit})\n",
    "            \n",
    "            # Fetch all rows\n",
    "            rows = cur.fetchall()\n",
    "        \n",
    "        return rows\n",
    "    def fetch_single_record(self, record_id):\n",
    "        with self.conn.cursor() as cur:\n",
    "            query = \"SELECT id, analysis_data FROM audio_analysis WHERE id = %s\"\n",
    "            cur.execute(query, (record_id,))\n",
    "            row = cur.fetchone()\n",
    "        return row\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# HELPER FUNCTIONS\n",
    "###############################################################################\n",
    "def convert_counts_to_probs_feature(counts_dict, max_bits=10):\n",
    "    \"\"\"\n",
    "    Convert a dictionary of quantum measurement counts into a probability\n",
    "    distribution vector of length 2^max_bits.\n",
    "    \"\"\"\n",
    "    total_counts = sum(counts_dict.values())\n",
    "    if total_counts == 0:\n",
    "        return np.zeros(2**max_bits, dtype=np.float32)\n",
    "\n",
    "    feature_vec = np.zeros(2**max_bits, dtype=np.float32)\n",
    "    for bitstring, c in counts_dict.items():\n",
    "        if len(bitstring) > max_bits:\n",
    "            truncated = bitstring[-max_bits:]\n",
    "        else:\n",
    "            truncated = bitstring.rjust(max_bits, '0')\n",
    "        idx = int(truncated, 2)\n",
    "        feature_vec[idx] += c / total_counts\n",
    "    return feature_vec\n",
    "\n",
    "\n",
    "def load_image_as_tensor(image_path: str):\n",
    "    \"\"\"\n",
    "    Loads an image as a 1x128x128 tensor (grayscale). Returns None if not found.\n",
    "    \"\"\"\n",
    "    transform_img = T.Compose([\n",
    "        T.Resize((128, 128)),\n",
    "        T.ToTensor()\n",
    "    ])\n",
    "    if not os.path.exists(image_path):\n",
    "        return None\n",
    "    with Image.open(image_path) as img:\n",
    "        img = img.convert(\"L\")\n",
    "        return transform_img(img)  # => shape [1,128,128]\n",
    "\n",
    "\n",
    "def parse_quality(fname: str):\n",
    "    \"\"\"\n",
    "    Using filename format: <Something><rating>[optional extras].wav\n",
    "    e.g. Bhairavi4_run2.wav => rating=4\n",
    "\n",
    "    Returns an integer rating in [1..5]. If not found, returns None.\n",
    "    \"\"\"\n",
    "    base = fname.replace(\".wav\", \"\")\n",
    "    m = re.match(r\"^([A-Za-z]+)([1-5])(.*)\", base)\n",
    "    if not m:\n",
    "        return None\n",
    "    rating = int(m.group(2))\n",
    "    return rating\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# DATA FETCH + PARSE FOR QUALITY ONLY\n",
    "###############################################################################\n",
    "def fetch_training_data_quality_only(limit=None):\n",
    "    \"\"\"\n",
    "    1) Fetch data from the DB (limit # of rows).\n",
    "    2) Parse the filename for a quality rating in [1..5].\n",
    "    3) Build arrays for:\n",
    "        - mel spectrogram image\n",
    "        - scalar features\n",
    "        - quantum features\n",
    "        - quality label (0..4)\n",
    "    4) Return (data_dict, used_ids).\n",
    "    \"\"\"\n",
    "    db = QuantumMusicDBFetchOnly()\n",
    "    rows = db.fetch_limited_analysis_data(limit=limit or HYPERPARAMS[\"DB_LIMIT\"])\n",
    "    db.close()\n",
    "\n",
    "    if not rows:\n",
    "        print(\"No data found in DB.\")\n",
    "        return None, []\n",
    "\n",
    "    images_mel = []\n",
    "    scalar_feats = []\n",
    "    quantum_feats = []\n",
    "    labels_quality = []\n",
    "    used_ids = []\n",
    "\n",
    "    for (record_id, analysis_data) in rows:\n",
    "        fname = analysis_data[\"file_name\"]\n",
    "        rating = parse_quality(fname)\n",
    "        if rating is None:\n",
    "            # skip if we can't parse a valid rating\n",
    "            continue\n",
    "\n",
    "        used_ids.append(record_id)\n",
    "\n",
    "        # Load mel-spectrogram => shape (1, 128, 128)\n",
    "        base_no_ext = fname.replace(\".wav\", \"\")\n",
    "        mel_path = os.path.join(\"data\", \"analysisoutput\", f\"{base_no_ext}_mel.png\")\n",
    "        mel_img = load_image_as_tensor(mel_path)\n",
    "        if mel_img is None:\n",
    "            mel_img = torch.zeros((1, 128, 128))\n",
    "\n",
    "        images_mel.append(mel_img)\n",
    "\n",
    "        # Extract scalar features\n",
    "        summary_dict = analysis_data.get(\"summary\", {})\n",
    "        pitch_dev = summary_dict.get(\"pitch_deviation\", {})\n",
    "        tnr_dict  = summary_dict.get(\"tone_to_noise_ratio\", {})\n",
    "        praat_dict= summary_dict.get(\"praat\", {})\n",
    "        dynamics  = summary_dict.get(\"dynamics\", {})\n",
    "\n",
    "        avg_dev = pitch_dev.get(\"mean\", 0.0)\n",
    "        std_dev = pitch_dev.get(\"std\", 0.0)\n",
    "        avg_tnr = tnr_dict.get(\"mean\", 0.0)\n",
    "        avg_hnr = praat_dict.get(\"hnr_mean\", 0.0)\n",
    "\n",
    "        rms_db_stats = dynamics.get(\"rms_db\", {})\n",
    "        mean_rms_db  = rms_db_stats.get(\"mean\", 0.0)\n",
    "        lufs_stats   = dynamics.get(\"lufs\", {})\n",
    "        mean_lufs    = lufs_stats.get(\"mean\", 0.0)\n",
    "\n",
    "        time_matrices = analysis_data.get(\"time_matrices\", {})\n",
    "        time_matrix_small = time_matrices.get(\"time_matrix_small\", [])\n",
    "\n",
    "        jitter_vals = [x.get(\"jitter\", 0.0) or 0.0 for x in time_matrix_small]\n",
    "        shimmer_vals= [x.get(\"shimmer\", 0.0) or 0.0 for x in time_matrix_small]\n",
    "        vib_vals    = [x.get(\"vibrato_rate\", 0.0) or 0.0 for x in time_matrix_small]\n",
    "        f1_vals     = [x.get(\"formants\", {}).get(\"F1\", 0.0) or 0.0 for x in time_matrix_small]\n",
    "\n",
    "        avg_jitter  = float(np.mean(jitter_vals))  if jitter_vals else 0.0\n",
    "        avg_shimmer = float(np.mean(shimmer_vals)) if shimmer_vals else 0.0\n",
    "        avg_vibrato = float(np.mean(vib_vals))      if vib_vals else 0.0\n",
    "        avg_formant = float(np.mean(f1_vals))      if f1_vals else 0.0\n",
    "\n",
    "        scalars = [\n",
    "            avg_dev, std_dev,\n",
    "            avg_hnr, avg_tnr,\n",
    "            mean_rms_db, mean_lufs,\n",
    "            avg_jitter, avg_shimmer,\n",
    "            avg_vibrato, avg_formant\n",
    "        ]\n",
    "\n",
    "        # Quantum features\n",
    "        quantum_dict = analysis_data.get(\"quantum_analysis\", {})\n",
    "        angles = quantum_dict.get(\"scaled_angles\", [])\n",
    "        max_len = 10\n",
    "        angle_arr = np.zeros(max_len, dtype=np.float32)\n",
    "        for i in range(min(max_len, len(angles))):\n",
    "            angle_arr[i] = angles[i]\n",
    "\n",
    "        counts_d = quantum_dict.get(\"measurement_counts\", {})\n",
    "        dist_vec = convert_counts_to_probs_feature(counts_d, max_bits=10)\n",
    "        combined_q = np.concatenate([angle_arr, dist_vec], axis=0)\n",
    "\n",
    "        scalar_feats.append(scalars)\n",
    "        quantum_feats.append(combined_q)\n",
    "\n",
    "        # rating in [1..5] => label in [0..4]\n",
    "        labels_quality.append(rating - 1)\n",
    "\n",
    "    if not scalar_feats:\n",
    "        print(\"No valid data after filtering for quality-only training.\")\n",
    "        return None, []\n",
    "\n",
    "    images_mel    = torch.stack(images_mel)\n",
    "    scalar_feats  = np.array(scalar_feats, dtype=np.float32)\n",
    "    quantum_feats = np.array(quantum_feats, dtype=np.float32)\n",
    "    labels_quality= np.array(labels_quality, dtype=np.int64)\n",
    "\n",
    "    data_dict = {\n",
    "        \"images_mel\": images_mel,\n",
    "        \"scalar_feats\": scalar_feats,\n",
    "        \"quantum_feats\": quantum_feats,\n",
    "        \"label_quality\": labels_quality\n",
    "    }\n",
    "\n",
    "    return data_dict, used_ids\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# DATASET CLASSES\n",
    "###############################################################################\n",
    "class QualityOnlyASTDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Each sample has:\n",
    "      1) mel_img\n",
    "      2) scalar feats\n",
    "      3) quantum feats\n",
    "      4) quality label (0â€“4)\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dict):\n",
    "        self.mel_imgs   = data_dict[\"images_mel\"]\n",
    "        self.scalars    = data_dict[\"scalar_feats\"]\n",
    "        self.quants     = data_dict[\"quantum_feats\"]\n",
    "        self.labels_qual= data_dict[\"label_quality\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_qual)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mel = self.mel_imgs[idx]\n",
    "        scal= torch.tensor(self.scalars[idx], dtype=torch.float32)\n",
    "        quan= torch.tensor(self.quants[idx], dtype=torch.float32)\n",
    "        lbl = torch.tensor(self.labels_qual[idx], dtype=torch.long)\n",
    "        return (mel, scal, quan, lbl)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# MODEL CLASSES\n",
    "###############################################################################\n",
    "class QualityOnlyASTModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A single-output model (5 classes) for \"quality\".\n",
    "    We fuse AST(768) + scalar(64) + quantum(64) => 896 => single head (5 outputs).\n",
    "    \"\"\"\n",
    "    def __init__(self, scalar_dim=10, quantum_dim=(10 + 2**10), num_quality=5, num_unfrozen_layers=0):\n",
    "        super().__init__()\n",
    "        self.config = ASTConfig.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "        self.ast_model = ASTModel.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\", config=self.config)\n",
    "\n",
    "        # Freeze AST\n",
    "        for param in self.ast_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Unfreeze last N layers if requested\n",
    "        if num_unfrozen_layers > 0:\n",
    "            total_layers = 12\n",
    "            start_layer = max(0, total_layers - num_unfrozen_layers)\n",
    "            for layer_idx in range(start_layer, total_layers):\n",
    "                for param in self.ast_model.encoder.layer[layer_idx].parameters():\n",
    "                    param.requires_grad = True\n",
    "\n",
    "        # Scalar MLP\n",
    "        self.scalar_fc = nn.Sequential(\n",
    "            nn.Linear(scalar_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "\n",
    "        # Quantum MLP\n",
    "        self.quantum_fc = nn.Sequential(\n",
    "            nn.Linear(quantum_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "\n",
    "        combined_dim = 768 + 64 + 64  # AST(768) + Scalar(64) + Quantum(64)\n",
    "\n",
    "        self.quality_head = nn.Sequential(\n",
    "            nn.Linear(combined_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, num_quality)\n",
    "        )\n",
    "\n",
    "    def forward(self, mel_img, scal_in, quan_in):\n",
    "        B, C, H, W = mel_img.shape\n",
    "        freq = HYPERPARAMS[\"AST_FREQ\"]\n",
    "        time = HYPERPARAMS[\"AST_TIME\"]\n",
    "\n",
    "        # Resize mel => (B,1,128,1024)\n",
    "        mel_resized = F.interpolate(\n",
    "            mel_img, size=(freq, time),\n",
    "            mode='bilinear', align_corners=False\n",
    "        )\n",
    "        # (B,128,1024)\n",
    "        mel_for_ast = mel_resized.squeeze(1)\n",
    "\n",
    "        outputs = self.ast_model(input_values=mel_for_ast, output_hidden_states=True)\n",
    "        hidden = outputs.last_hidden_state  # (B, seq_len, 768)\n",
    "        ast_embedding = hidden[:, 0, :]     # [CLS] => (B,768)\n",
    "\n",
    "        emb_scal = self.scalar_fc(scal_in)   # (B,64)\n",
    "        emb_quan = self.quantum_fc(quan_in)  # (B,64)\n",
    "\n",
    "        fused = torch.cat([ast_embedding, emb_scal, emb_quan], dim=1)  # (B,896)\n",
    "\n",
    "        logits_quality = self.quality_head(fused)\n",
    "        return logits_quality\n",
    "\n",
    "\n",
    "class QualityOnlyASTModelNoQuantum(nn.Module):\n",
    "    \"\"\"\n",
    "    A single-output model (5 classes) for \"quality\", but **no** quantum features.\n",
    "    We fuse AST(768) + scalar(64) => 832 => single head (5 outputs).\n",
    "    \"\"\"\n",
    "    def __init__(self, scalar_dim=10, num_quality=5, num_unfrozen_layers=0):\n",
    "        super().__init__()\n",
    "        self.config = ASTConfig.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "        self.ast_model = ASTModel.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\", config=self.config)\n",
    "\n",
    "        # Freeze AST\n",
    "        for param in self.ast_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Unfreeze last N layers if requested\n",
    "        if num_unfrozen_layers > 0:\n",
    "            total_layers = 12\n",
    "            start_layer = max(0, total_layers - num_unfrozen_layers)\n",
    "            for layer_idx in range(start_layer, total_layers):\n",
    "                for param in self.ast_model.encoder.layer[layer_idx].parameters():\n",
    "                    param.requires_grad = True\n",
    "\n",
    "        # Scalar MLP\n",
    "        self.scalar_fc = nn.Sequential(\n",
    "            nn.Linear(scalar_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "\n",
    "        combined_dim = 768 + 64  # AST(768) + Scalar(64)\n",
    "\n",
    "        self.quality_head = nn.Sequential(\n",
    "            nn.Linear(combined_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, num_quality)\n",
    "        )\n",
    "\n",
    "    def forward(self, mel_img, scal_in):\n",
    "        B, C, H, W = mel_img.shape\n",
    "        freq = HYPERPARAMS[\"AST_FREQ\"]\n",
    "        time = HYPERPARAMS[\"AST_TIME\"]\n",
    "\n",
    "        # Resize mel => (B,1,128,1024)\n",
    "        mel_resized = F.interpolate(\n",
    "            mel_img, size=(freq, time),\n",
    "            mode='bilinear', align_corners=False\n",
    "        )\n",
    "        mel_for_ast = mel_resized.squeeze(1)  # (B,128,1024)\n",
    "\n",
    "        outputs = self.ast_model(input_values=mel_for_ast, output_hidden_states=True)\n",
    "        hidden = outputs.last_hidden_state  # (B, seq_len, 768)\n",
    "        ast_embedding = hidden[:, 0, :]     # [CLS] => (B,768)\n",
    "\n",
    "        emb_scal = self.scalar_fc(scal_in)  # (B,64)\n",
    "\n",
    "        fused = torch.cat([ast_embedding, emb_scal], dim=1)  # (B,832)\n",
    "\n",
    "        logits_quality = self.quality_head(fused)\n",
    "        return logits_quality\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# TRAINING FUNCTION (Quality Only, Quantum vs. NoQuantum)\n",
    "###############################################################################\n",
    "def train_quality_model(include_quantum=True):\n",
    "    \"\"\"\n",
    "    If `include_quantum=True`, build QualityOnlyASTModel (with quantum).\n",
    "    Otherwise, build QualityOnlyASTModelNoQuantum.\n",
    "    \"\"\"\n",
    "    data_dict, used_ids = fetch_training_data_quality_only(limit=HYPERPARAMS[\"DB_LIMIT\"])\n",
    "    if data_dict is None:\n",
    "        print(\"No data for training.\")\n",
    "        return None\n",
    "\n",
    "    # Create dataset\n",
    "    dataset_full = QualityOnlyASTDataset(data_dict)\n",
    "    n_samples = len(dataset_full)\n",
    "    test_size = int(0.2 * n_samples)\n",
    "    train_size = n_samples - test_size\n",
    "    train_ds, test_ds = torch.utils.data.random_split(dataset_full, [train_size, test_size])\n",
    "    train_dl = DataLoader(train_ds, batch_size=HYPERPARAMS[\"BATCH_SIZE\"], shuffle=True)\n",
    "    test_dl  = DataLoader(test_ds, batch_size=HYPERPARAMS[\"BATCH_SIZE\"], shuffle=False)\n",
    "\n",
    "    scalar_dim  = data_dict[\"scalar_feats\"].shape[1]\n",
    "    quantum_dim = data_dict[\"quantum_feats\"].shape[1]  # 10 + 2**10 = 10 + 1024 = 1034\n",
    "\n",
    "    # Build model\n",
    "    if include_quantum:\n",
    "        model = QualityOnlyASTModel(\n",
    "            scalar_dim=scalar_dim,\n",
    "            quantum_dim=quantum_dim,\n",
    "            num_quality=5,\n",
    "            num_unfrozen_layers=HYPERPARAMS[\"NUM_AST_LAYERS_UNFROZEN\"]\n",
    "        ).to(HYPERPARAMS[\"DEVICE\"])\n",
    "    else:\n",
    "        model = QualityOnlyASTModelNoQuantum(\n",
    "            scalar_dim=scalar_dim,\n",
    "            num_quality=5,\n",
    "            num_unfrozen_layers=HYPERPARAMS[\"NUM_AST_LAYERS_UNFROZEN\"]\n",
    "        ).to(HYPERPARAMS[\"DEVICE\"])\n",
    "\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=HYPERPARAMS[\"LEARNING_RATE_HEAD\"],\n",
    "        weight_decay=HYPERPARAMS[\"WEIGHT_DECAY\"]\n",
    "    )\n",
    "    crit_ce = nn.CrossEntropyLoss()\n",
    "\n",
    "    EPOCHS = HYPERPARAMS[\"EPOCHS\"]\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    best_test_loss = float(\"inf\")\n",
    "    best_model_state = None\n",
    "    patience = HYPERPARAMS[\"PATIENCE\"]\n",
    "    epochs_no_improve = 0\n",
    "    stop_limit = HYPERPARAMS[\"STOP_LIMIT\"]\n",
    "\n",
    "    label_str = \"Quantum\" if include_quantum else \"NoQuantum\"\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        # TRAIN\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        for mel_img, scal, quan, qual_lbl in train_dl:\n",
    "            mel_img = mel_img.to(HYPERPARAMS[\"DEVICE\"])\n",
    "            scal    = scal.to(HYPERPARAMS[\"DEVICE\"])\n",
    "            quan    = quan.to(HYPERPARAMS[\"DEVICE\"])\n",
    "            qual_lbl= qual_lbl.to(HYPERPARAMS[\"DEVICE\"])\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if include_quantum:\n",
    "                logits_quality = model(mel_img, scal, quan)\n",
    "            else:\n",
    "                logits_quality = model(mel_img, scal)\n",
    "\n",
    "            loss = crit_ce(logits_quality, qual_lbl)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_dl)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # EVAL\n",
    "        model.eval()\n",
    "        total_test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for mel_img, scal, quan, qual_lbl in test_dl:\n",
    "                mel_img = mel_img.to(HYPERPARAMS[\"DEVICE\"])\n",
    "                scal    = scal.to(HYPERPARAMS[\"DEVICE\"])\n",
    "                quan    = quan.to(HYPERPARAMS[\"DEVICE\"])\n",
    "                qual_lbl= qual_lbl.to(HYPERPARAMS[\"DEVICE\"])\n",
    "\n",
    "                if include_quantum:\n",
    "                    logits_quality = model(mel_img, scal, quan)\n",
    "                else:\n",
    "                    logits_quality = model(mel_img, scal)\n",
    "\n",
    "                loss = crit_ce(logits_quality, qual_lbl)\n",
    "                total_test_loss += loss.item()\n",
    "\n",
    "        avg_test_loss = total_test_loss / len(test_dl)\n",
    "        test_losses.append(avg_test_loss)\n",
    "\n",
    "        print(f\"[{label_str}] Epoch {epoch+1}/{EPOCHS}: train_loss={avg_train_loss:.4f}, test_loss={avg_test_loss:.4f}\")\n",
    "\n",
    "        # Early Stopping\n",
    "        if avg_test_loss < best_test_loss:\n",
    "            best_test_loss = avg_test_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if (epoch + 1) >= stop_limit and epochs_no_improve >= patience:\n",
    "                print(f\"[{label_str}] No improvement for {patience} epochs after epoch {stop_limit}. Stopping.\")\n",
    "                break\n",
    "\n",
    "    print(f\"[{label_str}] Training complete or early stopped.\")\n",
    "\n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    # Evaluate final => confusion matrix\n",
    "    all_qual_preds, all_qual_truth = [], []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for mel_img, scal, quan, qual_lbl in test_dl:\n",
    "            mel_img = mel_img.to(HYPERPARAMS[\"DEVICE\"])\n",
    "            scal    = scal.to(HYPERPARAMS[\"DEVICE\"])\n",
    "            quan    = quan.to(HYPERPARAMS[\"DEVICE\"])\n",
    "            qual_lbl= qual_lbl.to(HYPERPARAMS[\"DEVICE\"])\n",
    "\n",
    "            if include_quantum:\n",
    "                logits_quality = model(mel_img, scal, quan)\n",
    "            else:\n",
    "                logits_quality = model(mel_img, scal)\n",
    "\n",
    "            pred_qual = logits_quality.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "            all_qual_preds.extend(pred_qual)\n",
    "            all_qual_truth.extend(qual_lbl.cpu().numpy())\n",
    "\n",
    "    cm_qual = confusion_matrix(all_qual_truth, all_qual_preds)\n",
    "    used_qual_indices = sorted(set(all_qual_truth) | set(all_qual_preds))\n",
    "    # Our labels go 0..4 internally => display as 1..5\n",
    "    used_qual_labels  = [q+1 for q in used_qual_indices]\n",
    "    disp_qual = ConfusionMatrixDisplay(cm_qual, display_labels=used_qual_labels)\n",
    "    disp_qual.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(f\"[{label_str}] Quality Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "    # Accuracy\n",
    "    qual_acc = accuracy_score(all_qual_truth, all_qual_preds)\n",
    "    print(f\"[{label_str}] Final test accuracy: {qual_acc:.4f}\")\n",
    "\n",
    "    # Training curves\n",
    "    plt.figure()\n",
    "    plt.plot(train_losses, label=\"Train Loss\")\n",
    "    plt.plot(test_losses, label=\"Test Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"[{label_str}] Training Curves\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Save model + used_ids\n",
    "    os.makedirs(\"data/modeloutput\", exist_ok=True)\n",
    "    if include_quantum:\n",
    "        model_fname = \"trained_quality_model_quantum.pt\"\n",
    "    else:\n",
    "        model_fname = \"trained_quality_model_noquantum.pt\"\n",
    "\n",
    "    checkpoint_path = os.path.join(\"data\", \"modeloutput\", model_fname)\n",
    "\n",
    "    checkpoint_dict = {\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"scalar_dim\": scalar_dim,\n",
    "        \"quantum_dim\": quantum_dim,\n",
    "        \"used_ids\": used_ids\n",
    "    }\n",
    "    torch.save(checkpoint_dict, checkpoint_path)\n",
    "    print(f\"[{label_str}] Model + metadata saved to: {checkpoint_path}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# COMPARISON FUNCTION: QUANTUM VS. NO-QUANTUM\n",
    "###############################################################################\n",
    "def compare_quality_models():\n",
    "    \"\"\"\n",
    "    1) Fetch the same dataset (quality-only).\n",
    "    2) Split 20% test.\n",
    "    3) Load quantum model + no-quantum model from disk.\n",
    "    4) Evaluate both on the same test set -> confusion matrix, accuracy.\n",
    "    5) Print comparison.\n",
    "    \"\"\"\n",
    "    data_dict, used_ids = fetch_training_data_quality_only(limit=HYPERPARAMS[\"DB_LIMIT\"])\n",
    "    if data_dict is None:\n",
    "        print(\"[Compare] No data available for comparison.\")\n",
    "        return\n",
    "\n",
    "    dataset_full = QualityOnlyASTDataset(data_dict)\n",
    "    n_samples = len(dataset_full)\n",
    "    test_size = int(0.2 * n_samples)\n",
    "    train_size = n_samples - test_size\n",
    "    # We only want to evaluate on the test set. We do not re-train here.\n",
    "    _, test_ds = torch.utils.data.random_split(dataset_full, [train_size, test_size])\n",
    "    test_dl = DataLoader(test_ds, batch_size=HYPERPARAMS[\"BATCH_SIZE\"], shuffle=False)\n",
    "\n",
    "    scalar_dim = data_dict[\"scalar_feats\"].shape[1]\n",
    "    quantum_dim= data_dict[\"quantum_feats\"].shape[1]\n",
    "\n",
    "    # Load quantum model\n",
    "    q_ckpt_path = \"data/modeloutput/trained_quality_model_quantum.pt\"\n",
    "    if not os.path.exists(q_ckpt_path):\n",
    "        print(f\"[Compare] Quantum model not found at {q_ckpt_path}\")\n",
    "        return\n",
    "    q_ckpt = torch.load(q_ckpt_path, map_location=HYPERPARAMS[\"DEVICE\"])\n",
    "\n",
    "    model_q = QualityOnlyASTModel(\n",
    "        scalar_dim=scalar_dim,\n",
    "        quantum_dim=quantum_dim,\n",
    "        num_quality=5,\n",
    "        num_unfrozen_layers=HYPERPARAMS[\"NUM_AST_LAYERS_UNFROZEN\"]\n",
    "    )\n",
    "    model_q.load_state_dict(q_ckpt[\"model_state\"])\n",
    "    model_q.to(HYPERPARAMS[\"DEVICE\"])\n",
    "    model_q.eval()\n",
    "\n",
    "    # Load no-quantum model\n",
    "    nq_ckpt_path = \"data/modeloutput/trained_quality_model_noquantum.pt\"\n",
    "    if not os.path.exists(nq_ckpt_path):\n",
    "        print(f\"[Compare] No-Quantum model not found at {nq_ckpt_path}\")\n",
    "        return\n",
    "    nq_ckpt = torch.load(nq_ckpt_path, map_location=HYPERPARAMS[\"DEVICE\"])\n",
    "\n",
    "    model_nq = QualityOnlyASTModelNoQuantum(\n",
    "        scalar_dim=scalar_dim,\n",
    "        num_quality=5,\n",
    "        num_unfrozen_layers=HYPERPARAMS[\"NUM_AST_LAYERS_UNFROZEN\"]\n",
    "    )\n",
    "    model_nq.load_state_dict(nq_ckpt[\"model_state\"])\n",
    "    model_nq.to(HYPERPARAMS[\"DEVICE\"])\n",
    "    model_nq.eval()\n",
    "\n",
    "    all_qual_truth = []\n",
    "    preds_qual_q, preds_qual_nq = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for mel_img, scal, quan, qual_lbl in test_dl:\n",
    "            mel_img = mel_img.to(HYPERPARAMS[\"DEVICE\"])\n",
    "            scal    = scal.to(HYPERPARAMS[\"DEVICE\"])\n",
    "            quan    = quan.to(HYPERPARAMS[\"DEVICE\"])\n",
    "            qual_lbl= qual_lbl.to(HYPERPARAMS[\"DEVICE\"])\n",
    "\n",
    "            # Quantum model\n",
    "            logits_q = model_q(mel_img, scal, quan)\n",
    "            pred_q   = logits_q.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "            # No-Quantum model\n",
    "            logits_nq = model_nq(mel_img, scal)\n",
    "            pred_nq   = logits_nq.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "            all_qual_truth.extend(qual_lbl.cpu().numpy())\n",
    "            preds_qual_q.extend(pred_q)\n",
    "            preds_qual_nq.extend(pred_nq)\n",
    "\n",
    "    # Accuracy\n",
    "    acc_q  = accuracy_score(all_qual_truth, preds_qual_q)\n",
    "    acc_nq = accuracy_score(all_qual_truth, preds_qual_nq)\n",
    "\n",
    "    print(\"\\n=== Quality-Only: Quantum vs. No-Quantum ===\")\n",
    "    print(f\"Quality Accuracy (Quantum):    {acc_q:.4f}\")\n",
    "    print(f\"Quality Accuracy (No-Quantum): {acc_nq:.4f}\")\n",
    "    diff = (acc_q - acc_nq)*100\n",
    "    print(f\"Difference in Accuracy = {diff:.2f} % points\")\n",
    "\n",
    "\n",
    "def run_quality_inference(record_id, model_path=\"data/modeloutput/trained_quality_model_quantum.pt\", include_quantum=True):\n",
    "    \"\"\"\n",
    "    Single-record inference using persisted Quality-only model.\n",
    "    Set include_quantum=True for quantum-enabled models.\n",
    "    \"\"\"\n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(model_path, map_location=HYPERPARAMS[\"DEVICE\"])\n",
    "    model_state = checkpoint[\"model_state\"]\n",
    "    scalar_dim  = checkpoint[\"scalar_dim\"]\n",
    "    quantum_dim = checkpoint.get(\"quantum_dim\", 0)\n",
    "\n",
    "    # Build the appropriate model\n",
    "    if include_quantum:\n",
    "        model = QualityOnlyASTModel(\n",
    "            scalar_dim=scalar_dim,\n",
    "            quantum_dim=quantum_dim,\n",
    "            num_quality=5,\n",
    "            num_unfrozen_layers=HYPERPARAMS[\"NUM_AST_LAYERS_UNFROZEN\"]\n",
    "        )\n",
    "    else:\n",
    "        model = QualityOnlyASTModelNoQuantum(\n",
    "            scalar_dim=scalar_dim,\n",
    "            num_quality=5,\n",
    "            num_unfrozen_layers=HYPERPARAMS[\"NUM_AST_LAYERS_UNFROZEN\"]\n",
    "        )\n",
    "\n",
    "    model.load_state_dict(model_state)\n",
    "    model.to(HYPERPARAMS[\"DEVICE\"])\n",
    "    model.eval()\n",
    "\n",
    "    # Fetch single record from DB\n",
    "    db = QuantumMusicDBFetchOnly()\n",
    "    row = db.fetch_single_record(record_id)\n",
    "    db.close()\n",
    "\n",
    "    if not row:\n",
    "        print(f\"No record found with ID={record_id}\")\n",
    "        return None\n",
    "\n",
    "    _, analysis_data = row\n",
    "    fname = analysis_data[\"file_name\"]\n",
    "    true_quality = parse_quality(fname)\n",
    "\n",
    "    # Load mel-spectrogram\n",
    "    base_no_ext = fname.replace(\".wav\", \"\")\n",
    "    mel_path = os.path.join(\"data\", \"analysisoutput\", f\"{base_no_ext}_mel.png\")\n",
    "    mel_img = load_image_as_tensor(mel_path)\n",
    "    if mel_img is None:\n",
    "        mel_img = torch.zeros((1, 128, 128))\n",
    "\n",
    "    # Extract scalar features\n",
    "    summary_dict = analysis_data.get(\"summary\", {})\n",
    "    pitch_dev = summary_dict.get(\"pitch_deviation\", {})\n",
    "    tnr_dict  = summary_dict.get(\"tone_to_noise_ratio\", {})\n",
    "    praat_dict= summary_dict.get(\"praat\", {})\n",
    "    dynamics  = summary_dict.get(\"dynamics\", {})\n",
    "\n",
    "    avg_dev = pitch_dev.get(\"mean\", 0.0)\n",
    "    std_dev = pitch_dev.get(\"std\", 0.0)\n",
    "    avg_tnr = tnr_dict.get(\"mean\", 0.0)\n",
    "    avg_hnr = praat_dict.get(\"hnr_mean\", 0.0)\n",
    "\n",
    "    rms_db_stats = dynamics.get(\"rms_db\", {})\n",
    "    mean_rms_db  = rms_db_stats.get(\"mean\", 0.0)\n",
    "    lufs_stats   = dynamics.get(\"lufs\", {})\n",
    "    mean_lufs    = lufs_stats.get(\"mean\", 0.0)\n",
    "\n",
    "    time_matrices = analysis_data.get(\"time_matrices\", {})\n",
    "    time_matrix_small = time_matrices.get(\"time_matrix_small\", [])\n",
    "\n",
    "    jitter_vals = [x.get(\"jitter\", 0.0) or 0.0 for x in time_matrix_small]\n",
    "    shimmer_vals= [x.get(\"shimmer\", 0.0) or 0.0 for x in time_matrix_small]\n",
    "    vib_vals    = [x.get(\"vibrato_rate\", 0.0) or 0.0 for x in time_matrix_small]\n",
    "    f1_vals     = [x.get(\"formants\", {}).get(\"F1\", 0.0) or 0.0 for x in time_matrix_small]\n",
    "\n",
    "    avg_jitter  = float(np.mean(jitter_vals))  if jitter_vals else 0.0\n",
    "    avg_shimmer = float(np.mean(shimmer_vals)) if shimmer_vals else 0.0\n",
    "    avg_vibrato = float(np.mean(vib_vals))     if vib_vals else 0.0\n",
    "    avg_formant = float(np.mean(f1_vals))      if f1_vals else 0.0\n",
    "\n",
    "    scalar_features = [\n",
    "        avg_dev, std_dev, avg_hnr, avg_tnr,\n",
    "        mean_rms_db, mean_lufs,\n",
    "        avg_jitter, avg_shimmer,\n",
    "        avg_vibrato, avg_formant\n",
    "    ]\n",
    "\n",
    "    # Quantum features (if applicable)\n",
    "    combined_quantum = None\n",
    "    if include_quantum:\n",
    "        quantum_dict = analysis_data.get(\"quantum_analysis\", {})\n",
    "        angles = quantum_dict.get(\"scaled_angles\", [])\n",
    "        max_len = 10\n",
    "        angle_arr = np.zeros(max_len, dtype=np.float32)\n",
    "        for i in range(min(max_len, len(angles))):\n",
    "            angle_arr[i] = angles[i]\n",
    "\n",
    "        counts_d = quantum_dict.get(\"measurement_counts\", {})\n",
    "        dist_vec = convert_counts_to_probs_feature(counts_d, max_bits=10)\n",
    "        combined_quantum = np.concatenate([angle_arr, dist_vec], axis=0)\n",
    "\n",
    "    # Prepare inputs\n",
    "    device = HYPERPARAMS[\"DEVICE\"]\n",
    "    mel_img_tensor = mel_img.unsqueeze(0).to(device)  # [1,1,128,128]\n",
    "    scalar_tensor  = torch.tensor(scalar_features, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if include_quantum:\n",
    "            quantum_tensor = torch.tensor(combined_quantum, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            logits_quality = model(mel_img_tensor, scalar_tensor, quantum_tensor)\n",
    "        else:\n",
    "            logits_quality = model(mel_img_tensor, scalar_tensor)\n",
    "\n",
    "        predicted_quality_idx = logits_quality.argmax(dim=1).item()\n",
    "        predicted_quality = predicted_quality_idx + 1  # label (0..4) => rating (1..5)\n",
    "\n",
    "    print(f\"Inference for Record ID={record_id}:\")\n",
    "    print(f\"  True Quality Rating: {true_quality}\")\n",
    "    print(f\"  Predicted Quality Rating: {predicted_quality}\")\n",
    "\n",
    "    return predicted_quality\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# EXAMPLE MAIN\n",
    "###############################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Train quantum\n",
    "    #model_q = train_quality_model(include_quantum=True)\n",
    "\n",
    "    # 2) Train no-quantum\n",
    "    #model_nq = train_quality_model(include_quantum=False)\n",
    "\n",
    "    # 3) Compare\n",
    "    #compare_quality_models()\n",
    "\n",
    "\n",
    "    pred_quality = run_quality_inference(\n",
    "    record_id=2661, \n",
    "    model_path=\"data/modeloutput/trained_quality_model_quantum_final.pt\",\n",
    "    include_quantum=True\n",
    "    )\n",
    "    print(f\"Predicted Quality Rating: {pred_quality}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantumvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
