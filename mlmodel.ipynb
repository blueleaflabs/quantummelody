{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS device for GPU acceleration on Apple Silicon.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/98/ykbpmmjd0pdf8zgd8bc1hhmr0000gn/T/ipykernel_3849/806627931.py:969: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location=HYPERPARAMS[\"DEVICE\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to database quantummusic. (fetch-only)\n",
      "Database connection closed.\n",
      "Inference for Record ID=2150:\n",
      "  True raga=Bhairav, True quality=4\n",
      "  Predicted raga=Bhairav, predicted quality=4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchaudio.transforms import MelSpectrogram\n",
    "import librosa\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from transformers import ASTModel, ASTConfig\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# MPS DEVICE CHECK\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "    print(\"Using MPS device for GPU acceleration on Apple Silicon.\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    print(\"MPS not available. Falling back to CPU.\")\n",
    "\n",
    "\n",
    "\n",
    "# HYPERPARAMETERS / GLOBAL SETTINGS\n",
    "\n",
    "HYPERPARAMS = {\n",
    "    \"OUTPUT_DIR\": \"data/trainingdataoutput\",\n",
    "    \"DB_LIMIT\": 1000,               # We'll read up to 1000 records from DB\n",
    "    \"BATCH_SIZE\": 8,\n",
    "    \"EPOCHS\": 10,                   # up to 10 or 20 epochs\n",
    "    \"LEARNING_RATE_MAIN\": 5e-5,     # for AST layers\n",
    "    \"LEARNING_RATE_HEAD\": 1e-4,     # for scalar/quantum/final layers\n",
    "    \"WEIGHT_DECAY\": 1e-4,           # L2 regularization\n",
    "    \"DEVICE\": DEVICE,\n",
    "    \"NUM_AST_LAYERS_UNFROZEN\": 4,   # partial unfreezing\n",
    "    \"PATIENCE\": 5,                  # early stopping patience\n",
    "    \"STOP_LIMIT\": 5,                # must train at least this many epochs\n",
    "    \"AST_FREQ\": 128,\n",
    "    \"AST_TIME\": 1024\n",
    "}\n",
    "\n",
    "\n",
    "# Minimal DB\n",
    "\n",
    "class QuantumMusicDBFetchOnly:\n",
    "    def __init__(self, db_name=\"quantummusic\", host=\"localhost\", user=\"postgres\", password=\"postgres\"):\n",
    "        import psycopg2\n",
    "        self.psycopg2 = psycopg2\n",
    "        self.db_name = db_name\n",
    "        self.host = host\n",
    "        self.user = user\n",
    "        self.password = password\n",
    "        self.conn = None\n",
    "        self.connect()\n",
    "\n",
    "    def connect(self):\n",
    "        try:\n",
    "            import psycopg2\n",
    "            self.conn = psycopg2.connect(\n",
    "                dbname=self.db_name,\n",
    "                host=self.host,\n",
    "                user=self.user,\n",
    "                password=self.password\n",
    "            )\n",
    "            print(f\"Connected to database {self.db_name}. (fetch-only)\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error connecting to database: {e}\")\n",
    "\n",
    "    def close(self):\n",
    "        if self.conn:\n",
    "            self.conn.close()\n",
    "            print(\"Database connection closed.\")\n",
    "\n",
    "    def fetch_limited_analysis_data(self, limit=1000):\n",
    "        \"\"\"\n",
    "        You can customize this query as needed.\n",
    "        \"\"\"\n",
    "        with self.conn.cursor() as cur:\n",
    "            query = \"\"\"\n",
    "                WITH cte AS (\n",
    "                    SELECT\n",
    "                        id,\n",
    "                        analysis_data,\n",
    "                        substring(file_name FROM '\\\\d+')::int AS file_num,\n",
    "                        ROW_NUMBER() OVER (\n",
    "                            PARTITION BY substring(file_name FROM '\\\\d+')::int\n",
    "                            ORDER BY random()\n",
    "                        ) AS rn\n",
    "                    FROM audio_analysis\n",
    "                    WHERE substring(file_name FROM '\\\\d+')::int IN (2, 3, 4, 5)\n",
    "                )\n",
    "                SELECT id, analysis_data\n",
    "                FROM cte\n",
    "                WHERE rn <= 42;\n",
    "            \"\"\"\n",
    "            cur.execute(query)\n",
    "            rows = cur.fetchall()\n",
    "        return rows\n",
    "\n",
    "    def fetch_single_record(self, record_id):\n",
    "        with self.conn.cursor() as cur:\n",
    "            query = \"SELECT id, analysis_data FROM audio_analysis WHERE id = %s\"\n",
    "            cur.execute(query, (record_id,))\n",
    "            row = cur.fetchone()\n",
    "        return row\n",
    "\n",
    "    def fetch_leftover_records(self, exclude_ids):\n",
    "        \"\"\"\n",
    "        Fetch records not in exclude_ids, with rating >=4 based on parse_raga_and_quality.\n",
    "        We'll fetch from the entire table or some subset.\n",
    "        \"\"\"\n",
    "        with self.conn.cursor() as cur:\n",
    "            # For simplicity, fetch everything. Then filter in Python\n",
    "            # Adjust for large DB if needed\n",
    "            q = \"SELECT id, analysis_data FROM audio_analysis\"\n",
    "            cur.execute(q)\n",
    "            rows = cur.fetchall()\n",
    "\n",
    "        leftover = []\n",
    "        for r_id, analysis_data in rows:\n",
    "            if r_id in exclude_ids:\n",
    "                continue\n",
    "            fname = analysis_data[\"file_name\"]\n",
    "            base = fname.replace(\".wav\", \"\")\n",
    "            m = re.match(r\"^([A-Za-z]+)([1-5])(.*)\", base)\n",
    "            if m:\n",
    "                rating = int(m.group(2))\n",
    "                if rating >= 4:\n",
    "                    leftover.append((r_id, analysis_data))\n",
    "        return leftover\n",
    "\n",
    "\n",
    "# Convert quantum measurement counts -> probability vector\n",
    "\n",
    "def convert_counts_to_probs_feature(counts_dict, max_bits=10):\n",
    "    total_counts = sum(counts_dict.values())\n",
    "    if total_counts == 0:\n",
    "        return np.zeros(2**max_bits, dtype=np.float32)\n",
    "\n",
    "    feature_vec = np.zeros(2**max_bits, dtype=np.float32)\n",
    "    for bitstring, c in counts_dict.items():\n",
    "        if len(bitstring) > max_bits:\n",
    "            truncated = bitstring[-max_bits:]\n",
    "        else:\n",
    "            truncated = bitstring.rjust(max_bits, '0')\n",
    "        idx = int(truncated, 2)\n",
    "        feature_vec[idx] += c / total_counts\n",
    "    return feature_vec\n",
    "\n",
    "\n",
    "# parse_raga_and_quality\n",
    "\n",
    "def parse_raga_and_quality(fname: str):\n",
    "    base = fname.replace(\".wav\", \"\")\n",
    "    m = re.match(r\"^([A-Za-z]+)([1-5])(.*)\", base)\n",
    "    if not m:\n",
    "        return None, None\n",
    "    raga = m.group(1)\n",
    "    rating = int(m.group(2))\n",
    "    return raga, rating\n",
    "\n",
    "\n",
    "# load_image_as_tensor\n",
    "\n",
    "transform_img = T.Compose([\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "def load_image_as_tensor(image_path: str):\n",
    "    if not os.path.exists(image_path):\n",
    "        return None\n",
    "    with Image.open(image_path) as img:\n",
    "        img = img.convert(\"L\")\n",
    "    return transform_img(img)\n",
    "\n",
    "\n",
    "# fetch_training_data_v2\n",
    "\n",
    "def fetch_training_data_v2(limit=None):\n",
    "    \"\"\"\n",
    "    We'll also store the DB record IDs used in 'used_ids' so we can exclude them later.\n",
    "    \"\"\"\n",
    "    db = QuantumMusicDBFetchOnly()\n",
    "    rows = db.fetch_limited_analysis_data(limit=limit or HYPERPARAMS[\"DB_LIMIT\"])\n",
    "    db.close()\n",
    "\n",
    "    if not rows:\n",
    "        print(\"No data found in DB.\")\n",
    "        return None, []\n",
    "\n",
    "    images_mfcc = []\n",
    "    images_log = []\n",
    "    scalar_feats = []\n",
    "    quantum_feats = []\n",
    "    labels_raga = []\n",
    "    labels_quality = []\n",
    "    used_ids = []\n",
    "\n",
    "    for (record_id, analysis_data) in rows:\n",
    "        used_ids.append(record_id)\n",
    "\n",
    "        fname = analysis_data[\"file_name\"]\n",
    "        base_no_ext = fname.replace(\".wav\", \"\")\n",
    "        raga, quality = parse_raga_and_quality(fname)\n",
    "        if raga is None or quality is None:\n",
    "            continue\n",
    "\n",
    "        mfcc_path = os.path.join(\"data\", \"analysisoutput\", f\"{base_no_ext}_mfcc.png\")\n",
    "        log_path  = os.path.join(\"data\", \"analysisoutput\", f\"{base_no_ext}_log_spectrogram.png\")\n",
    "\n",
    "        mfcc_img = load_image_as_tensor(mfcc_path)\n",
    "        if mfcc_img is None:\n",
    "            mfcc_img = torch.zeros((1,1))\n",
    "        log_img = load_image_as_tensor(log_path)\n",
    "        if log_img is None:\n",
    "            log_img = torch.zeros((1,1))\n",
    "\n",
    "        images_mfcc.append(mfcc_img)\n",
    "        images_log.append(log_img)\n",
    "\n",
    "        # scalar\n",
    "        res = analysis_data.get(\"results\", {})\n",
    "        dyn = analysis_data.get(\"dynamics_summary\", {})\n",
    "        adv = analysis_data.get(\"quantum_analysis\", {}).get(\"advanced_stats\", {})\n",
    "\n",
    "        avg_dev = res.get(\"average_dev_cents\", 0.0)\n",
    "        std_dev = res.get(\"std_dev_cents\", 0.0)\n",
    "        avg_hnr = res.get(\"avg_praat_hnr\", 0.0)\n",
    "        avg_tnr = res.get(\"avg_tnr\", 0.0)\n",
    "\n",
    "        rms_db_stats = dyn.get(\"rms_db\", {})\n",
    "        mean_rms_db  = rms_db_stats.get(\"mean\", 0.0)\n",
    "        lufs_stats   = dyn.get(\"lufs\", {})\n",
    "        mean_lufs    = lufs_stats.get(\"mean\", 0.0)\n",
    "\n",
    "        avg_jitter   = adv.get(\"avg_jitter\", 0.0)\n",
    "        avg_shimmer  = adv.get(\"avg_shimmer\", 0.0)\n",
    "        avg_vibrato  = adv.get(\"avg_vibrato_rate\", 0.0)\n",
    "        avg_formant  = adv.get(\"avg_F1\", 0.0)\n",
    "\n",
    "        scalars = [\n",
    "            avg_dev, std_dev, avg_hnr, avg_tnr,\n",
    "            mean_rms_db, mean_lufs,\n",
    "            avg_jitter, avg_shimmer, avg_vibrato, avg_formant\n",
    "        ]\n",
    "\n",
    "        quantum_dict = analysis_data.get(\"quantum_analysis\", {})\n",
    "        angles = quantum_dict.get(\"scaled_angles\", [])\n",
    "        max_len = 10\n",
    "        angle_arr = np.zeros(max_len, dtype=np.float32)\n",
    "        for i in range(min(max_len, len(angles))):\n",
    "            angle_arr[i] = angles[i]\n",
    "\n",
    "        counts_d = quantum_dict.get(\"measurement_counts\", {})\n",
    "        dist_vec = convert_counts_to_probs_feature(counts_d, max_bits=10)\n",
    "        combined_q = np.concatenate([angle_arr, dist_vec], axis=0)\n",
    "\n",
    "        scalar_feats.append(scalars)\n",
    "        quantum_feats.append(combined_q)\n",
    "        labels_raga.append(raga)\n",
    "        labels_quality.append(quality)\n",
    "\n",
    "    images_mfcc = torch.stack(images_mfcc)\n",
    "    images_log  = torch.stack(images_log)\n",
    "    scalar_feats = np.array(scalar_feats, dtype=np.float32)\n",
    "    quantum_feats = np.array(quantum_feats, dtype=np.float32)\n",
    "\n",
    "    unique_ragas = sorted(list(set(labels_raga)))\n",
    "    raga_to_idx = {r: i for i, r in enumerate(unique_ragas)}\n",
    "    label_raga_idx = [raga_to_idx[r] for r in labels_raga]\n",
    "    label_quality_idx = np.array([q - 1 for q in labels_quality], dtype=np.int64)\n",
    "\n",
    "    data_dict = {\n",
    "        \"images_mfcc\": images_mfcc,\n",
    "        \"images_log\": images_log,\n",
    "        \"scalar_feats\": scalar_feats,\n",
    "        \"quantum_feats\": quantum_feats,\n",
    "        \"label_raga_idx\": np.array(label_raga_idx, dtype=np.int64),\n",
    "        \"label_quality\": label_quality_idx,\n",
    "        \"raga_to_idx\": raga_to_idx,\n",
    "        \"unique_ragas\": unique_ragas\n",
    "    }\n",
    "\n",
    "    return data_dict, used_ids\n",
    "\n",
    "\n",
    "# Datasets / Model classes (unchanged)\n",
    "\n",
    "class MultiLabelASTDataset(Dataset):\n",
    "    def __init__(self, data_dict):\n",
    "        self.mfcc_imgs = data_dict[\"images_mfcc\"]\n",
    "        self.log_imgs  = data_dict[\"images_log\"]\n",
    "        self.scalars   = data_dict[\"scalar_feats\"]\n",
    "        self.quants    = data_dict[\"quantum_feats\"]\n",
    "        self.raga_lbl  = data_dict[\"label_raga_idx\"]\n",
    "        self.qual_lbl  = data_dict[\"label_quality\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raga_lbl)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        scal = torch.tensor(self.scalars[idx], dtype=torch.float32)\n",
    "        qua  = torch.tensor(self.quants[idx], dtype=torch.float32)\n",
    "        rag  = torch.tensor(self.raga_lbl[idx], dtype=torch.long)\n",
    "        qua_lbl = torch.tensor(self.qual_lbl[idx], dtype=torch.long)\n",
    "        return (\n",
    "            self.mfcc_imgs[idx],\n",
    "            self.log_imgs[idx],\n",
    "            scal,\n",
    "            qua,\n",
    "            rag,\n",
    "            qua_lbl\n",
    "        )\n",
    "\n",
    "class HybridASTModelV2(nn.Module):\n",
    "    def __init__(self, num_ragas, scalar_dim=10, quantum_dim=42,\n",
    "                 num_quality=5, num_unfrozen_layers=0):\n",
    "        super().__init__()\n",
    "        self.config = ASTConfig.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "        self.ast_model = ASTModel.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\", config=self.config)\n",
    "\n",
    "        for param in self.ast_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        if num_unfrozen_layers > 0:\n",
    "            total_layers = 12\n",
    "            start_layer = max(0, total_layers - num_unfrozen_layers)\n",
    "            for layer_idx in range(start_layer, total_layers):\n",
    "                for param in self.ast_model.encoder.layer[layer_idx].parameters():\n",
    "                    param.requires_grad = True\n",
    "\n",
    "        self.scalar_fc = nn.Sequential(\n",
    "            nn.Linear(scalar_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        self.quantum_fc = nn.Sequential(\n",
    "            nn.Linear(quantum_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "\n",
    "        combined_dim = 768 + 64 + 64\n",
    "        self.raga_head = nn.Sequential(\n",
    "            nn.Linear(combined_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, num_ragas)\n",
    "        )\n",
    "        self.quality_head = nn.Sequential(\n",
    "            nn.Linear(combined_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, num_quality)\n",
    "        )\n",
    "\n",
    "    def forward(self, mfcc_img, log_img, scal_in, quan_in):\n",
    "        B, C, H, W = log_img.shape\n",
    "        freq = HYPERPARAMS[\"AST_FREQ\"]\n",
    "        time = HYPERPARAMS[\"AST_TIME\"]\n",
    "\n",
    "        log_resized = F.interpolate(log_img, size=(freq, time), mode='bilinear', align_corners=False)\n",
    "        log_for_ast = log_resized.squeeze(1)  # shape (B,128,1024)\n",
    "\n",
    "        outputs = self.ast_model(input_values=log_for_ast, output_hidden_states=True)\n",
    "        hidden = outputs.last_hidden_state\n",
    "        ast_embedding = hidden[:, 0, :]  # (B,768)\n",
    "\n",
    "        emb_scal = self.scalar_fc(scal_in)\n",
    "        emb_quan = self.quantum_fc(quan_in)\n",
    "        fused = torch.cat([ast_embedding, emb_scal, emb_quan], dim=1)  # (B,896)\n",
    "\n",
    "        logits_raga = self.raga_head(fused)\n",
    "        logits_quality = self.quality_head(fused)\n",
    "        return logits_raga, logits_quality\n",
    "\n",
    "\n",
    "\n",
    "# Train Model, Return used_ids in checkpoint\n",
    "\n",
    "def train_model_v2():\n",
    "    data_dict, used_ids = fetch_training_data_v2(limit=HYPERPARAMS[\"DB_LIMIT\"])\n",
    "    if data_dict is None:\n",
    "        print(\"No data to train.\")\n",
    "        return None, None\n",
    "\n",
    "    scalar_feats = data_dict[\"scalar_feats\"]\n",
    "    quantum_feats = data_dict[\"quantum_feats\"]\n",
    "    raga_to_idx = data_dict[\"raga_to_idx\"]\n",
    "    num_ragas = len(raga_to_idx)\n",
    "\n",
    "    dataset_full = MultiLabelASTDataset(data_dict)\n",
    "    n_samples = len(dataset_full)\n",
    "    test_size = int(0.2 * n_samples)\n",
    "    train_size = n_samples - test_size\n",
    "    train_ds, test_ds = torch.utils.data.random_split(dataset_full, [train_size, test_size])\n",
    "\n",
    "    train_dl = DataLoader(train_ds, batch_size=HYPERPARAMS[\"BATCH_SIZE\"], shuffle=True)\n",
    "    test_dl  = DataLoader(test_ds, batch_size=HYPERPARAMS[\"BATCH_SIZE\"], shuffle=False)\n",
    "\n",
    "    model = HybridASTModelV2(\n",
    "        num_ragas=num_ragas,\n",
    "        scalar_dim=scalar_feats.shape[1],\n",
    "        quantum_dim=quantum_feats.shape[1],\n",
    "        num_quality=5,\n",
    "        num_unfrozen_layers=HYPERPARAMS[\"NUM_AST_LAYERS_UNFROZEN\"]\n",
    "    ).to(HYPERPARAMS[\"DEVICE\"])\n",
    "\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=HYPERPARAMS[\"LEARNING_RATE_HEAD\"],\n",
    "        weight_decay=HYPERPARAMS[\"WEIGHT_DECAY\"]\n",
    "    )\n",
    "    crit_ce = nn.CrossEntropyLoss()\n",
    "\n",
    "    EPOCHS = HYPERPARAMS[\"EPOCHS\"]\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    best_test_loss = float(\"inf\")\n",
    "    best_model_state = None\n",
    "    patience = HYPERPARAMS[\"PATIENCE\"]\n",
    "    epochs_no_improve = 0\n",
    "    stop_limit = HYPERPARAMS[\"STOP_LIMIT\"]\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "\n",
    "        for mfcc_img, log_img, scal, quan, rag_lbl, qual_lbl in train_dl:\n",
    "            mfcc_img = mfcc_img.to(HYPERPARAMS[\"DEVICE\"])\n",
    "            log_img  = log_img.to(HYPERPARAMS[\"DEVICE\"])\n",
    "            scal     = scal.to(HYPERPARAMS[\"DEVICE\"])\n",
    "            quan     = quan.to(HYPERPARAMS[\"DEVICE\"])\n",
    "            rag_lbl  = rag_lbl.to(HYPERPARAMS[\"DEVICE\"])\n",
    "            qual_lbl = qual_lbl.to(HYPERPARAMS[\"DEVICE\"])\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits_raga, logits_quality = model(mfcc_img, log_img, scal, quan)\n",
    "            loss_raga = crit_ce(logits_raga, rag_lbl)\n",
    "            loss_qual = crit_ce(logits_quality, qual_lbl)\n",
    "            loss = loss_raga + loss_qual\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_dl)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Evaluate\n",
    "        model.eval()\n",
    "        total_test_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for mfcc_img, log_img, scal, quan, rag_lbl, qual_lbl in test_dl:\n",
    "                mfcc_img = mfcc_img.to(HYPERPARAMS[\"DEVICE\"])\n",
    "                log_img  = log_img.to(HYPERPARAMS[\"DEVICE\"])\n",
    "                scal     = scal.to(HYPERPARAMS[\"DEVICE\"])\n",
    "                quan     = quan.to(HYPERPARAMS[\"DEVICE\"])\n",
    "                rag_lbl  = rag_lbl.to(HYPERPARAMS[\"DEVICE\"])\n",
    "                qual_lbl = qual_lbl.to(HYPERPARAMS[\"DEVICE\"])\n",
    "\n",
    "                logits_raga, logits_quality = model(mfcc_img, log_img, scal, quan)\n",
    "                loss_raga = crit_ce(logits_raga, rag_lbl)\n",
    "                loss_qual = crit_ce(logits_quality, qual_lbl)\n",
    "                total_test_loss += (loss_raga + loss_qual).item()\n",
    "\n",
    "        avg_test_loss = total_test_loss / len(test_dl)\n",
    "        test_losses.append(avg_test_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}: train_loss={avg_train_loss:.4f}, test_loss={avg_test_loss:.4f}\")\n",
    "\n",
    "        # Early stopping logic\n",
    "        if avg_test_loss < best_test_loss:\n",
    "            best_test_loss = avg_test_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if (epoch + 1) >= stop_limit and epochs_no_improve >= patience:\n",
    "                print(f\"No improvement for {patience} epochs after min epoch {stop_limit}. Stopping at epoch {epoch+1}.\")\n",
    "                break\n",
    "\n",
    "    print(\"Training complete or early stopped.\")\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        print(\"Loading best model state (lowest test loss).\")\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    # Evaluate final => confusion matrix\n",
    "    all_raga_preds, all_raga_truth = [], []\n",
    "    all_qual_preds, all_qual_truth = [], []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for mfcc_img, log_img, scal, quan, rag_lbl, qual_lbl in test_dl:\n",
    "            mfcc_img = mfcc_img.to(HYPERPARAMS[\"DEVICE\"])\n",
    "            log_img  = log_img.to(HYPERPARAMS[\"DEVICE\"])\n",
    "            scal     = scal.to(HYPERPARAMS[\"DEVICE\"])\n",
    "            quan     = quan.to(HYPERPARAMS[\"DEVICE\"])\n",
    "            rag_lbl  = rag_lbl.to(HYPERPARAMS[\"DEVICE\"])\n",
    "            qual_lbl = qual_lbl.to(HYPERPARAMS[\"DEVICE\"])\n",
    "\n",
    "            logits_raga, logits_quality = model(mfcc_img, log_img, scal, quan)\n",
    "            pred_raga = logits_raga.argmax(dim=1).cpu().numpy()\n",
    "            pred_qual = logits_quality.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "            all_raga_preds.extend(pred_raga)\n",
    "            all_raga_truth.extend(rag_lbl.cpu().numpy())\n",
    "            all_qual_preds.extend(pred_qual)\n",
    "            all_qual_truth.extend(qual_lbl.cpu().numpy())\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "    cm_raga = confusion_matrix(all_raga_truth, all_raga_preds)\n",
    "    used_raga_indices = sorted(set(all_raga_truth) | set(all_raga_preds))\n",
    "    idx_to_raga = {v: k for k, v in data_dict[\"raga_to_idx\"].items()}\n",
    "    used_raga_labels = [idx_to_raga[i] for i in used_raga_indices]\n",
    "\n",
    "    disp_raga = ConfusionMatrixDisplay(cm_raga, display_labels=used_raga_labels)\n",
    "    disp_raga.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(\"Raga Confusion Matrix (Best Model)\")\n",
    "    plt.show()\n",
    "\n",
    "    cm_qual = confusion_matrix(all_qual_truth, all_qual_preds)\n",
    "    used_qual_indices = sorted(set(all_qual_truth) | set(all_qual_preds))\n",
    "    used_qual_labels  = [q+1 for q in used_qual_indices]\n",
    "    disp_qual = ConfusionMatrixDisplay(cm_qual, display_labels=used_qual_labels)\n",
    "    disp_qual.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(\"Quality Confusion Matrix (Best Model)\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(train_losses, label=\"Train Loss\")\n",
    "    plt.plot(test_losses, label=\"Test Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Curves\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Save best model + used_ids\n",
    "    os.makedirs(\"data/modeloutput\", exist_ok=True)\n",
    "    checkpoint_path = os.path.join(\"data\", \"modeloutput\", \"trained_model.pt\")\n",
    "    checkpoint_dict = {\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"raga_to_idx\": data_dict[\"raga_to_idx\"],\n",
    "        \"scalar_dim\": scalar_feats.shape[1],\n",
    "        \"quantum_dim\": quantum_feats.shape[1],\n",
    "        \"num_ragas\": len(data_dict[\"raga_to_idx\"]),\n",
    "        \"used_ids\": used_ids  # new addition\n",
    "    }\n",
    "    torch.save(checkpoint_dict, checkpoint_path)\n",
    "    print(f\"Best model + metadata + used_ids saved to: {checkpoint_path}\")\n",
    "\n",
    "    return model, data_dict[\"raga_to_idx\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 1) NO-QUANTUM MODEL CLASS\n",
    "###############################################################################\n",
    "class HybridASTModelV2NoQuantum(nn.Module):\n",
    "    \"\"\"\n",
    "    Identical to your HybridASTModelV2, but it omits quantum features.\n",
    "    We only fuse the AST embedding + scalar_fc output (no quantum_fc).\n",
    "    \"\"\"\n",
    "    def __init__(self, num_ragas, scalar_dim=10, num_quality=5, num_unfrozen_layers=0):\n",
    "        super().__init__()\n",
    "        # Load AST config + model\n",
    "        self.config = ASTConfig.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "        self.ast_model = ASTModel.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\", config=self.config)\n",
    "\n",
    "        # Partial unfreeze\n",
    "        for param in self.ast_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        if num_unfrozen_layers > 0:\n",
    "            total_layers = 12\n",
    "            start_layer = max(0, total_layers - num_unfrozen_layers)\n",
    "            for layer_idx in range(start_layer, total_layers):\n",
    "                for param in self.ast_model.encoder.layer[layer_idx].parameters():\n",
    "                    param.requires_grad = True\n",
    "\n",
    "        # Only scalar MLP; no quantum MLP\n",
    "        self.scalar_fc = nn.Sequential(\n",
    "            nn.Linear(scalar_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "\n",
    "        # AST embedding => 768 dims, plus 64 => 832 total\n",
    "        combined_dim = 768 + 64\n",
    "\n",
    "        self.raga_head = nn.Sequential(\n",
    "            nn.Linear(combined_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, num_ragas)\n",
    "        )\n",
    "        self.quality_head = nn.Sequential(\n",
    "            nn.Linear(combined_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, num_quality)\n",
    "        )\n",
    "\n",
    "    def forward(self, mfcc_img, log_img, scalar_input):\n",
    "        \"\"\"\n",
    "        Omits the quantum features entirely.\n",
    "        \"\"\"\n",
    "        import torch.nn.functional as F\n",
    "        B, C, H, W = log_img.shape\n",
    "        freq = HYPERPARAMS[\"AST_FREQ\"]\n",
    "        time = HYPERPARAMS[\"AST_TIME\"]\n",
    "\n",
    "        # Interpolate log_img => (B,1,128,1024)\n",
    "        log_resized = F.interpolate(\n",
    "            log_img, size=(freq, time),\n",
    "            mode='bilinear', align_corners=False\n",
    "        )\n",
    "        # Squeeze channel => (B,128,1024)\n",
    "        log_for_ast = log_resized.squeeze(1)\n",
    "\n",
    "        # Pass to AST\n",
    "        outputs = self.ast_model(input_values=log_for_ast, output_hidden_states=True)\n",
    "        hidden = outputs.last_hidden_state  # (B, seq_len, 768)\n",
    "        ast_embedding = hidden[:, 0, :]     # [CLS] => (B,768)\n",
    "\n",
    "        # scalar\n",
    "        emb_scal = self.scalar_fc(scalar_input)  # (B,64)\n",
    "        fused = torch.cat([ast_embedding, emb_scal], dim=1)  # (B,832)\n",
    "\n",
    "        logits_raga = self.raga_head(fused)\n",
    "        logits_quality = self.quality_head(fused)\n",
    "        return logits_raga, logits_quality\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 2) TRAINING FUNCTION FOR NO-QUANTUM MODEL\n",
    "###############################################################################\n",
    "def train_model_v2_noquantum():\n",
    "    \"\"\"\n",
    "    Identical logic to train_model_v2, but using HybridASTModelV2NoQuantum.\n",
    "    Prints confusion matrices and training curves at the end.\n",
    "    \"\"\"\n",
    "    # Fetch data & used_ids\n",
    "    data_dict, used_ids = fetch_training_data_v2(limit=HYPERPARAMS[\"DB_LIMIT\"])\n",
    "    if data_dict is None:\n",
    "        print(\"[NoQuantum] No data found in DB.\")\n",
    "        return None, None\n",
    "\n",
    "    # We'll ignore quantum features. We only use scalar_feats in the model.\n",
    "    scalar_feats = data_dict[\"scalar_feats\"]\n",
    "    raga_to_idx = data_dict[\"raga_to_idx\"]\n",
    "    num_ragas = len(raga_to_idx)\n",
    "\n",
    "    # Build dataset, but in forward we'll skip 'quan'\n",
    "    dataset_full = MultiLabelASTDataset(data_dict)\n",
    "    n_samples = len(dataset_full)\n",
    "    test_size = int(0.2 * n_samples)\n",
    "    train_size = n_samples - test_size\n",
    "    train_ds, test_ds = torch.utils.data.random_split(dataset_full, [train_size, test_size])\n",
    "\n",
    "    train_dl = DataLoader(train_ds, batch_size=HYPERPARAMS[\"BATCH_SIZE\"], shuffle=True)\n",
    "    test_dl  = DataLoader(test_ds, batch_size=HYPERPARAMS[\"BATCH_SIZE\"], shuffle=False)\n",
    "\n",
    "    # Build the no-quantum model\n",
    "    model_nq = HybridASTModelV2NoQuantum(\n",
    "        num_ragas=num_ragas,\n",
    "        scalar_dim=scalar_feats.shape[1],\n",
    "        num_quality=5,\n",
    "        num_unfrozen_layers=HYPERPARAMS[\"NUM_AST_LAYERS_UNFROZEN\"]\n",
    "    ).to(HYPERPARAMS[\"DEVICE\"])\n",
    "\n",
    "    # Use L2 regularization\n",
    "    optimizer = optim.Adam(\n",
    "        model_nq.parameters(),\n",
    "        lr=HYPERPARAMS[\"LEARNING_RATE_HEAD\"],\n",
    "        weight_decay=HYPERPARAMS[\"WEIGHT_DECAY\"]\n",
    "    )\n",
    "    crit_ce = nn.CrossEntropyLoss()\n",
    "\n",
    "    EPOCHS = HYPERPARAMS[\"EPOCHS\"]\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    best_test_loss = float(\"inf\")\n",
    "    best_model_state = None\n",
    "    patience = HYPERPARAMS[\"PATIENCE\"]\n",
    "    epochs_no_improve = 0\n",
    "    stop_limit = HYPERPARAMS[\"STOP_LIMIT\"]\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model_nq.train()\n",
    "        total_train_loss = 0.0\n",
    "\n",
    "        for mfcc_img, log_img, scal, quan, rag_lbl, qual_lbl in train_dl:\n",
    "            mfcc_img = mfcc_img.to(HYPERPARAMS[\"DEVICE\"])\n",
    "            log_img  = log_img.to(HYPERPARAMS[\"DEVICE\"])\n",
    "            scal     = scal.to(HYPERPARAMS[\"DEVICE\"])\n",
    "            rag_lbl  = rag_lbl.to(HYPERPARAMS[\"DEVICE\"])\n",
    "            qual_lbl = qual_lbl.to(HYPERPARAMS[\"DEVICE\"])\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            # forward => ignore quantum feats\n",
    "            logits_raga, logits_quality = model_nq(mfcc_img, log_img, scal)\n",
    "            loss_raga = crit_ce(logits_raga, rag_lbl)\n",
    "            loss_qual = crit_ce(logits_quality, qual_lbl)\n",
    "            loss = loss_raga + loss_qual\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_dl)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Evaluate\n",
    "        model_nq.eval()\n",
    "        total_test_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for mfcc_img, log_img, scal, quan, rag_lbl, qual_lbl in test_dl:\n",
    "                mfcc_img = mfcc_img.to(HYPERPARAMS[\"DEVICE\"])\n",
    "                log_img  = log_img.to(HYPERPARAMS[\"DEVICE\"])\n",
    "                scal     = scal.to(HYPERPARAMS[\"DEVICE\"])\n",
    "                rag_lbl  = rag_lbl.to(HYPERPARAMS[\"DEVICE\"])\n",
    "                qual_lbl = qual_lbl.to(HYPERPARAMS[\"DEVICE\"])\n",
    "\n",
    "                logits_raga, logits_quality = model_nq(mfcc_img, log_img, scal)\n",
    "                loss_raga = crit_ce(logits_raga, rag_lbl)\n",
    "                loss_qual = crit_ce(logits_quality, qual_lbl)\n",
    "                total_test_loss += (loss_raga + loss_qual).item()\n",
    "\n",
    "        avg_test_loss = total_test_loss / len(test_dl)\n",
    "        test_losses.append(avg_test_loss)\n",
    "\n",
    "        print(f\"[NoQuantum] Epoch {epoch+1}/{EPOCHS}: train_loss={avg_train_loss:.4f}, test_loss={avg_test_loss:.4f}\")\n",
    "\n",
    "        # Early Stopping\n",
    "        if avg_test_loss < best_test_loss:\n",
    "            best_test_loss = avg_test_loss\n",
    "            best_model_state = model_nq.state_dict()\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if (epoch + 1) >= stop_limit and epochs_no_improve >= patience:\n",
    "                print(f\"[NoQuantum] No improvement for {patience} epochs after min epoch {stop_limit}. Stopping.\")\n",
    "                break\n",
    "\n",
    "    print(\"[NoQuantum] Training complete or early stopped.\")\n",
    "\n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model_nq.load_state_dict(best_model_state)\n",
    "\n",
    "    # Final confusion matrices for no-quantum approach\n",
    "    all_raga_preds, all_raga_truth = [], []\n",
    "    all_qual_preds, all_qual_truth = [], []\n",
    "\n",
    "    model_nq.eval()\n",
    "    with torch.no_grad():\n",
    "        for mfcc_img, log_img, scal, quan, rag_lbl, qual_lbl in test_dl:\n",
    "            mfcc_img = mfcc_img.to(HYPERPARAMS[\"DEVICE\"])\n",
    "            log_img  = log_img.to(HYPERPARAMS[\"DEVICE\"])\n",
    "            scal     = scal.to(HYPERPARAMS[\"DEVICE\"])\n",
    "            rag_lbl  = rag_lbl.to(HYPERPARAMS[\"DEVICE\"])\n",
    "            qual_lbl = qual_lbl.to(HYPERPARAMS[\"DEVICE\"])\n",
    "\n",
    "            logits_raga, logits_quality = model_nq(mfcc_img, log_img, scal)\n",
    "            pred_raga = logits_raga.argmax(dim=1).cpu().numpy()\n",
    "            pred_qual = logits_quality.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "            all_raga_preds.extend(pred_raga)\n",
    "            all_raga_truth.extend(rag_lbl.cpu().numpy())\n",
    "            all_qual_preds.extend(pred_qual)\n",
    "            all_qual_truth.extend(qual_lbl.cpu().numpy())\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "    # Raga Confusion Matrix\n",
    "    cm_raga = confusion_matrix(all_raga_truth, all_raga_preds)\n",
    "    used_raga_indices = sorted(set(all_raga_truth) | set(all_raga_preds))\n",
    "\n",
    "    idx_to_raga = data_dict[\"raga_to_idx\"]\n",
    "    inv_raga = {v:k for k,v in idx_to_raga.items()}\n",
    "    used_raga_labels = [inv_raga[i] for i in used_raga_indices]\n",
    "\n",
    "    disp_raga = ConfusionMatrixDisplay(cm_raga, display_labels=used_raga_labels)\n",
    "    disp_raga.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(\"Raga Confusion Matrix (No-Quantum)\")\n",
    "    plt.show()\n",
    "\n",
    "    # Quality Confusion Matrix\n",
    "    cm_qual = confusion_matrix(all_qual_truth, all_qual_preds)\n",
    "    used_qual_indices = sorted(set(all_qual_truth) | set(all_qual_preds))\n",
    "    used_qual_labels = [q+1 for q in used_qual_indices]\n",
    "\n",
    "    disp_qual = ConfusionMatrixDisplay(cm_qual, display_labels=used_qual_labels)\n",
    "    disp_qual.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(\"Quality Confusion Matrix (No-Quantum)\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot training curves\n",
    "    plt.figure()\n",
    "    plt.plot(train_losses, label=\"Train Loss (NoQuantum)\")\n",
    "    plt.plot(test_losses, label=\"Test Loss (NoQuantum)\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"No-Quantum Model Training Curves\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Save best model + used_ids\n",
    "    os.makedirs(\"data/modeloutput\", exist_ok=True)\n",
    "    checkpoint_path = os.path.join(\"data\", \"modeloutput\", \"trained_model_noquantum.pt\")\n",
    "    checkpoint_dict = {\n",
    "        \"model_state\": model_nq.state_dict(),\n",
    "        \"raga_to_idx\": data_dict[\"raga_to_idx\"],\n",
    "        \"scalar_dim\": scalar_feats.shape[1],\n",
    "        \"num_ragas\": num_ragas,\n",
    "        \"used_ids\": used_ids\n",
    "    }\n",
    "    torch.save(checkpoint_dict, checkpoint_path)\n",
    "    print(f\"[NoQuantum] Best model + metadata (no quantum) saved to: {checkpoint_path}\")\n",
    "\n",
    "    return model_nq, data_dict[\"raga_to_idx\"]\n",
    "\n",
    "###############################################################################\n",
    "# 3) COMPARISON FUNCTION: QUANTUM VS. NO-QUANTUM\n",
    "###############################################################################\n",
    "def compare_classical_vs_quantum_models():\n",
    "    \"\"\"\n",
    "    1) Re-fetch the dataset (same DB_LIMIT).\n",
    "    2) Build the same test set split (20%).\n",
    "    3) Load 'trained_model.pt' (quantum) and 'trained_model_noquantum.pt' (no quantum).\n",
    "    4) Evaluate both on the same test set, compare accuracy side-by-side.\n",
    "    \"\"\"\n",
    "    data_dict, used_ids = fetch_training_data_v2(limit=HYPERPARAMS[\"DB_LIMIT\"])\n",
    "    if data_dict is None:\n",
    "        print(\"[Compare] No data for comparison.\")\n",
    "        return\n",
    "\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "    dataset_full = MultiLabelASTDataset(data_dict)\n",
    "    n_samples = len(dataset_full)\n",
    "    test_size = int(0.2 * n_samples)\n",
    "    train_size = n_samples - test_size\n",
    "    _, test_ds = torch.utils.data.random_split(dataset_full, [train_size, test_size])\n",
    "    test_dl = DataLoader(test_ds, batch_size=HYPERPARAMS[\"BATCH_SIZE\"], shuffle=False)\n",
    "\n",
    "    #--- Load quantum model checkpoint\n",
    "    q_ckpt_path = \"data/modeloutput/trained_model.pt\"\n",
    "    q_ckpt = torch.load(q_ckpt_path, map_location=HYPERPARAMS[\"DEVICE\"])\n",
    "    q_model_state = q_ckpt[\"model_state\"]\n",
    "    q_raga_to_idx = q_ckpt[\"raga_to_idx\"]\n",
    "    q_scalar_dim  = q_ckpt[\"scalar_dim\"]\n",
    "    q_quantum_dim = q_ckpt[\"quantum_dim\"]\n",
    "    q_num_ragas   = q_ckpt[\"num_ragas\"]\n",
    "\n",
    "    # Rebuild quantum model\n",
    "    model_q = HybridASTModelV2(\n",
    "        num_ragas=q_num_ragas,\n",
    "        scalar_dim=q_scalar_dim,\n",
    "        quantum_dim=q_quantum_dim,\n",
    "        num_quality=5,\n",
    "        num_unfrozen_layers=HYPERPARAMS[\"NUM_AST_LAYERS_UNFROZEN\"]\n",
    "    )\n",
    "    model_q.load_state_dict(q_model_state)\n",
    "    model_q.to(HYPERPARAMS[\"DEVICE\"])\n",
    "    model_q.eval()\n",
    "\n",
    "    #--- Load no-quantum model checkpoint\n",
    "    nq_ckpt_path = \"data/modeloutput/trained_model_noquantum.pt\"\n",
    "    nq_ckpt = torch.load(nq_ckpt_path, map_location=HYPERPARAMS[\"DEVICE\"])\n",
    "    nq_model_state = nq_ckpt[\"model_state\"]\n",
    "    nq_raga_to_idx = nq_ckpt[\"raga_to_idx\"]\n",
    "    nq_scalar_dim  = nq_ckpt[\"scalar_dim\"]\n",
    "    nq_num_ragas   = nq_ckpt[\"num_ragas\"]\n",
    "\n",
    "    model_nq = HybridASTModelV2NoQuantum(\n",
    "        num_ragas=nq_num_ragas,\n",
    "        scalar_dim=nq_scalar_dim,\n",
    "        num_quality=5,\n",
    "        num_unfrozen_layers=HYPERPARAMS[\"NUM_AST_LAYERS_UNFROZEN\"]\n",
    "    )\n",
    "    model_nq.load_state_dict(nq_model_state)\n",
    "    model_nq.to(HYPERPARAMS[\"DEVICE\"])\n",
    "    model_nq.eval()\n",
    "\n",
    "    #--- Evaluate both on test\n",
    "    all_true_ragas = []\n",
    "    all_true_quals = []\n",
    "\n",
    "    preds_raga_q, preds_qual_q = [], []\n",
    "    preds_raga_nq, preds_qual_nq = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for mfcc_img, log_img, scal, quan, rag_lbl, qual_lbl in test_dl:\n",
    "            mfcc_img = mfcc_img.to(HYPERPARAMS[\"DEVICE\"])\n",
    "            log_img  = log_img.to(HYPERPARAMS[\"DEVICE\"])\n",
    "            scal     = scal.to(HYPERPARAMS[\"DEVICE\"])\n",
    "            quan     = quan.to(HYPERPARAMS[\"DEVICE\"])\n",
    "            rag_lbl  = rag_lbl.to(HYPERPARAMS[\"DEVICE\"])\n",
    "            qual_lbl = qual_lbl.to(HYPERPARAMS[\"DEVICE\"])\n",
    "\n",
    "            # quantum model\n",
    "            logits_raga_q, logits_quality_q = model_q(mfcc_img, log_img, scal, quan)\n",
    "            pred_r_q = logits_raga_q.argmax(dim=1).cpu().numpy()\n",
    "            pred_q_q = logits_quality_q.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "            # no-quantum model\n",
    "            logits_raga_nq, logits_quality_nq = model_nq(mfcc_img, log_img, scal)\n",
    "            pred_r_nq = logits_raga_nq.argmax(dim=1).cpu().numpy()\n",
    "            pred_q_nq = logits_quality_nq.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "            all_true_ragas.extend(rag_lbl.cpu().numpy())\n",
    "            all_true_quals.extend(qual_lbl.cpu().numpy())\n",
    "            preds_raga_q.extend(pred_r_q)\n",
    "            preds_qual_q.extend(pred_q_q)\n",
    "            preds_raga_nq.extend(pred_r_nq)\n",
    "            preds_qual_nq.extend(pred_q_nq)\n",
    "\n",
    "    rag_acc_q  = accuracy_score(all_true_ragas, preds_raga_q)\n",
    "    rag_acc_nq = accuracy_score(all_true_ragas, preds_raga_nq)\n",
    "    qual_acc_q  = accuracy_score(all_true_quals, preds_qual_q)\n",
    "    qual_acc_nq = accuracy_score(all_true_quals, preds_qual_nq)\n",
    "\n",
    "    print(\"\\n=== Comparison: Quantum Model vs. No-Quantum Model ===\")\n",
    "    print(f\"Raga Accuracy (Quantum):     {rag_acc_q:.4f}\")\n",
    "    print(f\"Raga Accuracy (No-Quantum):  {rag_acc_nq:.4f}\")\n",
    "    print(f\"Quality Accuracy (Quantum):  {qual_acc_q:.4f}\")\n",
    "    print(f\"Quality Accuracy (No-Quant): {qual_acc_nq:.4f}\")\n",
    "\n",
    "    d_raga = (rag_acc_q - rag_acc_nq)*100\n",
    "    d_qual = (qual_acc_q - qual_acc_nq)*100\n",
    "    print(f\"\\nDifference in Raga Accuracy = {d_raga:.2f} percentage points\")\n",
    "    print(f\"Difference in Quality Acc.  = {d_qual:.2f} percentage points\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  SINGLE-RECORD INFERENCE (unchanged)\n",
    "\n",
    "def run_inference_on_persisted_model(record_id, model_path=\"data/modeloutput/trained_model.pt\"):\n",
    "    \"\"\"\n",
    "    Single-record inference for quick demos, etc.\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(model_path, map_location=HYPERPARAMS[\"DEVICE\"])\n",
    "    model_state = checkpoint[\"model_state\"]\n",
    "    raga_to_idx = checkpoint[\"raga_to_idx\"]\n",
    "    scalar_dim = checkpoint[\"scalar_dim\"]\n",
    "    quantum_dim = checkpoint[\"quantum_dim\"]\n",
    "    num_ragas = checkpoint[\"num_ragas\"]\n",
    "\n",
    "    model = HybridASTModelV2(\n",
    "        num_ragas=num_ragas,\n",
    "        scalar_dim=scalar_dim,\n",
    "        quantum_dim=quantum_dim,\n",
    "        num_quality=5,\n",
    "        num_unfrozen_layers=HYPERPARAMS[\"NUM_AST_LAYERS_UNFROZEN\"]\n",
    "    )\n",
    "    model.load_state_dict(model_state)\n",
    "    model.to(HYPERPARAMS[\"DEVICE\"])\n",
    "    model.eval()\n",
    "\n",
    "    db = QuantumMusicDBFetchOnly()\n",
    "    row = db.fetch_single_record(record_id)\n",
    "    db.close()\n",
    "\n",
    "    if not row:\n",
    "        print(f\"No record found with ID={record_id}\")\n",
    "        return None, None\n",
    "\n",
    "    _, analysis_data = row\n",
    "    fname = analysis_data[\"file_name\"]\n",
    "    raga_true, quality_true = parse_raga_and_quality(fname)\n",
    "    base_no_ext = fname.replace(\".wav\", \"\")\n",
    "\n",
    "    mfcc_path = os.path.join(\"data\", \"analysisoutput\", f\"{base_no_ext}_mfcc.png\")\n",
    "    log_path  = os.path.join(\"data\", \"analysisoutput\", f\"{base_no_ext}_log_spectrogram.png\")\n",
    "\n",
    "    mfcc_img = load_image_as_tensor(mfcc_path)\n",
    "    if mfcc_img is None:\n",
    "        mfcc_img = torch.zeros((1,1))\n",
    "\n",
    "    log_img = load_image_as_tensor(log_path)\n",
    "    if log_img is None:\n",
    "        log_img = torch.zeros((1,1))\n",
    "    \n",
    "    res = analysis_data.get(\"results\", {})\n",
    "    dyn = analysis_data.get(\"dynamics_summary\", {})\n",
    "    adv = analysis_data.get(\"quantum_analysis\", {}).get(\"advanced_stats\", {})\n",
    "\n",
    "    avg_dev = res.get(\"average_dev_cents\", 0.0)\n",
    "    std_dev = res.get(\"std_dev_cents\", 0.0)\n",
    "    avg_hnr = res.get(\"avg_praat_hnr\", 0.0)\n",
    "    avg_tnr = res.get(\"avg_tnr\", 0.0)\n",
    "\n",
    "    rms_db_stats = dyn.get(\"rms_db\", {})\n",
    "    mean_rms_db  = rms_db_stats.get(\"mean\", 0.0)\n",
    "    lufs_stats   = dyn.get(\"lufs\", {})\n",
    "    mean_lufs    = lufs_stats.get(\"mean\", 0.0)\n",
    "\n",
    "    avg_jitter   = adv.get(\"avg_jitter\", 0.0)\n",
    "    avg_shimmer  = adv.get(\"avg_shimmer\", 0.0)\n",
    "    avg_vibrato  = adv.get(\"avg_vibrato_rate\", 0.0)\n",
    "    avg_formant  = adv.get(\"avg_F1\", 0.0)\n",
    "\n",
    "    scalars = [\n",
    "        avg_dev, std_dev, avg_hnr, avg_tnr,\n",
    "        mean_rms_db, mean_lufs,\n",
    "        avg_jitter, avg_shimmer, avg_vibrato, avg_formant\n",
    "    ]\n",
    "\n",
    "    quantum_dict = analysis_data.get(\"quantum_analysis\", {})\n",
    "    angles = quantum_dict.get(\"scaled_angles\", [])\n",
    "    max_len = 10\n",
    "    angle_arr = np.zeros(max_len, dtype=np.float32)\n",
    "    for i in range(min(max_len, len(angles))):\n",
    "        angle_arr[i] = angles[i]\n",
    "\n",
    "    counts_d = quantum_dict.get(\"measurement_counts\", {})\n",
    "    dist_vec = convert_counts_to_probs_feature(counts_d, max_bits=10)\n",
    "    combined_q = np.concatenate([angle_arr, dist_vec], axis=0)\n",
    "\n",
    "    device = HYPERPARAMS[\"DEVICE\"]\n",
    "    mfcc_img = mfcc_img.unsqueeze(0).to(device)\n",
    "    log_img  = log_img.unsqueeze(0).to(device)\n",
    "    scal_ten = torch.tensor(scalars, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    quan_ten = torch.tensor(combined_q, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits_raga, logits_quality = model(mfcc_img, log_img, scal_ten, quan_ten)\n",
    "        pred_raga_idx = torch.argmax(logits_raga, dim=1).item()\n",
    "        pred_qual_idx = torch.argmax(logits_quality, dim=1).item()\n",
    "\n",
    "    inv_map = {v: k for k, v in raga_to_idx.items()}\n",
    "    pred_raga_str = inv_map.get(pred_raga_idx, \"UnknownRaga\")\n",
    "    pred_quality = pred_qual_idx + 1\n",
    "\n",
    "    print(f\"Inference for Record ID={record_id}:\")\n",
    "    print(f\"  True raga={raga_true}, True quality={quality_true}\")\n",
    "    print(f\"  Predicted raga={pred_raga_str}, predicted quality={pred_quality}\")\n",
    "    return pred_raga_str, pred_quality\n",
    "\n",
    "\n",
    "\n",
    "# NEW: run_inference_on_persisted_model_v2\n",
    "\n",
    "def run_inference_on_persisted_model_v2(model_path=\"data/modeloutput/trained_model.pt\"):\n",
    "    \"\"\"\n",
    "    1) Loads the best model + used_ids from the checkpoint.\n",
    "    2) Fetch leftover records with rating >= 4, excluding used_ids.\n",
    "    3) Runs inference, prints (ID, file_name, actual raga/quality, predicted raga/quality).\n",
    "    \"\"\"\n",
    "    # 1) Load checkpoint\n",
    "    checkpoint = torch.load(model_path, map_location=HYPERPARAMS[\"DEVICE\"])\n",
    "    model_state = checkpoint[\"model_state\"]\n",
    "    raga_to_idx = checkpoint[\"raga_to_idx\"]\n",
    "    scalar_dim  = checkpoint[\"scalar_dim\"]\n",
    "    quantum_dim = checkpoint[\"quantum_dim\"]\n",
    "    num_ragas   = checkpoint[\"num_ragas\"]\n",
    "    used_ids    = checkpoint.get(\"used_ids\", [])\n",
    "\n",
    "    # 2) Rebuild model\n",
    "    model = HybridASTModelV2(\n",
    "        num_ragas=num_ragas,\n",
    "        scalar_dim=scalar_dim,\n",
    "        quantum_dim=quantum_dim,\n",
    "        num_quality=5,\n",
    "        num_unfrozen_layers=HYPERPARAMS[\"NUM_AST_LAYERS_UNFROZEN\"]\n",
    "    )\n",
    "    model.load_state_dict(model_state)\n",
    "    model.to(HYPERPARAMS[\"DEVICE\"])\n",
    "    model.eval()\n",
    "\n",
    "    # 3) Fetch leftover records\n",
    "    db = QuantumMusicDBFetchOnly()\n",
    "    leftover = db.fetch_leftover_records(exclude_ids=set(used_ids))\n",
    "    db.close()\n",
    "\n",
    "    if not leftover:\n",
    "        print(\"No leftover records found with rating >=4 that weren't used in training.\")\n",
    "        return\n",
    "\n",
    "    # For mapping predicted raga index -> string\n",
    "    inv_map = {v: k for k, v in raga_to_idx.items()}\n",
    "\n",
    "    # 4) Inference loop\n",
    "    for (rec_id, analysis_data) in leftover:\n",
    "        fname = analysis_data[\"file_name\"]\n",
    "        raga_true, qual_true = parse_raga_and_quality(fname)\n",
    "        base_no_ext = fname.replace(\".wav\", \"\")\n",
    "\n",
    "        # images\n",
    "        mfcc_path = os.path.join(\"data\", \"analysisoutput\", f\"{base_no_ext}_mfcc.png\")\n",
    "        log_path  = os.path.join(\"data\", \"analysisoutput\", f\"{base_no_ext}_log_spectrogram.png\")\n",
    "        mfcc_img = load_image_as_tensor(mfcc_path)\n",
    "        if mfcc_img is None:\n",
    "            mfcc_img = torch.zeros((1,1))\n",
    "        log_img = load_image_as_tensor(log_path)\n",
    "        if log_img is None:\n",
    "            log_img = torch.zeros((1,1))\n",
    "\n",
    "        # scalar\n",
    "        res = analysis_data.get(\"results\", {})\n",
    "        dyn = analysis_data.get(\"dynamics_summary\", {})\n",
    "        adv = analysis_data.get(\"quantum_analysis\", {}).get(\"advanced_stats\", {})\n",
    "\n",
    "        avg_dev = res.get(\"average_dev_cents\", 0.0)\n",
    "        std_dev = res.get(\"std_dev_cents\", 0.0)\n",
    "        avg_hnr = res.get(\"avg_praat_hnr\", 0.0)\n",
    "        avg_tnr = res.get(\"avg_tnr\", 0.0)\n",
    "\n",
    "        rms_db_stats = dyn.get(\"rms_db\", {})\n",
    "        mean_rms_db  = rms_db_stats.get(\"mean\", 0.0)\n",
    "        lufs_stats   = dyn.get(\"lufs\", {})\n",
    "        mean_lufs    = lufs_stats.get(\"mean\", 0.0)\n",
    "\n",
    "        avg_jitter   = adv.get(\"avg_jitter\", 0.0)\n",
    "        avg_shimmer  = adv.get(\"avg_shimmer\", 0.0)\n",
    "        avg_vibrato  = adv.get(\"avg_vibrato_rate\", 0.0)\n",
    "        avg_formant  = adv.get(\"avg_F1\", 0.0)\n",
    "\n",
    "        scalars = [\n",
    "            avg_dev, std_dev, avg_hnr, avg_tnr,\n",
    "            mean_rms_db, mean_lufs,\n",
    "            avg_jitter, avg_shimmer, avg_vibrato, avg_formant\n",
    "        ]\n",
    "\n",
    "        # quantum\n",
    "        quantum_dict = analysis_data.get(\"quantum_analysis\", {})\n",
    "        angles = quantum_dict.get(\"scaled_angles\", [])\n",
    "        max_len = 10\n",
    "        angle_arr = np.zeros(max_len, dtype=np.float32)\n",
    "        for i in range(min(max_len, len(angles))):\n",
    "            angle_arr[i] = angles[i]\n",
    "\n",
    "        counts_d = quantum_dict.get(\"measurement_counts\", {})\n",
    "        dist_vec = convert_counts_to_probs_feature(counts_d, max_bits=10)\n",
    "        combined_q = np.concatenate([angle_arr, dist_vec], axis=0)\n",
    "\n",
    "        # 5) Model forward pass\n",
    "        device = HYPERPARAMS[\"DEVICE\"]\n",
    "        mfcc_img = mfcc_img.unsqueeze(0).to(device)\n",
    "        log_img  = log_img.unsqueeze(0).to(device)\n",
    "        scal_ten = torch.tensor(scalars, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        quan_ten = torch.tensor(combined_q, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits_raga, logits_quality = model(mfcc_img, log_img, scal_ten, quan_ten)\n",
    "            pred_raga_idx = torch.argmax(logits_raga, dim=1).item()\n",
    "            pred_qual_idx = torch.argmax(logits_quality, dim=1).item()\n",
    "\n",
    "        pred_raga_str = inv_map.get(pred_raga_idx, \"UnknownRaga\")\n",
    "        pred_quality = pred_qual_idx + 1\n",
    "\n",
    "        # 6) Print\n",
    "        print(f\"\\nLeftover Inference => ID={rec_id}, file_name={fname}\")\n",
    "        print(f\"   Actual Raga={raga_true}, Quality={qual_true}\")\n",
    "        print(f\"   Predicted Raga={pred_raga_str}, Quality={pred_quality}\")\n",
    "\n",
    "    print(\"\\n--- Finished leftover inference (rating>=4) ---\")\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) TRAIN\n",
    "    #model, raga_to_idx = train_model_v2()\n",
    "    #model_nq, raga_nq = train_model_v2_noquantum()  \n",
    "    # saves to data/modeloutput/trained_model_noquantum.pt\n",
    "\n",
    "    #compare_classical_vs_quantum_models()\n",
    "\n",
    "    # 2) SINGLE RECORD\n",
    "    run_inference_on_persisted_model(2150, \"data/modeloutput/trained_model_rohantst1.pt\")\n",
    "\n",
    "    # 3) LEFTOVER RATING >= 4\n",
    "    #run_inference_on_persisted_model_v2(\"data/modeloutput/trained_model.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantumvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
