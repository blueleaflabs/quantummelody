{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchaudio.transforms import MelSpectrogram\n",
    "import librosa\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from transformers import ASTModel, ASTConfig\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "\n",
    "################################################################################\n",
    "# 1) MPS DEVICE CHECK\n",
    "################################################################################\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "    print(\"Using MPS device for GPU acceleration on Apple Silicon.\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    print(\"MPS not available. Falling back to CPU.\")\n",
    "\n",
    "################################################################################\n",
    "# HYPERPARAMETERS / GLOBAL SETTINGS\n",
    "################################################################################\n",
    "HYPERPARAMS = {\n",
    "    \"OUTPUT_DIR\": \"data/trainingdataoutput\",\n",
    "    # We'll read up to 1000 samples from DB now\n",
    "    \"DB_LIMIT\": 1000,\n",
    "\n",
    "    # Training config\n",
    "    \"BATCH_SIZE\": 8,\n",
    "    \"EPOCHS\": 10,          # up to 20 epochs\n",
    "    \"LEARNING_RATE_MAIN\": 5e-5,  # for AST layers\n",
    "    \"LEARNING_RATE_HEAD\": 1e-4,  # for scalar/quantum/final layers\n",
    "    \"WEIGHT_DECAY\": 1e-4,        # L2 regularization factor\n",
    "    \"DEVICE\": DEVICE,\n",
    "    \"NUM_AST_LAYERS_UNFROZEN\": 4, # partial unfreezing\n",
    "\n",
    "    # Early Stopping\n",
    "    \"PATIENCE\": 5,      # how many epochs of no improvement before stopping\n",
    "    \"STOP_LIMIT\": 5,   # must train at least this many epochs before early stop\n",
    "\n",
    "    # AST image sizing\n",
    "    \"AST_FREQ\": 128,\n",
    "    \"AST_TIME\": 1024,\n",
    "}\n",
    "\n",
    "################################################################################\n",
    "# Minimal DB to fetch analysis_data\n",
    "################################################################################\n",
    "class QuantumMusicDBFetchOnly:\n",
    "    \"\"\"\n",
    "    Minimal class to fetch analysis_data from the DB for ML.\n",
    "    Now includes a LIMIT for reading a fixed number of rows.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 db_name=\"quantummusic\",\n",
    "                 host=\"localhost\",\n",
    "                 user=\"postgres\",\n",
    "                 password=\"postgres\"):\n",
    "        import psycopg2\n",
    "        self.psycopg2 = psycopg2\n",
    "        self.db_name = db_name\n",
    "        self.host = host\n",
    "        self.user = user\n",
    "        self.password = password\n",
    "        self.conn = None\n",
    "        self.connect()\n",
    "\n",
    "    def connect(self):\n",
    "        try:\n",
    "            self.conn = self.psycopg2.connect(\n",
    "                dbname=self.db_name,\n",
    "                host=self.host,\n",
    "                user=self.user,\n",
    "                password=self.password\n",
    "            )\n",
    "            print(f\"Connected to database {self.db_name}. (fetch-only)\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error connecting to database: {e}\")\n",
    "\n",
    "    def close(self):\n",
    "        if self.conn:\n",
    "            self.conn.close()\n",
    "            print(\"Database connection closed.\")\n",
    "\n",
    "    def fetch_limited_analysis_data(self, limit=42): #all records, given that we have only about 1700\n",
    "        \"\"\"\n",
    "        Read a fixed 'limit' number of rows from 'audio_analysis' table.\n",
    "        Default limit=1000 now.\n",
    "        \"\"\"\n",
    "        with self.conn.cursor() as cur:\n",
    "            #query = f\"SELECT id, analysis_data FROM audio_analysis LIMIT {limit}\"\n",
    "            query = \"\"\"\n",
    "                WITH cte AS (\n",
    "                    SELECT\n",
    "                        id,\n",
    "                        analysis_data,\n",
    "                        -- Extract the digits as an integer\n",
    "                        substring(file_name FROM '\\\\d+')::int AS file_num,\n",
    "                        -- Assign a random order within each file_num group\n",
    "                        ROW_NUMBER() OVER (\n",
    "                            PARTITION BY substring(file_name FROM '\\\\d+')::int\n",
    "                            ORDER BY random()\n",
    "                        ) AS rn\n",
    "                    FROM audio_analysis\n",
    "                    WHERE substring(file_name FROM '\\\\d+')::int IN (2, 3, 4, 5)\n",
    "                )\n",
    "                SELECT id, analysis_data\n",
    "                FROM cte\n",
    "                WHERE rn <= 42;\n",
    "                \"\"\"\n",
    "            cur.execute(query)\n",
    "            rows = cur.fetchall()\n",
    "        return rows\n",
    "\n",
    "    def fetch_single_record(self, record_id):\n",
    "        with self.conn.cursor() as cur:\n",
    "            query = \"SELECT id, analysis_data FROM audio_analysis WHERE id = %s\"\n",
    "            cur.execute(query, (record_id,))\n",
    "            row = cur.fetchone()\n",
    "        return row\n",
    "\n",
    "################################################################################\n",
    "# Convert quantum measurement counts -> probability vector\n",
    "################################################################################\n",
    "def convert_counts_to_probs_feature(counts_dict, max_bits=10):\n",
    "    \"\"\"\n",
    "    Creates a probability distribution vector of length 2^max_bits.\n",
    "    \"\"\"\n",
    "    total_counts = sum(counts_dict.values())\n",
    "    if total_counts == 0:\n",
    "        return np.zeros(2**max_bits, dtype=np.float32)\n",
    "\n",
    "    feature_vec = np.zeros(2**max_bits, dtype=np.float32)\n",
    "    for bitstring, c in counts_dict.items():\n",
    "        if len(bitstring) > max_bits:\n",
    "            truncated = bitstring[-max_bits:]\n",
    "        else:\n",
    "            truncated = bitstring.rjust(max_bits, '0')\n",
    "        idx = int(truncated, 2)\n",
    "        feature_vec[idx] += c / total_counts\n",
    "    return feature_vec\n",
    "\n",
    "################################################################################\n",
    "# parse_raga_and_quality\n",
    "################################################################################\n",
    "def parse_raga_and_quality(fname: str):\n",
    "    base = fname.replace(\".wav\", \"\")\n",
    "    match = re.match(r\"^([A-Za-z]+)([1-5])(.*)\", base)\n",
    "    if not match:\n",
    "        return None, None\n",
    "    raga = match.group(1)\n",
    "    quality_str = match.group(2)\n",
    "    quality = int(quality_str)\n",
    "    return raga, quality\n",
    "\n",
    "################################################################################\n",
    "# load_image_as_tensor\n",
    "################################################################################\n",
    "transform_img = T.Compose([\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "def load_image_as_tensor(image_path: str):\n",
    "    if not os.path.exists(image_path):\n",
    "        return None\n",
    "    with Image.open(image_path) as img:\n",
    "        img = img.convert(\"L\")  # single-channel grayscale\n",
    "    return transform_img(img)\n",
    "\n",
    "################################################################################\n",
    "# fetch_training_data_v2\n",
    "################################################################################\n",
    "def fetch_training_data_v2(limit=None):\n",
    "    db = QuantumMusicDBFetchOnly()\n",
    "    limit_to_use = limit if limit is not None else HYPERPARAMS[\"DB_LIMIT\"]\n",
    "    rows = db.fetch_limited_analysis_data(limit=limit_to_use)\n",
    "    db.close()\n",
    "\n",
    "    if not rows:\n",
    "        print(\"No data found in DB.\")\n",
    "        return None\n",
    "\n",
    "    images_mfcc = []\n",
    "    images_log = []\n",
    "    scalar_feats = []\n",
    "    quantum_feats = []\n",
    "    labels_raga = []\n",
    "    labels_quality = []\n",
    "\n",
    "    for (record_id, analysis_data) in rows:\n",
    "        wav_fname = analysis_data[\"file_name\"]\n",
    "        base_no_ext = wav_fname.replace(\".wav\", \"\")\n",
    "\n",
    "        # parse raga & quality\n",
    "        raga, quality = parse_raga_and_quality(wav_fname)\n",
    "        if raga is None or quality is None:\n",
    "            continue\n",
    "        labels_raga.append(raga)\n",
    "        labels_quality.append(quality)\n",
    "\n",
    "        # load images\n",
    "        mfcc_path = os.path.join(\"data\", \"analysisoutput\", f\"{base_no_ext}_mfcc.png\")\n",
    "        log_path  = os.path.join(\"data\", \"analysisoutput\", f\"{base_no_ext}_log_spectrogram.png\")\n",
    "\n",
    "        mfcc_img = load_image_as_tensor(mfcc_path)\n",
    "        log_img  = load_image_as_tensor(log_path)\n",
    "\n",
    "        if mfcc_img is None:\n",
    "            mfcc_img = torch.zeros((1,1))\n",
    "        if log_img is None:\n",
    "            log_img = torch.zeros((1,1))\n",
    "\n",
    "        images_mfcc.append(mfcc_img)\n",
    "        images_log.append(log_img)\n",
    "\n",
    "        # scalar\n",
    "        res = analysis_data.get(\"results\", {})\n",
    "        dyn = analysis_data.get(\"dynamics_summary\", {})\n",
    "        adv = analysis_data.get(\"quantum_analysis\", {}).get(\"advanced_stats\", {})\n",
    "\n",
    "        avg_dev = res.get(\"average_dev_cents\", 0.0)\n",
    "        std_dev = res.get(\"std_dev_cents\", 0.0)\n",
    "        avg_hnr = res.get(\"avg_praat_hnr\", 0.0)\n",
    "        avg_tnr = res.get(\"avg_tnr\", 0.0)\n",
    "\n",
    "        rms_db_stats = dyn.get(\"rms_db\", {})\n",
    "        mean_rms_db  = rms_db_stats.get(\"mean\", 0.0)\n",
    "        lufs_stats   = dyn.get(\"lufs\", {})\n",
    "        mean_lufs    = lufs_stats.get(\"mean\", 0.0)\n",
    "\n",
    "        avg_jitter   = adv.get(\"avg_jitter\", 0.0)\n",
    "        avg_shimmer  = adv.get(\"avg_shimmer\", 0.0)\n",
    "        avg_vibrato  = adv.get(\"avg_vibrato_rate\", 0.0)\n",
    "        avg_formant  = adv.get(\"avg_F1\", 0.0)\n",
    "\n",
    "        scalars = [\n",
    "            avg_dev, std_dev, avg_hnr, avg_tnr,\n",
    "            mean_rms_db, mean_lufs,\n",
    "            avg_jitter, avg_shimmer, avg_vibrato, avg_formant\n",
    "        ]\n",
    "        scalar_feats.append(scalars)\n",
    "\n",
    "        # quantum\n",
    "        quantum_dict = analysis_data.get(\"quantum_analysis\", {})\n",
    "        angles = quantum_dict.get(\"scaled_angles\", [])\n",
    "        max_len = 10\n",
    "        angle_arr = np.zeros(max_len, dtype=np.float32)\n",
    "        for i in range(min(max_len, len(angles))):\n",
    "            angle_arr[i] = angles[i]\n",
    "\n",
    "        counts_d = quantum_dict.get(\"measurement_counts\", {})\n",
    "        dist_vec = convert_counts_to_probs_feature(counts_d, max_bits=10)\n",
    "        combined_q = np.concatenate([angle_arr, dist_vec], axis=0)\n",
    "        quantum_feats.append(combined_q)\n",
    "\n",
    "    images_mfcc = torch.stack(images_mfcc)\n",
    "    images_log  = torch.stack(images_log)\n",
    "    scalar_feats = np.array(scalar_feats, dtype=np.float32)\n",
    "    quantum_feats = np.array(quantum_feats, dtype=np.float32)\n",
    "\n",
    "    # label encodings\n",
    "    unique_ragas = sorted(list(set(labels_raga)))\n",
    "    raga_to_idx = {r: i for i, r in enumerate(unique_ragas)}\n",
    "    labels_raga_idx = [raga_to_idx[r] for r in labels_raga]\n",
    "\n",
    "    labels_quality_arr = np.array([q - 1 for q in labels_quality], dtype=np.int64)\n",
    "\n",
    "    data_dict = {\n",
    "        \"images_mfcc\": images_mfcc,\n",
    "        \"images_log\": images_log,\n",
    "        \"scalar_feats\": scalar_feats,\n",
    "        \"quantum_feats\": quantum_feats,\n",
    "        \"label_raga_idx\": np.array(labels_raga_idx, dtype=np.int64),\n",
    "        \"label_quality\": labels_quality_arr,\n",
    "        \"raga_to_idx\": raga_to_idx,\n",
    "        \"unique_ragas\": unique_ragas\n",
    "    }\n",
    "    return data_dict\n",
    "\n",
    "################################################################################\n",
    "# MultiLabelASTDataset\n",
    "################################################################################\n",
    "class MultiLabelASTDataset(Dataset):\n",
    "    def __init__(self, data_dict):\n",
    "        self.mfcc_imgs = data_dict[\"images_mfcc\"]\n",
    "        self.log_imgs  = data_dict[\"images_log\"]\n",
    "        self.scalars   = data_dict[\"scalar_feats\"]\n",
    "        self.quants    = data_dict[\"quantum_feats\"]\n",
    "        self.raga_lbl  = data_dict[\"label_raga_idx\"]\n",
    "        self.qual_lbl  = data_dict[\"label_quality\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raga_lbl)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        scal = torch.tensor(self.scalars[idx], dtype=torch.float32)\n",
    "        qua  = torch.tensor(self.quants[idx], dtype=torch.float32)\n",
    "        rag  = torch.tensor(self.raga_lbl[idx], dtype=torch.long)\n",
    "        qua_lbl = torch.tensor(self.qual_lbl[idx], dtype=torch.long)\n",
    "        return (\n",
    "            self.mfcc_imgs[idx],\n",
    "            self.log_imgs[idx],\n",
    "            scal,\n",
    "            qua,\n",
    "            rag,\n",
    "            qua_lbl\n",
    "        )\n",
    "\n",
    "################################################################################\n",
    "# HybridASTModelV2\n",
    "################################################################################\n",
    "class HybridASTModelV2(nn.Module):\n",
    "    def __init__(self, num_ragas, scalar_dim=10, quantum_dim=42,\n",
    "                 num_quality=5, num_unfrozen_layers=0):\n",
    "        super().__init__()\n",
    "        self.config = ASTConfig.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "        self.ast_model = ASTModel.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\",\n",
    "                                                  config=self.config)\n",
    "\n",
    "        # partial unfreeze\n",
    "        for param in self.ast_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        if num_unfrozen_layers > 0:\n",
    "            total_layers = 12\n",
    "            start_layer = max(0, total_layers - num_unfrozen_layers)\n",
    "            for layer_idx in range(start_layer, total_layers):\n",
    "                for param in self.ast_model.encoder.layer[layer_idx].parameters():\n",
    "                    param.requires_grad = True\n",
    "\n",
    "        # Add dropout to final heads for more regularization (optional)\n",
    "        self.scalar_fc = nn.Sequential(\n",
    "            nn.Linear(scalar_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        self.quantum_fc = nn.Sequential(\n",
    "            nn.Linear(quantum_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "\n",
    "        combined_dim = 768 + 64 + 64\n",
    "        self.raga_head = nn.Sequential(\n",
    "            nn.Linear(combined_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, num_ragas)\n",
    "        )\n",
    "        self.quality_head = nn.Sequential(\n",
    "            nn.Linear(combined_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, num_quality)\n",
    "        )\n",
    "\n",
    "    def forward(self, mfcc_img, log_img, scal_in, quan_in):\n",
    "        B, C, H, W = log_img.shape\n",
    "        freq = HYPERPARAMS[\"AST_FREQ\"]\n",
    "        time = HYPERPARAMS[\"AST_TIME\"]\n",
    "\n",
    "        log_resized = F.interpolate(\n",
    "            log_img,\n",
    "            size=(freq, time),\n",
    "            mode='bilinear',\n",
    "            align_corners=False\n",
    "        )\n",
    "        log_for_ast = log_resized.squeeze(1)  # (B,128,1024)\n",
    "\n",
    "        outputs = self.ast_model(input_values=log_for_ast, output_hidden_states=True)\n",
    "        hidden = outputs.last_hidden_state  # (B, seq_len, 768)\n",
    "        ast_embedding = hidden[:, 0, :]     # (B,768)\n",
    "\n",
    "        emb_scal = self.scalar_fc(scal_in)\n",
    "        emb_quan = self.quantum_fc(quan_in)\n",
    "        fused = torch.cat([ast_embedding, emb_scal, emb_quan], dim=1)  # (B,896)\n",
    "\n",
    "        logits_raga = self.raga_head(fused)\n",
    "        logits_quality = self.quality_head(fused)\n",
    "        return logits_raga, logits_quality\n",
    "\n",
    "################################################################################\n",
    "# train_model_v2 with Minimum EPOCHS + Early Stopping, L2, Confusion Plots\n",
    "################################################################################\n",
    "def train_model_v2():\n",
    "    data_dict = fetch_training_data_v2(limit=HYPERPARAMS[\"DB_LIMIT\"])\n",
    "    if data_dict is None:\n",
    "        print(\"No data to train.\")\n",
    "        return None, None\n",
    "\n",
    "    scalar_feats = data_dict[\"scalar_feats\"]\n",
    "    quantum_feats = data_dict[\"quantum_feats\"]\n",
    "    raga_to_idx = data_dict[\"raga_to_idx\"]\n",
    "    num_ragas = len(raga_to_idx)\n",
    "\n",
    "    dataset_full = MultiLabelASTDataset(data_dict)\n",
    "    n_samples = len(dataset_full)\n",
    "    test_size = int(0.2 * n_samples)\n",
    "    train_size = n_samples - test_size\n",
    "    train_ds, test_ds = torch.utils.data.random_split(dataset_full, [train_size, test_size])\n",
    "\n",
    "    train_dl = DataLoader(train_ds, batch_size=HYPERPARAMS[\"BATCH_SIZE\"], shuffle=True)\n",
    "    test_dl  = DataLoader(test_ds, batch_size=HYPERPARAMS[\"BATCH_SIZE\"], shuffle=False)\n",
    "\n",
    "    model = HybridASTModelV2(\n",
    "        num_ragas=num_ragas,\n",
    "        scalar_dim=scalar_feats.shape[1],\n",
    "        quantum_dim=quantum_feats.shape[1],\n",
    "        num_quality=5,\n",
    "        num_unfrozen_layers=HYPERPARAMS[\"NUM_AST_LAYERS_UNFROZEN\"]\n",
    "    ).to(HYPERPARAMS[\"DEVICE\"])\n",
    "\n",
    "    # L2 => weight_decay\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=HYPERPARAMS[\"LEARNING_RATE_HEAD\"],\n",
    "        weight_decay=HYPERPARAMS[\"WEIGHT_DECAY\"]\n",
    "    )\n",
    "\n",
    "    crit_ce = nn.CrossEntropyLoss()\n",
    "    EPOCHS = HYPERPARAMS[\"EPOCHS\"]\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    best_test_loss = float(\"inf\")\n",
    "    best_model_state = None\n",
    "    patience = HYPERPARAMS[\"PATIENCE\"]\n",
    "    epochs_no_improve = 0\n",
    "    stop_limit = HYPERPARAMS[\"STOP_LIMIT\"]  # must do at least this many epochs\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "\n",
    "        for mfcc_img, log_img, scal, quan, rag_lbl, qual_lbl in train_dl:\n",
    "            mfcc_img = mfcc_img.to(HYPERPARAMS[\"DEVICE\"])\n",
    "            log_img  = log_img.to(HYPERPARAMS[\"DEVICE\"])\n",
    "            scal     = scal.to(HYPERPARAMS[\"DEVICE\"])\n",
    "            quan     = quan.to(HYPERPARAMS[\"DEVICE\"])\n",
    "            rag_lbl  = rag_lbl.to(HYPERPARAMS[\"DEVICE\"])\n",
    "            qual_lbl = qual_lbl.to(HYPERPARAMS[\"DEVICE\"])\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits_raga, logits_quality = model(mfcc_img, log_img, scal, quan)\n",
    "            loss_raga = crit_ce(logits_raga, rag_lbl)\n",
    "            loss_qual = crit_ce(logits_quality, qual_lbl)\n",
    "            loss = loss_raga + loss_qual\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_dl)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Evaluate\n",
    "        model.eval()\n",
    "        total_test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for mfcc_img, log_img, scal, quan, rag_lbl, qual_lbl in test_dl:\n",
    "                mfcc_img = mfcc_img.to(HYPERPARAMS[\"DEVICE\"])\n",
    "                log_img  = log_img.to(HYPERPARAMS[\"DEVICE\"])\n",
    "                scal     = scal.to(HYPERPARAMS[\"DEVICE\"])\n",
    "                quan     = quan.to(HYPERPARAMS[\"DEVICE\"])\n",
    "                rag_lbl  = rag_lbl.to(HYPERPARAMS[\"DEVICE\"])\n",
    "                qual_lbl = qual_lbl.to(HYPERPARAMS[\"DEVICE\"])\n",
    "\n",
    "                logits_raga, logits_quality = model(mfcc_img, log_img, scal, quan)\n",
    "                loss_raga = crit_ce(logits_raga, rag_lbl)\n",
    "                loss_qual = crit_ce(logits_quality, qual_lbl)\n",
    "                total_test_loss += (loss_raga + loss_qual).item()\n",
    "\n",
    "        avg_test_loss = total_test_loss / len(test_dl)\n",
    "        test_losses.append(avg_test_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}: train_loss={avg_train_loss:.4f}, test_loss={avg_test_loss:.4f}\")\n",
    "\n",
    "        # Early Stopping Check\n",
    "        if avg_test_loss < best_test_loss:\n",
    "            best_test_loss = avg_test_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            # Only consider stopping if we've run at least stop_limit epochs\n",
    "            if (epoch + 1) >= stop_limit and epochs_no_improve >= patience:\n",
    "                print(f\"No improvement for {patience} epochs after epoch {stop_limit}, \"\n",
    "                      f\"stopping early at epoch {epoch+1}.\")\n",
    "                break\n",
    "\n",
    "    print(\"Training complete or early stopped.\")\n",
    "\n",
    "    # If we found a best_model_state, load it back\n",
    "    if best_model_state is not None:\n",
    "        print(\"Loading best model state (lowest test loss)...\")\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    # Final pass for confusion matrices\n",
    "    all_raga_preds, all_raga_truth = [], []\n",
    "    all_qual_preds, all_qual_truth = [], []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for mfcc_img, log_img, scal, quan, rag_lbl, qual_lbl in test_dl:\n",
    "            mfcc_img = mfcc_img.to(HYPERPARAMS[\"DEVICE\"])\n",
    "            log_img  = log_img.to(HYPERPARAMS[\"DEVICE\"])\n",
    "            scal     = scal.to(HYPERPARAMS[\"DEVICE\"])\n",
    "            quan     = quan.to(HYPERPARAMS[\"DEVICE\"])\n",
    "            rag_lbl  = rag_lbl.to(HYPERPARAMS[\"DEVICE\"])\n",
    "            qual_lbl = qual_lbl.to(HYPERPARAMS[\"DEVICE\"])\n",
    "\n",
    "            logits_raga, logits_quality = model(mfcc_img, log_img, scal, quan)\n",
    "            pred_raga = logits_raga.argmax(dim=1).cpu().numpy()\n",
    "            pred_qual = logits_quality.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "            all_raga_preds.extend(pred_raga)\n",
    "            all_raga_truth.extend(rag_lbl.cpu().numpy())\n",
    "            all_qual_preds.extend(pred_qual)\n",
    "            all_qual_truth.extend(qual_lbl.cpu().numpy())\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "    cm_raga = confusion_matrix(all_raga_truth, all_raga_preds)\n",
    "    used_raga_indices = sorted(set(all_raga_truth) | set(all_raga_preds))\n",
    "    idx_to_raga = {v: k for k, v in data_dict[\"raga_to_idx\"].items()}\n",
    "    used_raga_labels = [idx_to_raga[i] for i in used_raga_indices]\n",
    "\n",
    "    disp_raga = ConfusionMatrixDisplay(cm_raga, display_labels=used_raga_labels)\n",
    "    disp_raga.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(\"Raga Confusion Matrix (Best Model)\")\n",
    "    plt.show()\n",
    "\n",
    "    cm_qual = confusion_matrix(all_qual_truth, all_qual_preds)\n",
    "    used_qual_indices = sorted(set(all_qual_truth) | set(all_qual_preds))\n",
    "    used_qual_labels  = [q+1 for q in used_qual_indices]\n",
    "    disp_qual = ConfusionMatrixDisplay(cm_qual, display_labels=used_qual_labels)\n",
    "    disp_qual.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(\"Quality Confusion Matrix (Best Model)\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot training curves\n",
    "    plt.figure()\n",
    "    plt.plot(train_losses, label=\"Train Loss\")\n",
    "    plt.plot(test_losses, label=\"Test Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Curves (Min. EPOCHS + Early Stopping + L2)\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Save best model + metadata => data/modeloutput/trained_model.pt\n",
    "    os.makedirs(\"data/modeloutput\", exist_ok=True)\n",
    "    checkpoint_path = os.path.join(\"data\", \"modeloutput\", \"trained_model.pt\")\n",
    "\n",
    "    checkpoint_dict = {\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"raga_to_idx\": data_dict[\"raga_to_idx\"],\n",
    "        \"scalar_dim\": scalar_feats.shape[1],\n",
    "        \"quantum_dim\": quantum_feats.shape[1],\n",
    "        \"num_ragas\": len(data_dict[\"raga_to_idx\"])\n",
    "    }\n",
    "    torch.save(checkpoint_dict, checkpoint_path)\n",
    "    print(f\"Best model + metadata saved to: {checkpoint_path}\")\n",
    "\n",
    "    return model, data_dict[\"raga_to_idx\"]\n",
    "\n",
    "################################################################################\n",
    "# run_inference_on_persisted_model\n",
    "################################################################################\n",
    "def run_inference_on_persisted_model(record_id, model_path=\"data/modeloutput/trained_model.pt\"):\n",
    "    \"\"\"\n",
    "    Loads the persisted model+metadata from 'model_path', re-creates the model,\n",
    "    and runs inference on DB record {record_id}. Prints predicted raga + quality.\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(model_path, map_location=HYPERPARAMS[\"DEVICE\"])\n",
    "    model_state = checkpoint[\"model_state\"]\n",
    "    raga_to_idx = checkpoint[\"raga_to_idx\"]\n",
    "    scalar_dim = checkpoint[\"scalar_dim\"]\n",
    "    quantum_dim = checkpoint[\"quantum_dim\"]\n",
    "    num_ragas = checkpoint[\"num_ragas\"]\n",
    "\n",
    "    model = HybridASTModelV2(\n",
    "        num_ragas=num_ragas,\n",
    "        scalar_dim=scalar_dim,\n",
    "        quantum_dim=quantum_dim,\n",
    "        num_quality=5,\n",
    "        num_unfrozen_layers=HYPERPARAMS[\"NUM_AST_LAYERS_UNFROZEN\"]\n",
    "    )\n",
    "    model.load_state_dict(model_state)\n",
    "    model.to(HYPERPARAMS[\"DEVICE\"])\n",
    "    model.eval()\n",
    "\n",
    "    db = QuantumMusicDBFetchOnly()\n",
    "    row = db.fetch_single_record(record_id)\n",
    "    db.close()\n",
    "\n",
    "    if not row:\n",
    "        print(f\"No record found with ID={record_id}\")\n",
    "        return None, None\n",
    "\n",
    "    _, analysis_data = row\n",
    "    wav_fname = analysis_data[\"file_name\"]\n",
    "    base_no_ext = wav_fname.replace(\".wav\", \"\")\n",
    "\n",
    "    raga_true, quality_true = parse_raga_and_quality(wav_fname)\n",
    "\n",
    "    mfcc_path = os.path.join(\"data\", \"analysisoutput\", f\"{base_no_ext}_mfcc.png\")\n",
    "    log_path  = os.path.join(\"data\", \"analysisoutput\", f\"{base_no_ext}_log_spectrogram.png\")\n",
    "    mfcc_img = load_image_as_tensor(mfcc_path)\n",
    "    if mfcc_img is None:\n",
    "        mfcc_img = torch.zeros((1,1))\n",
    "\n",
    "    log_img = load_image_as_tensor(log_path)\n",
    "    if log_img is None:\n",
    "        log_img = torch.zeros((1,1))\n",
    "    \n",
    "    res = analysis_data.get(\"results\", {})\n",
    "    dyn = analysis_data.get(\"dynamics_summary\", {})\n",
    "    adv = analysis_data.get(\"quantum_analysis\", {}).get(\"advanced_stats\", {})\n",
    "\n",
    "    avg_dev = res.get(\"average_dev_cents\", 0.0)\n",
    "    std_dev = res.get(\"std_dev_cents\", 0.0)\n",
    "    avg_hnr = res.get(\"avg_praat_hnr\", 0.0)\n",
    "    avg_tnr = res.get(\"avg_tnr\", 0.0)\n",
    "\n",
    "    rms_db_stats = dyn.get(\"rms_db\", {})\n",
    "    mean_rms_db  = rms_db_stats.get(\"mean\", 0.0)\n",
    "    lufs_stats   = dyn.get(\"lufs\", {})\n",
    "    mean_lufs    = lufs_stats.get(\"mean\", 0.0)\n",
    "\n",
    "    avg_jitter   = adv.get(\"avg_jitter\", 0.0)\n",
    "    avg_shimmer  = adv.get(\"avg_shimmer\", 0.0)\n",
    "    avg_vibrato  = adv.get(\"avg_vibrato_rate\", 0.0)\n",
    "    avg_formant  = adv.get(\"avg_F1\", 0.0)\n",
    "\n",
    "    scalars = [\n",
    "        avg_dev, std_dev, avg_hnr, avg_tnr,\n",
    "        mean_rms_db, mean_lufs,\n",
    "        avg_jitter, avg_shimmer, avg_vibrato, avg_formant\n",
    "    ]\n",
    "\n",
    "    quantum_dict = analysis_data.get(\"quantum_analysis\", {})\n",
    "    angles = quantum_dict.get(\"scaled_angles\", [])\n",
    "    max_len = 10\n",
    "    angle_arr = np.zeros(max_len, dtype=np.float32)\n",
    "    for i in range(min(max_len, len(angles))):\n",
    "        angle_arr[i] = angles[i]\n",
    "\n",
    "    counts_d = quantum_dict.get(\"measurement_counts\", {})\n",
    "    dist_vec = convert_counts_to_probs_feature(counts_d, max_bits=10)\n",
    "    combined_q = np.concatenate([angle_arr, dist_vec], axis=0)\n",
    "\n",
    "    device = HYPERPARAMS[\"DEVICE\"]\n",
    "    mfcc_img = mfcc_img.unsqueeze(0).to(device)\n",
    "    log_img  = log_img.unsqueeze(0).to(device)\n",
    "    scal_ten = torch.tensor(scalars, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    quan_ten = torch.tensor(combined_q, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits_raga, logits_quality = model(mfcc_img, log_img, scal_ten, quan_ten)\n",
    "        pred_raga_idx = torch.argmax(logits_raga, dim=1).item()\n",
    "        pred_qual_idx = torch.argmax(logits_quality, dim=1).item()\n",
    "\n",
    "    inv_map = {v: k for k, v in raga_to_idx.items()}\n",
    "    pred_raga_str = inv_map.get(pred_raga_idx, \"UnknownRaga\")\n",
    "    pred_quality = pred_qual_idx + 1\n",
    "\n",
    "    print(f\"Inference for Record ID={record_id}:\")\n",
    "    print(f\"  True raga={raga_true}, True quality={quality_true}\")\n",
    "    print(f\"  Predicted raga={pred_raga_str}, predicted quality={pred_quality}\")\n",
    "    return pred_raga_str, pred_quality\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# Example usage\n",
    "################################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Train the model (with at least STOP_LIMIT=10 epochs) + Early Stopping\n",
    "    #model, raga_to_idx = train_model_v2()\n",
    "\n",
    "    # 2) Later or in a separate session:\n",
    "    run_inference_on_persisted_model(361, \"data/modeloutput/trained_model_42_balanced.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantumvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
