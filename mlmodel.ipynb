{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchaudio.transforms import MelSpectrogram\n",
    "import librosa\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from transformers import ASTModel\n",
    "\n",
    "# Settings\n",
    "OUTPUT_DIR = \"data/trainingdataoutput\"\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE_MAIN = 5e-5\n",
    "LEARNING_RATE_HEAD = 1e-4\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "###########################################################################\n",
    "# Minimal DB to fetch analysis_data\n",
    "###########################################################################\n",
    "class QuantumMusicDBFetchOnly:\n",
    "    \"\"\"Minimal class to fetch analysis_data from the DB for ML.\"\"\"\n",
    "    def __init__(self, db_name=\"quantummusic\", host=\"localhost\", user=\"postgres\", password=\"postgres\"):\n",
    "        import psycopg2\n",
    "        self.psycopg2 = psycopg2\n",
    "        self.db_name = db_name\n",
    "        self.host = host\n",
    "        self.user = user\n",
    "        self.password = password\n",
    "        self.conn = None\n",
    "        self.connect()\n",
    "\n",
    "    def connect(self):\n",
    "        try:\n",
    "            self.conn = self.psycopg2.connect(\n",
    "                dbname=self.db_name,\n",
    "                host=self.host,\n",
    "                user=self.user,\n",
    "                password=self.password\n",
    "            )\n",
    "            print(f\"Connected to database {self.db_name}. (fetch-only)\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error connecting to database: {e}\")\n",
    "\n",
    "    def close(self):\n",
    "        if self.conn:\n",
    "            self.conn.close()\n",
    "            print(\"Database connection closed.\")\n",
    "\n",
    "    def fetch_all_analysis_data(self):\n",
    "        with self.conn.cursor() as cur:\n",
    "            cur.execute(\"SELECT analysis_data FROM audio_analysis\")\n",
    "            rows = cur.fetchall()\n",
    "        return rows\n",
    "\n",
    "\n",
    "###########################################################################\n",
    "# Convert quantum measurement counts -> probability vector\n",
    "###########################################################################\n",
    "def convert_counts_to_probs_feature(counts_dict, max_bits=5):\n",
    "    total_counts = sum(counts_dict.values())\n",
    "    if total_counts == 0:\n",
    "        return np.zeros(2**max_bits, dtype=np.float32)\n",
    "\n",
    "    feature_vec = np.zeros(2**max_bits, dtype=np.float32)\n",
    "    for bitstring, c in counts_dict.items():\n",
    "        if len(bitstring) > max_bits:\n",
    "            truncated = bitstring[-max_bits:]\n",
    "        else:\n",
    "            truncated = bitstring.rjust(max_bits, '0')\n",
    "        idx = int(truncated, 2)\n",
    "        feature_vec[idx] += c / total_counts\n",
    "    return feature_vec\n",
    "\n",
    "\n",
    "###########################################################################\n",
    "# Fetch Training Data\n",
    "###########################################################################\n",
    "def fetch_training_data():\n",
    "    db = QuantumMusicDBFetchOnly()\n",
    "    rows = db.fetch_all_analysis_data()\n",
    "    db.close()\n",
    "    if not rows:\n",
    "        print(\"No data found in DB.\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    audio_feats, scalar_feats, quantum_feats, labels = [], [], [], []\n",
    "    # We'll also store distribution-based quantum features\n",
    "    dist_feats = []\n",
    "\n",
    "    for (analysis_data,) in rows:\n",
    "        # file path\n",
    "        fpath = os.path.join(OUTPUT_DIR, analysis_data[\"file_name\"])\n",
    "        if not os.path.exists(fpath):\n",
    "            continue\n",
    "\n",
    "        # load audio => mel spec\n",
    "        y, sr = librosa.load(fpath, sr=None)\n",
    "        meltr = MelSpectrogram(n_mels=128)\n",
    "        mel = meltr(torch.tensor(y).float()).numpy()  # shape(128, frames)\n",
    "        mel = np.expand_dims(mel, axis=0)             # shape(1, 128, frames)\n",
    "        audio_feats.append(mel)\n",
    "\n",
    "        # gather scalar features\n",
    "        res = analysis_data.get(\"results\", {})\n",
    "        avg_dev = res.get(\"average_dev_cents\", 0.0)\n",
    "        std_dev = res.get(\"std_dev_cents\", 0.0)\n",
    "        avg_hnr = res.get(\"avg_praat_hnr\", 0.0)\n",
    "        avg_tnr = res.get(\"avg_tnr\", 0.0)\n",
    "        # placeholder\n",
    "        scal = [avg_dev, std_dev, avg_hnr, avg_tnr, 0.0, 0.0]\n",
    "        scalar_feats.append(scal)\n",
    "\n",
    "        # gather quantum angles\n",
    "        quantum_dict = analysis_data.get(\"quantum_analysis\", {})\n",
    "        angles = quantum_dict.get(\"scaled_angles\", [])\n",
    "        max_len = 10\n",
    "        angle_arr = np.zeros(max_len, dtype=np.float32)\n",
    "        for i in range(min(max_len, len(angles))):\n",
    "            angle_arr[i] = angles[i]\n",
    "\n",
    "        # gather quantum distribution\n",
    "        qv = analysis_data.get(\"quantum_analysis_variational\", {})\n",
    "        counts_d = qv.get(\"counts\", {})\n",
    "        dist_vec = convert_counts_to_probs_feature(counts_d, max_bits=5)  # shape(32,)\n",
    "\n",
    "        quantum_feats.append(angle_arr)\n",
    "        dist_feats.append(dist_vec)\n",
    "\n",
    "        # label\n",
    "        lab = random.randint(0, 1)\n",
    "        labels.append(lab)\n",
    "\n",
    "    # combine quantum data\n",
    "    final_q_feats = []\n",
    "    for i in range(len(quantum_feats)):\n",
    "        concat_ = np.concatenate([quantum_feats[i], dist_feats[i]], axis=0)\n",
    "        final_q_feats.append(concat_)\n",
    "    final_q_feats = np.array(final_q_feats, dtype=np.float32)\n",
    "\n",
    "    return (\n",
    "        np.array(audio_feats, dtype=np.float32),\n",
    "        np.array(scalar_feats, dtype=np.float32),\n",
    "        final_q_feats,\n",
    "        np.array(labels, dtype=np.int64),\n",
    "    )\n",
    "\n",
    "\n",
    "###########################################################################\n",
    "# PyTorch Dataset\n",
    "###########################################################################\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, aud, scal, quan, labs):\n",
    "        self.aud = torch.tensor(aud, dtype=torch.float32)\n",
    "        self.scal = torch.tensor(scal, dtype=torch.float32)\n",
    "        self.quan = torch.tensor(quan, dtype=torch.float32)\n",
    "        self.labs = torch.tensor(labs, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.aud[idx],\n",
    "            self.scal[idx],\n",
    "            self.quan[idx],\n",
    "            self.labs[idx]\n",
    "        )\n",
    "\n",
    "\n",
    "###########################################################################\n",
    "# Hybrid AST Model\n",
    "###########################################################################\n",
    "class HybridASTModel(nn.Module):\n",
    "    def __init__(self, scalar_dim, quantum_dim, output_dim=2, freeze_ast=True):\n",
    "        super().__init__()\n",
    "        self.ast = ASTModel.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "\n",
    "        if freeze_ast:\n",
    "            for p in self.ast.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        self.scalar_fc = nn.Sequential(\n",
    "            nn.Linear(scalar_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.quantum_fc = nn.Sequential(\n",
    "            nn.Linear(quantum_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # pooler_output => 768 dims\n",
    "        self.final_fc = nn.Linear(768 + 64 + 64, output_dim)\n",
    "\n",
    "    def forward(self, audio_input, scalar_input, quantum_input):\n",
    "        ast_out = self.ast(audio_input).pooler_output\n",
    "        s = self.scalar_fc(scalar_input)\n",
    "        q = self.quantum_fc(quantum_input)\n",
    "        fused = torch.cat([ast_out, s, q], dim=1)\n",
    "        return self.final_fc(fused)\n",
    "\n",
    "\n",
    "###########################################################################\n",
    "# Training Pipeline\n",
    "###########################################################################\n",
    "def train_model():\n",
    "    data = fetch_training_data()\n",
    "    if data[0] is None:\n",
    "        print(\"No data to train.\")\n",
    "        return None\n",
    "    audio_features, scalar_features, quantum_features, labels = data\n",
    "\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    sc1 = StandardScaler()\n",
    "    scalar_features = sc1.fit_transform(scalar_features)\n",
    "\n",
    "    sc2 = StandardScaler()\n",
    "    quantum_features = sc2.fit_transform(quantum_features)\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    Xaud_tr, Xaud_te, Xs_tr, Xs_te, Xq_tr, Xq_te, y_tr, y_te = train_test_split(\n",
    "        audio_features, scalar_features, quantum_features, labels,\n",
    "        test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    train_ds = AudioDataset(Xaud_tr, Xs_tr, Xq_tr, y_tr)\n",
    "    test_ds  = AudioDataset(Xaud_te, Xs_te, Xq_te, y_te)\n",
    "\n",
    "    train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_dl  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model = HybridASTModel(\n",
    "        scalar_dim=scalar_features.shape[1],\n",
    "        quantum_dim=quantum_features.shape[1],\n",
    "        output_dim=2,\n",
    "        freeze_ast=False\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    # separate param groups\n",
    "    ast_params, head_params = [], []\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"ast.\" in name:\n",
    "            ast_params.append(param)\n",
    "        else:\n",
    "            head_params.append(param)\n",
    "\n",
    "    optimizer = optim.Adam([\n",
    "        {\"params\": ast_params,  \"lr\": LEARNING_RATE_MAIN},\n",
    "        {\"params\": head_params, \"lr\": LEARNING_RATE_HEAD},\n",
    "    ])\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        run_loss = 0\n",
    "        for aud, sc, qu, lab in train_dl:\n",
    "            aud = aud.to(DEVICE)\n",
    "            sc  = sc.to(DEVICE)\n",
    "            qu  = qu.to(DEVICE)\n",
    "            lab = lab.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(aud, sc, qu)\n",
    "            loss = crit(out, lab)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            run_loss += loss.item()\n",
    "        avg_tr_loss = run_loss / len(train_dl)\n",
    "        train_losses.append(avg_tr_loss)\n",
    "\n",
    "        # test\n",
    "        model.eval()\n",
    "        run_loss_te = 0\n",
    "        with torch.no_grad():\n",
    "            for aud, sc, qu, lab in test_dl:\n",
    "                aud, sc, qu = aud.to(DEVICE), sc.to(DEVICE), qu.to(DEVICE)\n",
    "                lab = lab.to(DEVICE)\n",
    "                out = model(aud, sc, qu)\n",
    "                loss = crit(out, lab)\n",
    "                run_loss_te += loss.item()\n",
    "        avg_te_loss = run_loss_te / len(test_dl)\n",
    "        test_losses.append(avg_te_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}, train_loss={avg_tr_loss:.4f}, test_loss={avg_te_loss:.4f}\")\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "    # Confusion matrix\n",
    "    from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "    all_preds, all_labels = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for aud, sc, qu, lab in test_dl:\n",
    "            aud, sc, qu = aud.to(DEVICE), sc.to(DEVICE), qu.to(DEVICE)\n",
    "            preds = model(aud, sc, qu).argmax(dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(lab.numpy())\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    disp = ConfusionMatrixDisplay(cm, display_labels=[0,1])\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot training curves\n",
    "    plt.figure()\n",
    "    plt.plot(train_losses, label=\"Train Loss\")\n",
    "    plt.plot(test_losses, label=\"Test Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Training Curves\")\n",
    "    plt.show()\n",
    "\n",
    "    return model\n",
    "\n",
    "# Example usage if you want to run it directly:\n",
    "# if __name__ == \"__main__\":\n",
    "#     trained_model = train_model()\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "def parse_raga_and_quality(fname: str):\n",
    "    \"\"\"\n",
    "    Example filename: 'Bhairavi5VenkateshKumar_500s_520s_Unknown.wav'\n",
    "    Returns raga='Bhairavi', quality=5\n",
    "    \"\"\"\n",
    "    # Strip any extension\n",
    "    base = fname.replace(\".wav\", \"\")\n",
    "    # Regex: one or more letters, followed by a single digit (1-5)\n",
    "    match = re.match(r\"^([A-Za-z]+)([1-5])(.*)\", base)\n",
    "    if not match:\n",
    "        # fallback\n",
    "        return None, None\n",
    "\n",
    "    raga = match.group(1)\n",
    "    quality_str = match.group(2)\n",
    "    quality = int(quality_str)  # convert \"5\" -> 5\n",
    "    return raga, quality\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "\n",
    "transform_img = T.Compose([\n",
    "    T.Resize((224, 224)),  # example size\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "def load_image_as_tensor(image_path: str):\n",
    "    if not os.path.exists(image_path):\n",
    "        # Could return None or a zero-tensor placeholder\n",
    "        return None\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    return transform_img(img)\n",
    "\n",
    "\n",
    "\n",
    "def fetch_training_data_v2():\n",
    "    db = QuantumMusicDBFetchOnly()\n",
    "    rows = db.fetch_all_analysis_data()\n",
    "    db.close()\n",
    "    if not rows:\n",
    "        print(\"No data found in DB.\")\n",
    "        return None\n",
    "\n",
    "    # We’ll gather the following lists for the dataset:\n",
    "    #  - images_mfcc, images_log, ...\n",
    "    #  - scalar_feats (from results + dynamics_summary + advanced_vocal_stats)\n",
    "    #  - quantum_feats\n",
    "    #  - labels_raga, labels_quality (or you might combine them)\n",
    "\n",
    "    images_mfcc = []\n",
    "    images_log = []\n",
    "    # Similarly for jitter, shimmer, formant, vibrato if desired\n",
    "    # images_jitter = []\n",
    "    # images_shimmer = []\n",
    "    # ...\n",
    "    \n",
    "    scalar_feats = []\n",
    "    quantum_feats = []\n",
    "    labels_raga = []\n",
    "    labels_quality = []\n",
    "\n",
    "    for (analysis_data,) in rows:\n",
    "        # Grab filename from the DB\n",
    "        wav_fname = analysis_data[\"file_name\"]  # e.g. 'Bhairavi5VenkateshKumar_500s_520s_Unknown.wav'\n",
    "        base_no_ext = wav_fname.replace(\".wav\", \"\")\n",
    "        \n",
    "        # 1) Parse raga and quality from file name\n",
    "        raga, quality = parse_raga_and_quality(wav_fname)\n",
    "        if raga is None or quality is None:\n",
    "            # If not parseable, skip or handle somehow\n",
    "            continue\n",
    "        \n",
    "        labels_raga.append(raga)\n",
    "        labels_quality.append(quality)\n",
    "\n",
    "        # 2) Precomputed images\n",
    "        #    data/analysisoutput/<base_no_ext>_mfcc.png\n",
    "        mfcc_path = os.path.join(\"data\", \"analysisoutput\", f\"{base_no_ext}_mfcc.png\")\n",
    "        log_path  = os.path.join(\"data\", \"analysisoutput\", f\"{base_no_ext}_log_spectrogram.png\")\n",
    "        \n",
    "        mfcc_img = load_image_as_tensor(mfcc_path)\n",
    "        log_img  = load_image_as_tensor(log_path)\n",
    "        # etc. for jitter, shimmer, ...\n",
    "        \n",
    "        images_mfcc.append(mfcc_img if mfcc_img is not None else torch.zeros((3,224,224)))\n",
    "        images_log.append(log_img  if log_img  is not None else torch.zeros((3,224,224)))\n",
    "\n",
    "        # 3) Scalar features from \"results\" + \"dynamics_summary\" + possibly \"advanced_vocal_stats\"\n",
    "        #    You can pick whichever ones you want. For example:\n",
    "        res = analysis_data.get(\"results\", {})\n",
    "        dyn = analysis_data.get(\"dynamics_summary\", {})\n",
    "        adv = analysis_data.get(\"quantum_analysis\", {}).get(\"advanced_stats\", {})\n",
    "        \n",
    "        avg_dev = res.get(\"average_dev_cents\", 0.0)\n",
    "        std_dev = res.get(\"std_dev_cents\", 0.0)\n",
    "        avg_hnr = res.get(\"avg_praat_hnr\", 0.0)\n",
    "        avg_tnr = res.get(\"avg_tnr\", 0.0)\n",
    "\n",
    "        # from dynamics_summary\n",
    "        rms_db_stats = dyn.get(\"rms_db\", {})\n",
    "        mean_rms_db  = rms_db_stats.get(\"mean\", 0.0)\n",
    "        lufs_stats   = dyn.get(\"lufs\", {})\n",
    "        mean_lufs    = lufs_stats.get(\"mean\", 0.0)\n",
    "\n",
    "        # from advanced_vocal_stats (inside advanced_stats in quantum_analysis or similar)\n",
    "        avg_jitter   = adv.get(\"avg_jitter\", 0.0)\n",
    "        avg_shimmer  = adv.get(\"avg_shimmer\", 0.0)\n",
    "        avg_vibrato  = adv.get(\"avg_vibrato_rate\", 0.0)\n",
    "        avg_formant  = adv.get(\"avg_F1\", 0.0)  # or you can use multiple formants\n",
    "\n",
    "        # Combine them into one scalar vector\n",
    "        scalars = [\n",
    "            avg_dev, std_dev, avg_hnr, avg_tnr,\n",
    "            mean_rms_db, mean_lufs,\n",
    "            avg_jitter, avg_shimmer, avg_vibrato, avg_formant\n",
    "        ]\n",
    "        scalar_feats.append(scalars)\n",
    "\n",
    "        # 4) Quantum features\n",
    "        #    e.g. angles + measurement_counts => distribution\n",
    "        quantum_dict = analysis_data.get(\"quantum_analysis\", {})\n",
    "        angles = quantum_dict.get(\"scaled_angles\", [])\n",
    "        \n",
    "        # example: put up to 10 angles\n",
    "        max_len = 10\n",
    "        angle_arr = np.zeros(max_len, dtype=np.float32)\n",
    "        for i in range(min(max_len, len(angles))):\n",
    "            angle_arr[i] = angles[i]\n",
    "\n",
    "        # If you need distribution from measurement_counts\n",
    "        counts_d = quantum_dict.get(\"measurement_counts\", {})\n",
    "        dist_vec = convert_counts_to_probs_feature(counts_d, max_bits=5)  # shape(32,)\n",
    "\n",
    "        # combine angle_arr + dist_vec => shape (42,) if you do so\n",
    "        combined_q = np.concatenate([angle_arr, dist_vec], axis=0)\n",
    "        quantum_feats.append(combined_q)\n",
    "\n",
    "    # Now convert your lists into arrays or Tensors as desired\n",
    "    # For images, you might store them as a list of Tensors, or keep them on disk\n",
    "    images_mfcc = torch.stack(images_mfcc)  # shape(N, 3, 224, 224)\n",
    "    images_log  = torch.stack(images_log)\n",
    "\n",
    "    scalar_feats = np.array(scalar_feats, dtype=np.float32)\n",
    "    quantum_feats = np.array(quantum_feats, dtype=np.float32)\n",
    "\n",
    "    # If you want to encode raga as an integer label:\n",
    "    # e.g. map each unique raga to an integer\n",
    "    all_ragas = list(set(labels_raga))\n",
    "    raga_to_idx = {r: i for i, r in enumerate(sorted(all_ragas))}\n",
    "    labels_raga_idx = [raga_to_idx[r] for r in labels_raga]\n",
    "\n",
    "    # ‘quality’ is already numeric (1..5). You might keep it as is or shift to 0..4\n",
    "    labels_quality_arr = np.array(labels_quality, dtype=np.int64)  # shape(N,)\n",
    "\n",
    "    return {\n",
    "        \"images_mfcc\": images_mfcc,\n",
    "        \"images_log\": images_log,\n",
    "        \"scalar_feats\": scalar_feats,\n",
    "        \"quantum_feats\": quantum_feats,\n",
    "        \"label_raga_str\": labels_raga,      # string labels\n",
    "        \"label_raga_idx\": np.array(labels_raga_idx, dtype=np.int64),\n",
    "        \"label_quality\": labels_quality_arr,\n",
    "        \"raga_to_idx\": raga_to_idx\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
